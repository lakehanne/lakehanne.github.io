---
layout: post
date: 2015-11-05 17:21:00
title: "Optimal Controllers: A Model Predictive Control Approach (II)"
excerpt: " [sic] Predictions constitute the bulwark of model predictive controllers. [...] the prediction that this family of controllers have in their structure explains their robust performance when correctly implemented in a typical process."
permalink: Optimal-Controllers-MPC-II
comments: true
mathjax: true
---

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-NC6GN7"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NC6GN7');</script>
<!-- End Google Tag Manager -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End: Mathjax AMS Symbols -->

<!--Mathjax Parser -->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!--End Mathjax Parser -->

<!--Use Dollar Signs for inline math mode: -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" async src="path-to-mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>

<!--End Use Dollar Signs for inline math mode: -->

<!-- End: Mathjax $ Symbols -->


<!-- Topic Start -->

*This post continues the [**part I**](http://lakehanne.github.io/Optimal-Controllers-MPC/) of my series on the state space approach for Optimal Model Predictive Controllers*
 

### <center>**Predictions in System Models**</center>

#####<center>**<u>Background</u>**</center>
In principle, MPC's are long-range predictive controllers(LRPC). Contrary to classical control laws, they are potent if there is a process dead-time or if the setpoint is well-known ahead of time. They were introduced by [Richalet](#richalet) and [Cutler](#cutler) in the 1970's and '80's. As mentioned previously, predictions constitute the bulwark of model predictive controllers. Matter-of-factly, the prediction that this family of controllers have in their structure explains their robust performance when correctly implemented in a typical process. The basic algorithm involves:

* At a current time \\(k\\), we predict an output, \\(\hat{y}_k\\), over a certain output horizon, \\(n\_y\\) based on a mathematical model of the plant dynamics. The predicted output is a function of future possible control scenarios.

*  From the proposed scenarios, the strategy that delivers the best control action to bring the current process output to the setpoint is chosen.

*  The chosen control law is applied to the real process input only at the present time \\(k\\).

The above procedure is repeated at the next sampling instant leading to an updated control action with correctness based on latest measurements. In literature, we refer to this as the **_receding horizon concept_**

Other model-based controllers include pole-placement methods and Linear Quadratic methods. When there are no contraints on the control law and the setpoint is not complex, LQ-, pole-placement and predictive controllers generally yield an equivalent result.

We will assume the model of the plant has been correctly estimated as mentioned in the previous post. The model to be controlled (= *plant*) is used in predicting the process output over a defined prediction horizon, $n_y$. Typically, we would want to use an \\(i\\)-step ahead prediction based on our understanding of the system dynamics. The prediction horizon needs to be carefully selected such the computed optimal control law is not equivalent to a linear quadratic controller. Typically this happens when \\(n\_y - n\_u\\) is greater than the open-loop settling time and \\(n\_u\\) is \\(\geq\\) 5. For large \\(n\_y - n\_u\\) and large \\(n\_u\\), [Generalized Predictive Controllers](#GPCs) give an almost equal control to an optimal controller with the same weights. With \\(n\_y - n\_u\\) and \\(n\_u\\) small, the resulting control law may be severely [suboptimal](#Rossiter). Model predictive controllers are implemented in discrete time since control decisions are made discretely ( = instanteneously). Continuous time systems are sampled to obtain a discrete-time equivalent. The rule of thumb is that the sampling time should be \\(10 - 20\\) times slower than the dominant transient response of the system (e.g. rise time or settling time). 

>It is important not to sample at a faster rate than the dominant transient response of the system; otherwise, the high frequency gains within the system will not be picked up by the model. In other words, we should sample fast enough to pick up disturbances, but no faster.

Model Predictive Controllers are easy to implement when the model is linear and discrete but they do find applications in nonlinear processes as well. It behooves that a nonlinear model of the process would then be employed to design the controller in such an instance. Nonlinear models will be treated in a future post. Nonlinear systems that use model predictive control tend to have a more rigorous underpinning.

Within the framework of model predictive controllers, there are several variants in literature. Among these variants are <a name=GPCs></a>Clarke's Generalized Predictive Controllers (GPCs), [Dynamic Matrix Control](#cutler), Extended Predictive Self-Adaptive Control (EPSAC), Predictive Functional Control (PFC), Ydstie's Extended Horizon Adaptive Control (EHAC), and Unified Predictive Control (UPC) et cet'era. MPCs find applications in nonminimum phase systems, time-delay systems and unstable processes. I will briefly do a once-over on MAC, DMC and GPC with transfer function models and come back full circle to describe GPC with state-space models since these are generally straightforward to code and have less mathematical labyrinths.

####<center><u>**Prediction in Richalet's Model Algorithm Control (MAC)**</u></center>

Generally, these make use of impulse response function models. Suppose we denote the output of a discrete LTI system by a discrete impulse response \\(h(j)\\) as in

\begin{equation}
  H(z^{-1}) = A^{-1}(z^{-1}) \, B^(z^{-1})
\end{equation} 

it follows that the output can be written as a function of \\(h(t)\\) as follows

\begin{equation}  \label{eqn:impulse}
  y(t) = \sum_{j=1}^{n=\infty} \, h(j) \, u(t-j)
\end{equation} 

where \\(h(j)\\) are the respective coefficients of the impulse response. If we assume a stable and causal system, for an \\(n\\) terminal sampling instant, equation \eqref{eqn:impulse} becomes

\begin{equation}  \label{eqn:impulse-lim}
  y(t) = \sum_{j=1}^{n} \, h(j) \, u(t-j)
\end{equation} 

such that we can compute the recursive form of equation \eqref{eqn:impulse-lim} as

\begin{equation}  \label{eqn:impulse-rec}
  y\_{mdl}(t + k) = \tilde{h}^T \, \tilde{u}(t + k) = \tilde{u}^T(t + k)\, \tilde{h}
\end{equation} 

where, \\[\tilde{h}^T = h(1), \, h(2), \, h(3), \, \ldots, \, h(n) \\]
and \\[\tilde{u}^T(t + k) = u(t + k - 1), \, u(t + k - 2), \, u(t + k -3), \, \ldots, \, u(t + k - n) \\].

The cost function is given by

\begin{gather}  \label{eqn:mac-cost}
  J_{MAC} &= e^T \, e + \beta^2 \Delta u^T \Delta u  
\end{gather}
 
 where  \\(e = y\_r - y\_{mdl}\\), and \\(\beta\\) is a penalty function for the input variable \\(u(t)\\).


<div class="boxed">
<u><b>MAC Algorithm Summary</b></u>
<ul>
  <li>Define a reference trajectory, $y_{r}$, that $y(k)$ should track.</li>
  <li>Tune output predictions based on an $FIR$ model order to deal with model disturbances and uncertainties. </li>
  <li>Use $\beta$ to penalize the control law.</li>
  <li>Formulate the output predictions using equation \eqref{eqn:impulse-rec} with the assumption that the plant is stable and causal.</li>
</ul>
</div>


####<center>**<u>Prediction in Cutler's Dynamic Matrix Control</u>**</center>

Here, a [finite step response model](http://lakehanne.github.io/Optimal-Controllers-MPC/#fsr) is employed in the prediction. It is essentially composed of three tuning factors viz., the prediction horizon, \\(n_y\\), control weighting factor, \\(\beta\\), and the control horizon, \\(n\_u\\).

<div class="boxed">
<u><b>DMC Assumption</b></u>
<ul>
  <li>The process is stable and causal process</li>
</ul>
</div>


####<center><u>**GPC: Prediction of the plant output described by a transfer function**</u></center>

It seems to me that most literature is filled with prediction of the output described by a transfer function model. I assume this is due to their relative easiness of computation compared against FIR and FSR models and their applicability to unstable processes. Most papers out of Europe tend to favor transfer function methods while US researchers typically prefer state-space methods. Typically, [**Diophantine identity equations**](http://mathworld.wolfram.com/DiophantineEquation.html) are used to form the regressor model. In my opinion, this is royally complex and more often than not obscures the detail of what is being solved. My advice is one should generally stay out of Diophantine models when possible and only use them when necessary. I will briefly expand on this treatment and focus on MPC treatments using recursive state space models when describing GPC algorithms.

The transfer function model is of the form:

\begin{equation}  \label{eqn:tf_model}
  y(k) = \dfrac{z^{-d}B(z^{-1})}{A(z^{-1})}u(k-1)
\end{equation}

with \\(A(z^{-1})\\) and \\(B(z^{-1})\\) defined by the polynomial expansions in the previous post I. A close inspection of equation \eqref{eqn:tf_model} reveals that the transfer function model subsumes both the FIR (i.e. \\(A\\) = 1 and the coefficients of the \\(B\\) polynomial are the impulse response elements)  and FSR models (i.e. A = 1 and the coeeficients of the \\(B\\) polynomial are the step response coefficients \\(b\_0 = s\_0; \, b\_j = s\_j - s\_{j-1} \forall j \geq 1 \\).  

Transfer function models have the good properties of using greedy polynomial orders and variables in representing linear processes but an assumption about the model order has to be made. 

#####<center><u>**Output predictions in TFM with Diophantine identities**</u></center>

Another way of writing equation \eqref{eqn:tf_model} is by forming an \\(i\\)-step ahead predictor as follows:

\begin{equation}  \label{eqn:tf_isa}
  y(k + i) = \dfrac{z^{-d}B(z^{-1})}{A(z^{-1})}u(k + i -1)
\end{equation}

It follows from the [certainty equivalence principle](https://en.wikipedia.org/wiki/Stochastic_control#Certainty_equivalence) that if we substitute the delay, \\(d, \text{ and } B, \, A \\) polynomials with their estimates (i.e. \\(\hat{d}, \hat{B}, \, \text{and} \hat{A} \\)), the optimal control solution obtained would be the same as the system's delay, \\(d, \text{ and } B, \text{ and } A \\) polynomials. Let's form the predicted output estimate as follows:

\begin{equation}   \label{eqn:tf_pred}
  \hat{y}(k + i) = \dfrac{z^{-\hat{d}}\hat{B}(z^{-1})}{\hat{A}(z^{-1})}u(k + i -1)
\end{equation}

We could rearrange the above equation as

\begin{split}   
  \hat{y}(k + i) = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - \hat{y}(k + i)[\hat{A}(z^{-1}) -1] \nonumber 
\end{split}

Since \\(A(z^{-1}) = 1 + a_1\,z^{-1} + a\_2 + \, z^{-2} + \cdots + a\_{n\_a}z^{-n\_a} \\), by definition, we can write 

\begin{align}   
  \hat{y}(k + i) &= z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - \hat{y}(k + i)(a_1\,z^{-1} + a\_2 z^{-2} + \cdots + a\_{n\_a}z^{-n\_a}) \nonumber \\
\end{align}

\begin{align}
    \qquad \qquad &= z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - a\_1 \hat{y}(k + i -1) - a\_2 \hat{y}(k + i -2) - \cdots - a\_{n\_a} \hat{y}(k + i - n\_a)    \nonumber
\end{align}

\begin{align}
    & = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - (a\_1 + a\_2 z^{-1} + \cdots + a\_{n\_a}z^{-n\_a + 1}) \hat{y}(k + i - 1) \nonumber
\end{align}

such that we can write 

\begin{align}  \label{eqn:tf_isa2}
    \hat{y}(k + i) & = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - z(\hat{A} - 1) \hat{y}(k + i - 1) 
\end{align}

from equation \eqref{eqn:tf_pred}, where \\(z(\hat{A} - 1) = a\_1 + a\_2 z^{-1} + \, z^{-1} + \cdots + a\_{n\_a}z^{-n\_a + 1}\\).

The equation above is the \\(i\\)-step ahead predictor using the estimates and runs independently of the process. However, it is a fact of life that disturbances and uncertainties get a vote in any prediction model of a physical/chemical process. Therefore equation \eqref{eqn:tf_isa2} is not well-posed. To avoid prediction errors, we could replace \\(\hat{y}(k)\\) with \\(y(k) \\) in the equation and rearrange the model a bit further. Let's introduce the Diophantine identity equation 

<a name=Dioph></a>
<div class="boxed">
<b><center>Diophantine Identity Equation</center></b>
<ul>
\begin{equation}  
    \dfrac{1}{\hat{A}} = E_i + z^{-i}\dfrac{F_i}{\hat{A}}
\end{equation}
</ul>
</div>

where degree of \\(E\_i \leq i -1\\) and \\(F\_i\\) being of degree \\(n\_A -1\\).

Multiplying out equation \eqref{eqn:tf_pred} with \\(E\_i\\) gives

\begin{align}
    E\_i \hat{A}(z^{-1}) \hat{y}(k+i) & = z^{-\hat{d}} E\_iB(z^{-1}) u(k+i-1)
\end{align}

Rearranging the Diophantine equation and substituting \\(E\_i \hat{A}(z^{-1}) = 1 - z^{-i}F\_i \\) into the above equation, we find that

\begin{align}   \label{eqn:rigged}
    \hat{y}(k+i) & = z^{-\hat{d}} E\_iB(z^{-1}) u(k+i-1) + F\_i \underbrace{\hat{y}(k)}_{\textbf{replace with \\(y(k)\\)}}.
\end{align}
    
But
    
\begin{equation}
    E\_i\hat{B}(z^{-1}) = \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} - \dfrac{q^{-i} \hat{B}(z^{-1}) F\_i}{\hat{A(z^{-1})}},
\end{equation}

if we multiply the [Diophantine equation](#Dioph) by \\(\hat{B}(z^{-1})\\) such that 

\begin{align}  \label{eqn:tf_isasep}
    \hat{y}(k+i) & = z^{-\hat{d}} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} u(k+i-1) + F\_i y(k) - z^{-d} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} F\_i u(k -1)   \nonumber
\end{align}

\begin{align}
\qquad \qquad \quad & = \underbrace{z^{-\hat{d}} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} u(k+i-1)} + \underbrace{F\_i (y(k) - \hat{y}(k) ) }_{\textbf{correction } }
\end{align}

\begin{align}
\quad  &  \textbf{prediction} \nonumber 
\end{align}

We see that equation \eqref{eqn:tf_isasep} nicely separates the output predictor into a prediction part (from the past input) and a correction part (based on error between model and prediction at time, \\(k\\). Essentially, we have manipulated the equation \eqref(eqn:rigged) such that we have a correcting term in the prediction of the output by subsituting \\(y(k)\\) in place of \\(\hat{y}(k)\\) in equation \eqref(eqn:rigged) to negate issue of model-plant mismatch.

> Continued in [Part III](lakehanne.github.io/Optimal-Controllers-MPC-III).




<br></br>
##### Related References
<a name=Soeterboek></a>	[[Ronald Soeterboek]. 'Predictive Control: A Unified Approach'. Prentice Hall International (UK) Limited, 1992](http://repository.tudelft.nl/view/ir/uuid%3A18a07849-f43c-4d98-afb2-0b8a3a2e8b20/). 

<a name=cutler></a>[[C. R. Cutler, B. L. Ramaker]: Dynamic matrix control -- A computer control algorithm](https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004232009)

<a name=richalet></a> [J. Richalet, A. Rault, J.L. Testud and J. papon]: 'Model Predictive Heuristic Control: Applications to Industrial Processes', _Automatica_, Vol. 14. No. 5, pp. 413 - 428, 1978.

<a name=Rossiter></a> 	[[J.A. Rossiter]: 	'Model Predictive Control: A Practical Approach'. CRC Press LLC, Florida, USA. 2003](https://www.crcpress.com/Model-Based-Predictive-Control-A-Practical-Approach/Rossiter/9780849312915). 