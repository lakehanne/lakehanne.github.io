---
layout: post
date: 2016-09-30 12:21:00
title: "<center>Nonlinear Identification with Deep Neural Networks</center>"
excerpt: "<center>All the World is a Nonlinear System <br>
          He Linearized to the Right <br>
          He Linearized to the Left <br>
          Till Nothing was Right <br>
          And Nothing was Left <br>
          -- <i>Stephen Billings
        </center></i>"
permalink: Deep-Nets-Identification
comments: true
mathjax: true
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!--Mathjax Parser -->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

 <center>"All the world is a Nonlinear System <br>
          He Linearized to the Right <br>
          He Linearized to the Left <br>
          Till Nothing was Right <br>
          And Nothing was Left."<br>
          -- Stephen Billings</center> 

[Table of Contents](#table-of-contents)

  - [Introduction](#introduction)

  - [Deep Neural Networks](#what-are-dynamic-neural-networks?)

### Introduction
<TABLE BORDER="4"    WIDTH="85%"   CELLPADDING="1" CELLSPACING="2" ALIGN="CENTER">

    <TR ALIGN="CENTER">
      <TH>Abbreviation</TH>
      <TH>Meaning</TH>
      <TH>Abbreviation</TH>
      <TH>Meaning</TH>
    </TR>

    <TR ALIGN="CENTER">
      <TD>POMDP</TD>
      <TD>Partially Observable Markov Decision Process</TD>  
      <TD>DNN</TD>
      <TD>Deep Neural Network</TD>   
    </TR>

    <TR ALIGN="CENTER"> 
      <TD>SISO</TD>
      <TD>Single Input Single Output</TD>   
      <TD>MIMO</TD>
      <TD>Multi Iinput Multi Output</TD>  
    </TR>   

    <TR ALIGN="CENTER">
      <TD>RT</TD>
      <TD>Radiotherapy</TD> 
      <TD>MLP</TD>
      <TD>Multilayer Network (MLP)</TD>     
    </TR>  

    <TR ALIGN="CENTER">
      <TD>RNN</TD>
      <TD>Recurrent Neural Network</TD>  
      <TD>LSTM</TD>
      <TD>Long Short-Term Memory</TD>  
    </TR>  

    <TR ALIGN="CENTER">
      <TD>GRU</TD>
      <TD>Gated Recurrent Unit</TD>   
      <TD>MSE</TD>
      <TD>Mean Square Error</TD>
    </TR>  

    <TR ALIGN="CENTER">
      <TD>FPE</TD>
      <TD>Akaike's Final Prediction Error</TD>    
      <TD>AIC</TD>
      <TD>Akaike Information Criterion</TD>
    </TR>    
</TABLE>

(Artificial) Neural Networks applications are the latest rage in town. In the past four years alone, neural networks have been used more than ever before in approximating complex real-world nonlinear phenomena. If you use email that automatically disables spam, if you use a voice recognition software to navigate around your city, or if you use google translator to snoop on other people's conversations when in a foreign country, chances are high that you have used an application that is implementing a neural network in the backend. When NNs are combined in multiple layers, with each layer representing a level of abstraction, they are called deep neural networks. Deep networks allow compositional models made up of several simple processing layers with each layer transforming a complex representation at a higher, more absstract level to lower-level features. Broad applications of DNNs range from [image classification](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), [speech processing](http://ieeexplore.ieee.org/document/6638947/?arnumber=6638947), [language models](http://arxiv.org/abs/1410.4615) to [handwriting classification](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Convolution_nets.pdf). They are increasingly finding applications to control problems. For example, [AlphaGo](https://deepmind.com/research/alphago/) beat world-record players in the difficult game of GO earlier this year. Even so in Atari games, they have proven to be able to achieve human-level control e.g. based on raw pixel values, or using raw sensory inputs to achieve advanced manipulation skills. 

> Deep networks allow compositional models made up of several simple processing layers that each transform a complex representation at a higher, more absstract level to lower-level features.

The probing inquirer might ask, what makes (D)NNs so efficient at tasks that are otherwise difficult to solve in closed-form anyway? Well for one reason, (D)NNs in their most basic forms are an artificial representation of the pulse-frequencies found in neurons in the brain. They are the closest approximation to the giant supercomputer we humans have in our head (the human brain is estimated to have billions of neural circuit connections with back-coupled neurons and feedforward connections working together to help us understand our world). Imagine trying to mathematically formulate the factors of variation that dynamically capture how swarms of birds fly in different formations so that we can predict formation based on abstract formulae for different conditions (such as wind speed, humidity, light intensity et cetera) that affect flying behavior? It is a task that is difficult to manually formalize. Many problems in control have stochastic, high-dimensional dynamics that are highly nonlinear, usually POMDP ( in continuous processes). Analytical solutions that may be found may be too complicated to abstract into closed-form equations, building dynamical models that generalize to new contexts may be difficult, and the complexity of solving such high-dimensional spaced tasks may be intractable. Many researchers have thus resolved to devising learning algorithms that can <i>learn</i> these complex patterns in nature by building neural network structures that learn high-dimensional nonlinear phenomena. When implemented in the various structural combinations for pattern recognition or identification problems as discussed in this paper, disregarding the time chartacteristics, (D)NN output units are similar to gnostic cells in the brain.

### What are Dynamic Neural Networks?

<b><u>Static neural networks</u></b> are characterized by compact sets \\(\mathbb{U}\_i \subset \mathbb{R}^n \\) being mapped into elements \\(y\_i \in \mathbb{R}^m \, \forall \, \,  (i = 1,2, \cdots, n) \\) in the space of the output by a decision function \\(\mathbb{F}\\) (the elements of \\(\mathbb{U}\_i\\)would be the input elements mapped to class \\(y\_i\\) ). Static neural networks are used in pattern recognition problems. For a <b><u>dynamical system</u></b>, the operator $\mathbb{F}$ defines a plant based on an input-output pair of functions \\(\{u(k), y(k)\} \, \, \forall \, \, k \in [0, T]\\). In either case, the objective is the same: finding the operator \\(\hat{\mathbb{F}}\\) that approximates the actual operator \\(\mathbb{F}\\) to a sufficient degree \\(\epsilon > 0\\), \\(i.e.\ \\),
\begin{align}
  \lVert \hat{y} - y \lVert = \lVert \hat{\mathbb{F}}(u) - \mathbb{F}(u) \lVert \le \epsilon, \qquad u \in \mathbb{U}
\end{align}

where \\(\lVert \cdot \lVert \\) is an appropriately-chosen norm on the output space and \\(\epsilon \\) is a sufficiently positive number.

For low-dimensional systems, finite linear combinations of the input space and a set of affine functions can uniformly approximate continuous functions of \\(n\\) real variables with support in the unit hypercube. Cybenko [developed theorems](https://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf) that showed networks with one internal layer and an arbitrary continuous sigmoidal function can approximate continuous functions with arbitrary precision, provided that there are no restraints placed on the number of nodes or the size of the network weights. In other words, a single hidden layer is sufficient as a universal function approximator with the ability to approximate any Borel measurable function from one finite dimensional space to another. In terms of representability by hidden layers in multilayer networks, it is a settled question that arbitrary decision regions are well <i><b>approximated</b></i> by continuous deep networks with continuous nonlinearity<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. The focus of deep networks is on approximated solutions and not exact solutions as Mathematicians would love to think.

>It is a settled question that arbitrary decision regions in nonlinear systems are well <i><b>approximated</b></i> by continuous deep networks with continuous nonlinearity.

When the dimension of the dataset increases, finding a learning algorithm that scales to the learning problem irrespective of translation and geometric irregularities in data, and identifies the underlying structure in spite of noise in the data becomes more complicated. Enter deep learning. Representing the complicated structure of high-dimensional data is what deep nets are good at doing mostly because (i) the presence of local minima in the loss function is not a hindrance to learning for gradient-based algorithms that deep learning algorithms use (e.g. Boltzman machines successfuly learn nonlinear phenomena), and (ii) backpropagation for gradient computation in multi-layered networks with continuous, differentiable, and bounded activation functions can learn nonlinear phenomena well <a href="#fn2" class="footnoteRef" id="fnref2"><sup>1</sup></a>.

This blog post serves as an addendum to the paper I submitted to <i>American Control Conference (ACC) </i> last week titled <b><i>[Nonlinear Systems Identification Using Deep Dynamic Neural Networks](http://bibbase.org/service/mendeley/ac6ea3d6-def2-3e3f-b8fd-4db9697c94d6/file/3570398e-42e5-bf5b-64cd-cee1cd135a15/2017-Nonlinear_Systems_Identification_Using_Deep_Dynamic_Neural_Networks.pdf.pdf)</i></b>. Here I briefly go over the model structures I used, describe the datasets and expand a little more on the training procedure on the <i>Database for System Identification</i> data that I used. The codes are indexed on [github](https://github.com/lakehanne/FARNN) and if you do not care about the theory behind my choices in training the respective networks I described in the paper, you are welcome to start trainng your network.

### Datasets and preliminaries

Radiation therapy are by far the best procedure for the treatment of distributed cancer cells. They are effective at shrinking cancer tumor cells using high-precision localization of dosage targets to knock off tumor cells in the human body. Typically, the patient is treated with image-guided radiation therapy whereby a stereoscopic camera observes the patient while they are being treated with electron beams on a 6-DoF couch. The couch is adjusted by a radiation therapist during treatment to correct for involuntary motions and deviation of the patient's position from planned target(s). The cancer RT treatment is often stopped during treatment to correct for motion deviations using the 6-DoF robot. But the robots used are highly rigid systems that produce just enough torques for motion correction but not enough articulation to account for the non-rigidity of the curvatures of the human body. In a recent work, we tested the ability of inflatable air bladders (IABs) to compensate for the inadequate actuations provided in motion alignment correction systems (e.g. 6-DoF robot couches) used in maskless head and neck cancer radiotherapy. The soft-robot's precision was evaluated in the motion alignment and correction of these 6-DoF robots in a simulated cancer RT scenario.  We had a mannequin head lying in a supine position on a table that simulated our proposed motion alignment correction set-up during cancer RT. A soft-robot actuator in the form of an inflatable air bladder (IAB) moved the mannequin head based on supplied air pressure. This corrected for non-rigid motions during treatment. The IAB was actuated by current-driven proportional pneumatic valves; the experimental set-up is described in this [CASE2015 Paper](http://arxiv.org/abs/1506.04787), but the change in head motion is now recorded by a more accurate motion capture (mocap) system instead of an RGB-D camera system. The mocap system is capable of measuring head position with less than 1 \\(mm \\) error. This is a SISO system with input as current (generated from pseudo-random binary sequences) in \\(mm \\) and outputs as head height in \\(mm \\). We collected \\(10,070\\) samples of input-output data offline, and in all experiments, we separate the dataset in a \\( 60:40\% \\) ratio for training and testing purposes. 

<div class="fig figcenter fighighlight"> 
<img src="/imgs/NNDyn/setup.png" align="center" height="500px" />
<div class="figcaption" align="middle">Fig. 1: Experimental Set-up for soft robot system.</div>
</div>

The main idea in system identification is to charcterize the behavior of a plant by an operator \\( \mathbb{F} \\) that resolves the dynamics of a plant based on input-output datapair \\( \{u\_i(k), y\_i(k)\} \\)  for all \\(i = 1, 2, \cdots , N \\). Previously, an LTI model generated from a <i>prediction error model</i> was used in characterizing the system. This was insufficient to approximate the system nonlinearity. So here we propose NN-based Hammerstein block-structured models (Fig. 2).

<div class="fig figcenter fighighlight"> 
<center><img src="/imgs/NNDyn/ham.png"/></center>
<div class="figcaption" align="center">Fig 2: The Hammerstein block-structured model.</div>
</div>

In the Hammerstein model above, there are two block structures: one is a nonlinear element (the \\(g(\cdot)\\) block that models the dynamics from inputs to states while the second structure approximates the linear dynamics from states to the output space. I used deep recurrent network modules to model the nonlinear element while a linear multiulayer network models the \\(G(z^{-1}) \\) block.   


#### Multilayer Networks
These are the [Rumelhart et al's](https://www.researchgate.net/profile/Terrence_Sejnowski/publication/242509302_Learning_and_relearning_in_Boltzmann_machines/links/54a4b00f0cf256bf8bb327cc.pdf) feedforward networks with forward connections only signals between adjoining units. The networks are composed of layers of input and output signals with one or more hidden layers embedded between them. A differentiable, continuous and bounded monotonically increasing function (such as the sigmoid function) is used as the output of each network node (or unit). The relationship between the input and output datapair is usually learned by the backpropagation algorithm (while this is not the only means of training a network, it seems to me by far the most efficient and popular method) . This algorithm uses gradient descent to modify the parameters and thresholds of the network to the end of minimizing the error between the predicted output and actual output of the network. For a detailed treatment of how the backpropagation algorithm works, I recommend [Matt Mazur's backpropagation example blog post](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/). Multilayer networks have signal that flow in one direction only (possibly why they are often referred to as feedforward networks in literature). Introduced by Rumelhart-Williams-Hinton in the eighties, joining the weights and biases of the network completely parameterizes the system they are trained on. 

<div class="fig figcenter fighighlight"> 
<center><img src="/imgs/NNDyn/mlp.png" padding-right="20cm"/></center>
<div class="figcaption" align="middle">Fig 3: A simple multilayer network with 2 inputs, 3 nodes in <br>the hidden layer and 2 outputs.</div>
</div>

#### <b>Recurrent Networks</b>

We can think of these as feedforward networks with tapped delay lines. Inspired by the behavior of cells in nature with asynchronous parallel processing i.e. content-addressable memory with phase-space flow of the state of a system <a href="#fn3" class="footnoteRef" id="fnref3"><sup>1</sup></a>., they have the ability to capture long-term information based on pieces of the information that is available to them. In complex systems composed of several subsystems that interact to yield collective intelligence, the nature of the collective properties is insensitive to the individual behavior of specific sub-elements. These are what RNNs are good at (non-Markovian control). For an expository introduction to Recurrent Neural Networks, I recommend going through Andrej Karpathy's illuminating blog post on the [unreasonable effectiveness of recurrent networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). 

RNNs are the meat of this work and I dwell a little on their mathematical structure here. Assume the following text that is stored in memory, <i>"Funahashi, K. I. (1989). On the approximate realization of continuous mappings by neural networks."</i> A content addressable memory should be able to retrieve the entire memory given enough partial information such as "Funahashi, 1989". It should deal with errors by obtaining this reference through a self-correcting process. Now suppose the text in memory is stochastic in nature such that the equations that describe the content being written to memory is a flow in state space making the model more difficult to describe. Therefore stability becomes relative. But if minimal changes are being made in the letters written into memory, the stochastic effects are small so that the core of local stable points in the phase space remain. To bring the point home, a system whose physical dynamics in phase space is dominated by a substantial amount of locally stable nodes to which it is attracted is in general a content-addressable memory.

<div class="fig figcenter fighighlight"> 
<center><img src="/imgs/NNDyn/rnn.png" width="400px" height="300px"/></center>
<div class="figcaption" align="middle">Fig 4: A simple recurrent neural network.</div>
</div>

>a system whose physical dynamics in phase space is dominated by a substantial amount of locally stable nodes to which it is attarcted is generally referred to as content-addressable memory.

From Fig. 5, there is a striking disimilarily to the multilayer network depicted ini Fig. 4: that is the self-feedback of neurons in the hidden layer. Through this recurrent nature, neurons in the hidden layer are able to randomly and asynchronously determine if they should fire or not, based on their thresholds. The strong back-coupling of RNNs alllow for modeling delays in discrete-time dynamical systems and asynchronously process information. With a single hidden-layer, for example, such a network can be described by 

\begin{align}
x(k+1) = f(x(k)), \qquad x(0) = x\_o
\end{align}

where \\(f(\cdot) \\) is a suitably chosen nonlinear, bounded continuous function. If the system under consideration is a continuous-time system, the dynamical system in the feedback path has a diagonal transfer matrix with elements \\(1/(s + \alpha) \\) along the diagonal <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>. \\(\alpha \\) is an unknown parameter that parameterizes the lagged outputs; the system is then represented by the equation

\begin{align}
\dot{x} = -\alpha x + f(x) + U
\end{align}

where \\( x(t) \in \mathbb{R}^n \\) is the system's state at time t, and the constant vector \\(U \in \mathbb{R}^n \\) is the input. The gradients' computation in a RNN are commonly computed through [backpropagation thorugh time (BPTT)](http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2013/pdfs/Werbos.backprop.pdf), using Real-Time Recurrent Learning (Robinson and Fallside 1987), or other sensitivity analysis methods (Billings, S.A.). 

##### <b><u>Long Short-Term Memory (LSTMs) </u></b>
While ordinary recurrent networks are good for finite-delays, when the sequence of recurrence goes back in time by a significant order (e.g.  \\(> 500\\)), the gradients can become intractable. For example, consider a network node whose output unit \\(k\\) at time \\(t\\) is \\(y\_k(t)\\) so that the error computed for node \\(k\\) is 

\begin{align}
  \epsilon\_k(t) = {\sigma'\_k} \, [net\_k(t))(\hat{y}^k(t) - y\_k(t)]
\end{align}

where 
\begin{align}
  net\_i(t) = \sum\limits\_{j}^{}w\_{ij} \, y^j (t -1)
\end{align}

and
\begin{align}
  y^i(t) = \sigma\_i \, [net\_i(t)]
\end{align}

is the activation of an non-input node \\(i\\) with activation function \\(\sigma\_i\\). The error signal of a non-output unit \\(j\\) that is backpropagated is (see Horchreiter's analysis, 1997),

\begin{align}
  e\_j(t) = \sigma'\_j[net\_j(t)] \sum\limits\_{i}^{}w\_{ij} \, e\_i (t+1)
\end{align}

so that the contribution to \\(w_{ji}\\)'s weight update is \\(\alpha e\_j(t)y^l(t-1)\\), where \\(\alpha\\) is the step size for the gradient descent function on a neuron in layer \\(l\\), that is connected to neuron \\(j\\). Now if the indices of a fully connected neuron range from \\(1 \text{ to } n\\), then the local error flow from a neuron unit \\(u\\) to \\(v\\) that is backpropagated in time for \\(q\\) time steps to a unit \\(v\\) is scaled as follows

\begin{align}
  \dfrac{\partial e\_v(t-q)}{\partial e\_u(t)} &= \sigma'\_v[net\_v(t-1)]w_{uv} \qquad \text{ if } q = 1 \text{ or } 
\end{align}

\begin{align}    \label{eq:bptterr}
  \dfrac{\partial e\_v(t-q)}{\partial e\_u(t)} &= \sigma'\_v[net\_v(t-q)] \sum\limits\_{l = 1}^{n} \dfrac{\partial e\_l(t-q+1)}{\partial e\_u(t)} w_{lv} \qquad \text{ if } q > 1
\end{align}

If \\(l\_q = v \text{ and } l\_0 = u \\) is follows that

\begin{align}      \label{eq:errorback}
    \dfrac{\partial e\_v(t-q)}{\partial e\_u(t)} = \sum\limits\_{l\_1 = 1}^{n} \cdots \sum\limits\_{l\_{q-1} = 1}^{m} \, \prod\limits\_{m=1}^{q} \sigma'\_{lm}[net\_{lm}(t -m)] w\_{l\_m} l\_{m-1}
\end{align}

so that if \\(|\sigma'\_{lm}[net\_{lm}(t -m)] w\_{l\_m} l\_{m-1}| > 1.0 \text{ for all} m\\), then the product term in \eqref{eq:errorback} is intractable (causing the errors that flow back in time from u to v to be too large). Similarly, if \\(|\sigma'\_{lm}[net\_{lm}(t -m)] w\_{l\_m} l\_{m-1}| < 1.0 \text{ for all } m\\), then the product term exponentially decreases with the degree of \\(q\\) so thatthe error vanishes before reaching \\(u\\). These two conditions make learning difficult within a reasonable time-frame because (i) the error signals that are backpropagated in time can become infinitely high so that the gradients explode causing oscillating weights in the network or (ii) the errors being temporally backpropagated  could be so small that they cause gradients to vanish so that it becomes a lot more complex to compute gradients with slowly-varying network parameters.

Because of these issues with remembering long-term contextualization of inputs, RNNs have undergone different transformations in their basic architecture since the first [Hopfield Net](https://en.wikipedia.org/wiki/Hopfield_network). Recent works have sought to improve the vanishing and exploding gradients problem. Of important use to this work are Horchreiter's [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) and Cho's [Gated Recurrent Units](http://arxiv.org/abs/1406.1078) (LSTM). The LSTM network protects error flow from stochastic perturbations by employing <i>multiplicative units</i> that enforces constant error flows and solves long time-lag issues. For a weight matrix \\(W\\), the LSTM takes \\(O(W)\\) operations <a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>.

##### <u>Gated Recurrent Units (GRUs) </u>
Like LSTMs, <b>GRUs</b> were invented to solve the long-term contextualization of delayed signals, albeit with lesser computational complexity compared against LSTMs. The GRU consists of two types of RNN (an encoder and a decoder) acting separately transforming the input sequence into hidden states that are computed in different fashions to maximize the conditional log likelihood 

\begin{align}
  \text{max}\_{\theta} \dfrac{1}{N} \sum\limits\_{n=1}^{N}log p\_\theta (y\_n | x\_n)
\end{align}

where \\(n\\) is the lenth of the sequence, \\(\theta\\) is the vector of model parameters, and \\(p\_\theta\\) = \\(p(y\_1, \cdots, y\_n | x\_1, \cdots, x\_n)\\). For an input sequence \\(\textbf{x} = (x\_1, \cdots, x\_n)\\), the hidden state of the encoder is generated as a nonlinear function represented like so

\begin{align}
  h\_t = f(h\_{t-1}, u\_t)
\end{align}

such that the RNN can predict the next element in the sequence by learning the probability distribution over the input sequence. The output at each time step is the conditional distribution \\(p(x\_k = 1| x\_1, \cdots, x\_n\\). For a \\(1-of-K\\) encoding, the probability output can be computed for the entire sequence using 

\begin{align}
p(x) = \prod\limits\_{k = 1}^{K}p(x_k|x\_{k-1}, \cdots , x\_1).
\end{align}

The encoder reads the input sequence and updates the hidden state of the RNN as a summary \\(c\\) for the whole sequence.

The decoder uses another simple RNN by predicting the next output sequence \\(y\_k\\) given the hidden state, \\(h\_k\\) but \\(y_k\\) and \\(h\_k\\) are in turn conditioned on \\(y\_{k-1}\\) and the generated summary from the input sequence, \\(\textbf{c}\\). In other words, 

\begin{align}
  h\_{k} = f(h\_{k-1}, y\_{k-1}, \textbf{c})
\end{align}

Again, the conditional distribution of \\(y\_k\\) is given by 

\begin{align}
P(y\_k | y\_{k-1}, \cdots, y\_k, \textbf{c}) = g(\textbf{h}\_k, y\_{k-1}, \textbf{c})
\end{align}

We can imagine the encoder-decoder as jointly maximizing the conditional log-likelihood 

\begin{align}
max\_\theta \dfrac{1}{N}\sum\limits_{n=1}^{N} log p\_\theta(y\_n|x\_n)
\end{align}

The hidden unit is composed of two critical gates, as is the case for the LSTM, that adaptively forgets or remebers a long-range sequence. The reset gate is given by 

\begin{align}
r\_j = \sigma\left([\textbf{W}\_r\textbf{x}]\_j + [\textbf{U}\_r \textbf{h}\_{k-1}]\_j\right)
\end{align}

with \\(\sigma\\) being the logistic sigmoid function and \\([\cdot]\_j\\) being the \\(j-th\\) element of the vector. The update gate is computed as

\begin{align}    
  z\_j = \sigma\left([\textbf{W}\textbf{x}]\_j + [\textbf{U}(\textbf{r} \odot \textbf{h}\_{t-1} )]\_j\right)
\end{align}

The activation of the hidden unit \\(h\_j\\) is determined through

\begin{align}
h\_j^{(k)} = z\_j h\_j ^ {k -1} + (1 - z\_j) \bar{h}\_j^k
\end{align}

where

\begin{align}   \label{eq:hidden}
\bar{h}\_j^k = \phi\left([\textbf{Wx}\_j + [\textbf{U}(\textbf{r \odot \textbf{h}\_{t-1}]\_j\right)
\end{align}

Examining \autoref{eq:hidden}, we see that the hidden state ignores past signals that are not relevant to future evolution of the system when the reset gate is 0. The update gate determines how much information from the past hidden states to carry over into the current hidden state (similar to the memory cell in the LSTM). GRUs thus allow a more compact representation of long-term dependency in RNN's without exposing the RNN state to the exploding or vanishing gradients problem.

...continued in [post 2](https://lakehanne.github.io/Deep-Nets-Identification-2)

### Acknowledgement
I am grateful to the maintainers of the Torch package which has helped in accelerating the training my models.
I am also grateful to [Hugo Larochelle](https://twitter.com/hugo_larochelle) and [Tejas Kulkarni](https://cbmm.mit.edu/about/people/kulkarni) for pointing me in the direction of best practices for regularizing the layers of a recurrent neural network during training.  

<section class="footnotes">
<hr>
<h4>References</h4> 
<ul>
<li id="fn1"><p> Cybenko, G. (1993). Approximation by Superpositions of a Sigmoidal Function. Approximation Theory and Its Applications, 9(3), 17–28. http://doi.org/10.1007/BF02836480<a href="#fnref1">↩</a></p></li>
</ul>

<ul>
<li id="fn2"><p>
LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2323. http://doi.org/10.1109/5.726791
<a href="#fnref2">↩</a></p></li>
</ul>

<ul>
<li id="fn3"><p>
Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences of the United States of America, 79(8), 2554–2558. http://doi.org/10.1073/pnas.79.8.2554
<a href="#fnref3">↩</a></p></li>
</ul>

<ul>
<li id="fn4"><p>
Narendra, K. S. (1992). Identification and Control of Dynamical Systems Using Neural Networks.
<a href="#fnref4">↩</a></p></li>
</ul>

<ul>
<li id="fn5"><p>
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–80. http://doi.org/10.1162/neco.1997.9.8.1735
<a href="#fnref5">↩</a></p></li>
</ul>

</section>


