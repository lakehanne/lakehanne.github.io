---
layout: post
date: 2016-09-30 12:21:00
title: "<center>Nonlinear Identification with Deep Neural Networks</center>"
excerpt: "<center>All the world is a Nonlinear System <br>
          He Linearized to the Right <br>
          He Linearized to the Left <br>
          Till Nothing was Right <br>
          And Nothing was Left <br>
          -- <i>Stephen Billings
        </center></i>"
permalink: Deep-Nets-Identification
comments: true
mathjax: true
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!--Mathjax Parser -->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

 <center>"All the world is a Nonlinear System <br>
          He Linearized to the Right <br>
          He Linearized to the Left <br>
          Till Nothing was Right <br>
          And Nothing was Left."<br>
          -- Stephen Billings</center> 

[Table of Contents](#table-of-contents)

  - [Introduction](#introduction)

  - [Deep Neural Networks](#what-are-dynamic-neural-networks?)

### Introduction
<TABLE BORDER="4"    WIDTH="85%"   CELLPADDING="1" CELLSPACING="2" ALIGN="CENTER">

    <TR ALIGN="CENTER">
      <TH>Abbreviation</TH>
      <TH>Meaning</TH>
      <TH>Abbreviation</TH>
      <TH>Meaning</TH>
    </TR>

    <TR ALIGN="CENTER">
      <TD>NN</TD>
      <TD>Neural Network</TD>  
      <TD>DNN</TD>
      <TD>Deep Neural Network</TD>   
    </TR>

    <TR ALIGN="CENTER"> 
      <TD>SISO</TD>
      <TD>Single Input Single Output</TD>   
      <TD>MIMO</TD>
      <TD>Multi Iinput Multi Output</TD>  
    </TR>   

    <TR ALIGN="CENTER">
      <TD>RT</TD>
      <TD>Radiotherapy</TD> 
      <TD>MLP</TD>
      <TD>Multilayer Network (MLP)</TD>     
    </TR>  

    <TR ALIGN="CENTER">
      <TD>RNN</TD>
      <TD>Recurrent Neural Network</TD>  
      <TD>LSTM</TD>
      <TD>Long Short-Term Memory</TD>  
    </TR>  

    <TR ALIGN="CENTER">
      <TD>GRU</TD>
      <TD>Gated Recurrent Unit</TD>   
      <TD>MSE</TD>
      <TD>Mean Square Error</TD>
    </TR>  

    <TR ALIGN="CENTER">
      <TD>FPE</TD>
      <TD>Akaike's Final Prediction Error</TD>    
      <TD>AIC</TD>
      <TD>Akaike Information Criterion</TD>
    </TR>    
</TABLE>

Artificial Neural Networks applications (henceforth called Neural Networks) are the latest rage in town. In the past four years alone, neural networks have been used more than ever before in approximating complex real-world nonlinear phenomena. If you use email that automatically disables spam, if you use a voice recognition software to navigate around your city, or if you use google translator to snoop on other people's conversations when in a foreign country, chances are high that you have used an application that is implementing a neural network in the backend. When NNs are combined in multiple layers, with each layer representing a level of abstraction, they are called deep neural networks. Deep networks allow compositional models made up of several simple processing layers that each transform the representation at a higher, more absstract level. Broad applications of DNNs range from [image classification](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), [speech processing](http://ieeexplore.ieee.org/document/6638947/?arnumber=6638947), [language models](http://arxiv.org/abs/1410.4615) to [handwriting classification](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Convolution_nets.pdf). Heck, they have even beaten world-record players in the difficult game of GO using [AlphaGo](https://deepmind.com/research/alphago/). 

The probing inquirer might ask, what makes (D)NNs so efficient at tasks that are otherwise difficult to solve in closed-form anyway? Well for one reason, (D)NNs in their most basic form are an artificial representation of the pulse-frequencies found in neurons in the brain. They are the closest approximation of the giant supercomputer we humans have in our head (the human brain is estimated to have billions of neural circuit connections with back-coupled neurons and feedforward connections working together to help us understand our world). Imagine trying to mathematically formulate the factors of variation that dynaically capture how swarms of birds fly in different formations so that we can predict formation based on abstract formulae for different conditions (such as wind speed, humidity, light intensity et cetera) that affect flying behavior? It is a task that is difficult to manually formalize. Analytical solutions that may be found may not generalize to new contexts. It seems reasonable that a model of the human brain, which by the way is very good at nonlinear abstraction, should be adopted to complicated engineering modeling and design tasks. Many researchers have thus resolved to devising learning algorithms that can learn these <i>complex patterns in nature</i> by building neural network structures that learn nonlinear phenomena. When implemented in the various structural combinations for pattern recognition or identification problems as discussed in this paper, disregarding the time chartacteristics, (D)NN output units are similar to gnostic cells in the brain.

<!-- Ever since Rosenblatt's first formulation of the perceptron and its use in the Mark I computer, the perceptron has been combined in various structural forms to make it better at adaptively learning patterns in images, speech and other domains. The scope of this blog post is an introductory treatise on deep dynamic neural networks for identification of complicated systems.  -->

### What are Dynamic Neural Networks?

<u>Static neural networks</u> are characterized by compact sets \\(\mathbb{U}\_i \subset \mathbb{R}^n \\) being mapped into elements \\(y\_i \in \mathbb{R}^m \, \forall \, \,  (i = 1,2, \cdots, n) \\) in the space of the output by a decision function \\(\mathbb{F}\\) (the elements of \\(\mathbb{U}\_i\\)would be the input elements mapped to class \\(y\_i\\) ). Static neural networks are used in pattern recognition problems. For a <u>dynamical system</u>, operator $\mathbb{F}$ defines a plant based on an input-output pair of funtions \\(u(k), y(k) \, \, \forall \, \, k \in [0, T]\\). In either case, the objective is the same: finding the operator \\(\hat{\mathbb{F}}\\) that approximates the actual operator \\(\mathbb{F}\\) to a sufficient degree \\(\epsilon > 0\\), \\(i.e.\ \\),
\begin{align}
  \lVert \hat{y} - y \lVert = \lVert \hat{\mathbb{F}}(u) - \mathbb{F}(u) \lVert \le \epsilon, \qquad u \in \mathbb{U}
\end{align}

where \\(\lVert \cdot \lVert \\) is an appropriate norm on the output space and \\(\epsilon \\) is a sufficiently positive number.

For low-dimensional systems, finite linear combinations of the input space and a set of affine functions can uniformly approximate continuous functions of \\(n\\) real variables with support in the unit hypercube. Cybenko [developed theorems](https://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf) that showed networks with one internal layer and an arbitrary continuous sigmoidal function can approximate continuous functions with arbitrary precision if there are no restraints placed on the number of nodes or the size of weights. In other words, a single hidden layer is sufficient as a universal function approximator with the ability to approximate any Borel measurable function from one finite dimensional space to another.

When the dimension od data increases, finding a learning algorithm that scales to the learning problem irrespective of translation and geometric invariance, and that identifies the underlying structure irrespective of noise in the data becomes important. Representing the complicated structure in high-dimensional data is what deep nets are good at doing. 

This blog post serves as an addendum to the paper I submitted to ACC last week titled Nonlinear Systems Identification Using Deep Dynamic Neural Networks. Here I briefly go over the model structures I used, describe the datasets and expnd a little more on the training procedure. For training my models, I used the Torch package now mostly maintained by Soumith Chintala at Facebook. The repos are on my github [FARNN Repo page](https://github.com/lakehanne/FARNN.git).

### Datasets

In a recent work, we tested the ability of inflatable air bladders to compensate for motion alignment correction systems (6-DoF robot couches) in maskless cancer radiotherapy. The models developed therein were LTI systems identified from input-output data pairs. This work extends the erstwhile LTI models to nonlinear approximating models based on different deep neural network architectures modeled after the Hammerstein block structure. 

<div class="fig figcenter fighighlight"> 
<img src="/assets/hammerstein/ham.png" align="center"/>
</div>

In the Hammerstein model above, there are two block structures: one is a nonlinear element (the \\(g(\cdot)\\) block that models dynamics from inputs to states while the second structure approximates the linear dynamics from states to the output space. I used deep recurrent network modules to model the nonlinear element while a multiulayer network models the \\(G(z^{-1}) \\) block.   


### Multilayer Networks
These are the [Rumelhart et al's](https://www.researchgate.net/profile/Terrence_Sejnowski/publication/242509302_Learning_and_relearning_in_Boltzmann_machines/links/54a4b00f0cf256bf8bb327cc.pdf) feedforward networks with forward connections only signals between adjoining units. The networks are composed of layers of input and output signals with one or more hidden layers embedded between them. A differentiable, continuous and boumded monotonically increasing function (such as the sigmoid function) is used as the output of each network node (or unit). The relationship between the input and output datapair is usually learned by the backpropagation algorithm. This algorithm uses gradient descent to modify the parameters and thresholds of the network to the end of minimizing the error between the predicted output and actual output of the network. For a detailed treatment of how the backpropagation algorithm works, I recommend [Matt Mazur's backpropagation example blog post](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)

<br></br>
Optimal controllers are mostly used in discrete processes ([others](#Gawthrop) have postulated continuous time models for such controllers but the vast majority of commercial users of optimal controllers are in discrete time) and are very useful for processes where the setpoint is known ahead of time. Typically, a criterion function is used to determine how well the controller is tracking a desired trajectory. An example model predictive controller (MPC) criterion function, in its basic form, is given as  
