---
layout: post
date: 2016-09-30 12:21:00
title: "<center>Nonlinear Identification with Deep Neural Networks</center>"
excerpt: "<center>All the World is a Nonlinear System <br>
          He Linearized to the Right <br>
          He Linearized to the Left <br>
          Till Nothing was Right <br>
          And Nothing was Left <br>
          -- <i>Stephen Billings
        </center></i>"
permalink: Deep-Nets-Identification
comments: true
mathjax: true
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!--Mathjax Parser -->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

 <center>"All the world is a Nonlinear System <br>
          He Linearized to the Right <br>
          He Linearized to the Left <br>
          Till Nothing was Right <br>
          And Nothing was Left."<br>
          -- Stephen Billings</center> 

[Table of Contents](#table-of-contents)

  - [Introduction](#introduction)

  - [Deep Neural Networks](#what-are-dynamic-neural-networks?)

### Introduction
<TABLE BORDER="4"    WIDTH="85%"   CELLPADDING="1" CELLSPACING="2" ALIGN="CENTER">

    <TR ALIGN="CENTER">
      <TH>Abbreviation</TH>
      <TH>Meaning</TH>
      <TH>Abbreviation</TH>
      <TH>Meaning</TH>
    </TR>

    <TR ALIGN="CENTER">
      <TD>POMDP</TD>
      <TD>Partially Observable Markov Decision Process</TD>  
      <TD>DNN</TD>
      <TD>Deep Neural Network</TD>   
    </TR>

    <TR ALIGN="CENTER"> 
      <TD>SISO</TD>
      <TD>Single Input Single Output</TD>   
      <TD>MIMO</TD>
      <TD>Multi Iinput Multi Output</TD>  
    </TR>   

    <TR ALIGN="CENTER">
      <TD>RT</TD>
      <TD>Radiotherapy</TD> 
      <TD>MLP</TD>
      <TD>Multilayer Network (MLP)</TD>     
    </TR>  

    <TR ALIGN="CENTER">
      <TD>RNN</TD>
      <TD>Recurrent Neural Network</TD>  
      <TD>LSTM</TD>
      <TD>Long Short-Term Memory</TD>  
    </TR>  

    <TR ALIGN="CENTER">
      <TD>GRU</TD>
      <TD>Gated Recurrent Unit</TD>   
      <TD>MSE</TD>
      <TD>Mean Square Error</TD>
    </TR>  

    <TR ALIGN="CENTER">
      <TD>FPE</TD>
      <TD>Akaike's Final Prediction Error</TD>    
      <TD>AIC</TD>
      <TD>Akaike Information Criterion</TD>
    </TR>    
</TABLE>

Artificial Neural Networks applications (henceforth called Neural Networks) are the latest rage in town. In the past four years alone, neural networks have been used more than ever before in approximating complex real-world nonlinear phenomena. If you use email that automatically disables spam, if you use a voice recognition software to navigate around your city, or if you use google translator to snoop on other people's conversations when in a foreign country, chances are high that you have used an application that is implementing a neural network in the backend. When NNs are combined in multiple layers, with each layer representing a level of abstraction, they are called deep neural networks. Deep networks allow compositional models made up of several simple processing layers that each transform a complex representation at a higher, more absstract level to low-level features. Broad applications of DNNs range from [image classification](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), [speech processing](http://ieeexplore.ieee.org/document/6638947/?arnumber=6638947), [language models](http://arxiv.org/abs/1410.4615) to [handwriting classification](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Convolution_nets.pdf). They are increasingly finding applications to control such as in [AlphaGo](https://deepmind.com/research/alphago/) (where they beat world-record players in the difficult game of GO), in playing Atari games based on raw pixel values, or using raw sensory inputs to achieve advanced manipulation skills. 

> Deep networks allow compositional models made up of several simple processing layers that each transform a complex representation at a higher, more absstract level to lower-level features.

The probing inquirer might ask, what makes (D)NNs so efficient at tasks that are otherwise difficult to solve in closed-form anyway? Well for one reason, (D)NNs in their most basic form are an artificial representation of the pulse-frequencies found in neurons in the brain. They are the closest approximation to the giant supercomputer we humans have in our head (the human brain is estimated to have billions of neural circuit connections with back-coupled neurons and feedforward connections working together to help us understand our world). Imagine trying to mathematically formulate the factors of variation that dynamically capture how swarms of birds fly in different formations so that we can predict formation based on abstract formulae for different conditions (such as wind speed, humidity, light intensity et cetera) that affect flying behavior? It is a task that is difficult to manually formalize. Many problems in control have stochastic, high-dimensional dynamics that are highly nonlinear, usually POMDP (that is to say the state at the current time is unknown based on available previous states and actions). Analytical solutions that may be found may be too complicated to abstract into closed-form equations, building dynamical models that generalize to new contexts may be difficult and the complexity of solving such high-dimensional spaced tasks may be intractable. Many researchers have thus resolved to devising learning algorithms that can <i>learn</i> these complex patterns in nature by building neural network structures that learn high-dimensional nonlinear phenomena. When implemented in the various structural combinations for pattern recognition or identification problems as discussed in this paper, disregarding the time chartacteristics, (D)NN output units are similar to gnostic cells in the brain.

<!-- Ever since Rosenblatt's first formulation of the perceptron and its use in the Mark I computer, the perceptron has been combined in various structural forms to make it better at adaptively learning patterns in images, speech and other domains. The scope of this blog post is an introductory treatise on deep dynamic neural networks for identification of complicated systems.  -->

### What are Dynamic Neural Networks?

<b><u>Static neural networks</u></b> are characterized by compact sets \\(\mathbb{U}\_i \subset \mathbb{R}^n \\) being mapped into elements \\(y\_i \in \mathbb{R}^m \, \forall \, \,  (i = 1,2, \cdots, n) \\) in the space of the output by a decision function \\(\mathbb{F}\\) (the elements of \\(\mathbb{U}\_i\\)would be the input elements mapped to class \\(y\_i\\) ). Static neural networks are used in pattern recognition problems. For a <b><u>dynamical system</u></b>, the operator $\mathbb{F}$ defines a plant based on an input-output pair of funtions \\(u(k), y(k) \, \, \forall \, \, k \in [0, T]\\). In either case, the objective is the same: finding the operator \\(\hat{\mathbb{F}}\\) that approximates the actual operator \\(\mathbb{F}\\) to a sufficient degree \\(\epsilon > 0\\), \\(i.e.\ \\),
\begin{align}
  \lVert \hat{y} - y \lVert = \lVert \hat{\mathbb{F}}(u) - \mathbb{F}(u) \lVert \le \epsilon, \qquad u \in \mathbb{U}
\end{align}

where \\(\lVert \cdot \lVert \\) is an appropriately-chosen norm on the output space and \\(\epsilon \\) is a sufficiently positive number.

For low-dimensional systems, finite linear combinations of the input space and a set of affine functions can uniformly approximate continuous functions of \\(n\\) real variables with support in the unit hypercube. Cybenko [developed theorems](https://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf) that showed networks with one internal layer and an arbitrary continuous sigmoidal function can approximate continuous functions with arbitrary precision if there are no restraints placed on the number of nodes or the size of weights. In other words, a single hidden layer is sufficient as a universal function approximator with the ability to approximate any Borel measurable function from one finite dimensional space to another. In terms of representability by hidden layers in multilayer networks, it is a settled question that arbitrary decision regions are well <i><b>approximated</b></i> by continuous deep networks with continuous nonlinearity. The focus of deep networks is on approximated solutions and not exact solutions as Mathematicians would love to think.

>It is a settled question that arbitrary decision regions are well <i><b>approximated</b></i> by continuous deep networks with continuous nonlinearity.

When the dimension of the dataset increases, finding a learning algorithm that scales to the learning problem irrespective of translation and geometric invariance, and that identifies the underlying structure irrespective of noise in the data becomes more complicated. Enter deep learning. Representing the complicated structure in high-dimensional data is what deep nets are good at doing mostly because (i) the presence of local minima in the loss function is not a hindrance to learning for gradient-based algorithms such as deep learning algorithms (e.g. Boltzman machines successfuly learn nonlinear phenomena), (ii) backpropagation for gradient computation in multi-layered networks with continuous, differentiable, and bounded activation functions can learn nonlinear phenomena well.

This blog post serves as an addendum to the paper I submitted to <i>American Control Conference (ACC) </i> last week titled <b><i>Nonlinear Systems Identification Using Deep Dynamic Neural Networks</i></b>. Here I briefly go over the model structures I used, describe the datasets and expand a little more on the training procedure on the <i>Database for System Identification</i> data that I used. 

### Datasets and preliminaries

In a recent work, we tested the ability of inflatable air bladders (IABs) to compensate for malformities in motion alignment correction systems (e.g. 6-DoF robot couches) used in maskless head and neck cancer radiotherapy. Radiation therapy are by far the best procedure for the treatment of distributed cancer cells. They are effective at shrinking cancer tumor cells using high-precision localization of dosage targets to knock off tumor cells in the human body. Typically, the patient is treated with image-guided radiation therapy where a stereoscopic camera observes the patient while they are being treated with beams from a CBCT on a 6-DoF couch. The couch is adjusted by a radiation therapist during treatment to correct for involuntary motions and deviation of the patient's position from planned target(s). The cancer RT treatmentis often stopped during treatment to correct motion using the 6-DoF robot. But the robots used are highly rigid systems that produce enough torques for motion correction but not enough articulation to account for the non-rigidity of the curvatures of the human body. 

We tested a soft-robot's precision in the motion alignment and correction of these 6-DoF robots in a simulated cancer RT scenario.  We had a mannequin head lying in a supine position on a table that simulated our proposed motion alignment correction set-up during cancer RT. A soft-robot actuator in the form of an inflatable air bladder (IAB) moved the mannequin head based on supplied air pressure. This corrected for non-rigid motions during treatment. The IAB was actuated by current-driven proportional pneumatic valves; the experimental set-up is described in my [CASE2015 Paper](http://arxiv.org/abs/1506.04787), but the change in head motion is now recorded by a motion capture (mocap) system instead of an RGB-D camera system. The mocap system is capable of measuring head position with less than 1 \\(mm \\) error. This is a SISO system with input as current (generated from pseudo-random binary sequences) in \\(mm \\) and outputs as head height in \\(mm \\). We collected \\(10,070\\) samples of input-output data offline, and in all experiments, we separate the dataset in a \\( 60:40\% \\) ratio for training and testing purposes. 

<div class="fig figcenter fighighlight"> 
<img src="/imgs/NNDyn/setup.png" align="center" height="500px" />
<div class="figcaption" align="middle">Fig. 1: Experimental Set-up for soft robot system.</div>
</div>

The chief idea in system identification experiments is to charcterize the behavior of a plant by an operator \\( \mathbb{F} \\) that resolves the dynamics of a plant based on input-output datapair \\( {u\_i(k), y\_i(k)} \\)  for all \\(i = 1, 2, \cdots , N \\). Previously, we used the <i>prediction error model</i> in characterizing input-output relationships, here, we propose Hammerstein block-structured models, with the nonlinear element approximated by recurrent neural networks and the linear dynaical subsystem approximated by a multilayer network.

<div class="fig figcenter fighighlight"> 
<img src="/assets/hammerstein/ham.png" align="right"/>
</div>

In the Hammerstein model above, there are two block structures: one is a nonlinear element (the \\(g(\cdot)\\) block that models dynamics from inputs to states while the second structure approximates the linear dynamics from states to the output space. I used deep recurrent network modules to model the nonlinear element while a multiulayer network models the \\(G(z^{-1}) \\) block.   


### Multilayer Networks
These are the [Rumelhart et al's](https://www.researchgate.net/profile/Terrence_Sejnowski/publication/242509302_Learning_and_relearning_in_Boltzmann_machines/links/54a4b00f0cf256bf8bb327cc.pdf) feedforward networks with forward connections only signals between adjoining units. The networks are composed of layers of input and output signals with one or more hidden layers embedded between them. A differentiable, continuous and boumded monotonically increasing function (such as the sigmoid function) is used as the output of each network node (or unit). The relationship between the input and output datapair is usually learned by the backpropagation algorithm. This algorithm uses gradient descent to modify the parameters and thresholds of the network to the end of minimizing the error between the predicted output and actual output of the network. For a detailed treatment of how the backpropagation algorithm works, I recommend [Matt Mazur's backpropagation example blog post](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)

<br></br>
Optimal controllers are mostly used in discrete processes ([others](#Gawthrop) have postulated continuous time models for such controllers but the vast majority of commercial users of optimal controllers are in discrete time) and are very useful for processes where the setpoint is known ahead of time. Typically, a criterion function is used to determine how well the controller is tracking a desired trajectory. An example model predictive controller (MPC) criterion function, in its basic form, is given as  





I am grateful to the maintainers of the Torch package who helped me in accelerating the training my models.