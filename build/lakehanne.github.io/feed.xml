<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Olalekan Ogunmolu</title>
    <description>To Discover and Understand.</description>
    <link>lakehanne.github.io/</link>
    <atom:link href="lakehanne.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>2016-10-14 15:18:17 -0500</pubDate>
    <lastBuildDate>2016-10-14 15:18:17 -0500</lastBuildDate>
    <generator>Jekyll v</generator>
    
      <item>
        <title>&lt;center&gt;Nonlinear Identification with Deep Neural Networks&lt;/center&gt;</title>
        <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;!--Mathjax Parser --&gt;

&lt;script type=&quot;text/javascript&quot; async
  src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;p&gt;&lt;center&gt;&amp;quot;All the world is a Nonlinear System &lt;br&gt;
          He Linearized to the Right &lt;br&gt;
          He Linearized to the Left &lt;br&gt;
          Till Nothing was Right &lt;br&gt;
          And Nothing was Left.&amp;quot;&lt;br&gt;
          -- Stephen Billings&lt;/center&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;Table of Contents&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;#what-are-dynamic-neural-networks?&quot;&gt;Deep Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Introduction&lt;/h3&gt;

&lt;TABLE BORDER=&quot;4&quot;    WIDTH=&quot;85%&quot;   CELLPADDING=&quot;1&quot; CELLSPACING=&quot;2&quot; ALIGN=&quot;CENTER&quot;&gt;

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TH&gt;Abbreviation&lt;/TH&gt;
      &lt;TH&gt;Meaning&lt;/TH&gt;
      &lt;TH&gt;Abbreviation&lt;/TH&gt;
      &lt;TH&gt;Meaning&lt;/TH&gt;
    &lt;/TR&gt;

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;POMDP&lt;/TD&gt;
      &lt;TD&gt;Partially Observable Markov Decision Process&lt;/TD&gt;  
      &lt;TD&gt;DNN&lt;/TD&gt;
      &lt;TD&gt;Deep Neural Network&lt;/TD&gt;   
    &lt;/TR&gt;

    &lt;TR ALIGN=&quot;CENTER&quot;&gt; 
      &lt;TD&gt;SISO&lt;/TD&gt;
      &lt;TD&gt;Single Input Single Output&lt;/TD&gt;   
      &lt;TD&gt;MIMO&lt;/TD&gt;
      &lt;TD&gt;Multi Iinput Multi Output&lt;/TD&gt;  
    &lt;/TR&gt;   

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;RT&lt;/TD&gt;
      &lt;TD&gt;Radiotherapy&lt;/TD&gt; 
      &lt;TD&gt;MLP&lt;/TD&gt;
      &lt;TD&gt;Multilayer Network (MLP)&lt;/TD&gt;     
    &lt;/TR&gt;  

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;RNN&lt;/TD&gt;
      &lt;TD&gt;Recurrent Neural Network&lt;/TD&gt;  
      &lt;TD&gt;LSTM&lt;/TD&gt;
      &lt;TD&gt;Long Short-Term Memory&lt;/TD&gt;  
    &lt;/TR&gt;  

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;GRU&lt;/TD&gt;
      &lt;TD&gt;Gated Recurrent Unit&lt;/TD&gt;   
      &lt;TD&gt;MSE&lt;/TD&gt;
      &lt;TD&gt;Mean Square Error&lt;/TD&gt;
    &lt;/TR&gt;  

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;FPE&lt;/TD&gt;
      &lt;TD&gt;Akaike's Final Prediction Error&lt;/TD&gt;    
      &lt;TD&gt;AIC&lt;/TD&gt;
      &lt;TD&gt;Akaike Information Criterion&lt;/TD&gt;
    &lt;/TR&gt;    
&lt;/TABLE&gt;

&lt;p&gt;(Artificial) Neural Networks applications are the latest rage in town. In the past four years alone, neural networks have been used more than ever before in approximating complex real-world nonlinear phenomena. If you use email that automatically disables spam, if you use a voice recognition software to navigate around your city, or if you use google translator to snoop on other people&amp;#39;s conversations when in a foreign country, chances are high that you have used an application that is implementing a neural network in the backend. When NNs are combined in multiple layers, with each layer representing a level of abstraction, they are called deep neural networks. Deep networks allow compositional models made up of several simple processing layers with each layer transforming a complex representation at a higher, more absstract level to lower-level features. Broad applications of DNNs range from &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;image classification&lt;/a&gt;, &lt;a href=&quot;http://ieeexplore.ieee.org/document/6638947/?arnumber=6638947&quot;&gt;speech processing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1410.4615&quot;&gt;language models&lt;/a&gt; to &lt;a href=&quot;http://axon.cs.byu.edu/%7Emartinez/classes/678/Papers/Convolution_nets.pdf&quot;&gt;handwriting classification&lt;/a&gt;. They are increasingly finding applications to control problems. For example, &lt;a href=&quot;https://deepmind.com/research/alphago/&quot;&gt;AlphaGo&lt;/a&gt; beat world-record players in the difficult game of GO earlier this year. Even so in Atari games, they have proven to be able to achieve human-level control e.g. based on raw pixel values, or using raw sensory inputs to achieve advanced manipulation skills. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Deep networks allow compositional models made up of several simple processing layers that each transform a complex representation at a higher, more absstract level to lower-level features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The probing inquirer might ask, what makes (D)NNs so efficient at tasks that are otherwise difficult to solve in closed-form anyway? Well for one reason, (D)NNs in their most basic forms are an artificial representation of the pulse-frequencies found in neurons in the brain. They are the closest approximation to the giant supercomputer we humans have in our head (the human brain is estimated to have billions of neural circuit connections with back-coupled neurons and feedforward connections working together to help us understand our world). Imagine trying to mathematically formulate the factors of variation that dynamically capture how swarms of birds fly in different formations so that we can predict formation based on abstract formulae for different conditions (such as wind speed, humidity, light intensity et cetera) that affect flying behavior? It is a task that is difficult to manually formalize. Many problems in control have stochastic, high-dimensional dynamics that are highly nonlinear, usually POMDP ( in continuous processes). Analytical solutions that may be found may be too complicated to abstract into closed-form equations, building dynamical models that generalize to new contexts may be difficult, and the complexity of solving such high-dimensional spaced tasks may be intractable. Many researchers have thus resolved to devising learning algorithms that can &lt;i&gt;learn&lt;/i&gt; these complex patterns in nature by building neural network structures that learn high-dimensional nonlinear phenomena. When implemented in the various structural combinations for pattern recognition or identification problems as discussed in this paper, disregarding the time chartacteristics, (D)NN output units are similar to gnostic cells in the brain.&lt;/p&gt;

&lt;h3&gt;What are Dynamic Neural Networks?&lt;/h3&gt;

&lt;p&gt;&lt;b&gt;&lt;u&gt;Static neural networks&lt;/u&gt;&lt;/b&gt; are characterized by compact sets \(\mathbb{U}_i \subset \mathbb{R}^n \) being mapped into elements \(y_i \in \mathbb{R}^m \, \forall \, \,  (i = 1,2, \cdots, n) \) in the space of the output by a decision function \(\mathbb{F}\) (the elements of \(\mathbb{U}_i\)would be the input elements mapped to class \(y_i\) ). Static neural networks are used in pattern recognition problems. For a &lt;b&gt;&lt;u&gt;dynamical system&lt;/u&gt;&lt;/b&gt;, the operator $\mathbb{F}$ defines a plant based on an input-output pair of functions \({u(k), y(k)} \, \, \forall \, \, k \in [0, T]\). In either case, the objective is the same: finding the operator \(\hat{\mathbb{F}}\) that approximates the actual operator \(\mathbb{F}\) to a sufficient degree \(\epsilon &amp;gt; 0\), \(i.e.\ \),
\begin{align}
  \lVert \hat{y} - y \lVert = \lVert \hat{\mathbb{F}}(u) - \mathbb{F}(u) \lVert \le \epsilon, \qquad u \in \mathbb{U}
\end{align}&lt;/p&gt;

&lt;p&gt;where \(\lVert \cdot \lVert \) is an appropriately-chosen norm on the output space and \(\epsilon \) is a sufficiently positive number.&lt;/p&gt;

&lt;p&gt;For low-dimensional systems, finite linear combinations of the input space and a set of affine functions can uniformly approximate continuous functions of \(n\) real variables with support in the unit hypercube. Cybenko &lt;a href=&quot;https://www.dartmouth.edu/%7Egvc/Cybenko_MCSS.pdf&quot;&gt;developed theorems&lt;/a&gt; that showed networks with one internal layer and an arbitrary continuous sigmoidal function can approximate continuous functions with arbitrary precision, provided that there are no restraints placed on the number of nodes or the size of the network weights. In other words, a single hidden layer is sufficient as a universal function approximator with the ability to approximate any Borel measurable function from one finite dimensional space to another. In terms of representability by hidden layers in multilayer networks, it is a settled question that arbitrary decision regions are well &lt;i&gt;&lt;b&gt;approximated&lt;/b&gt;&lt;/i&gt; by continuous deep networks with continuous nonlinearity&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. The focus of deep networks is on approximated solutions and not exact solutions as Mathematicians would love to think.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It is a settled question that arbitrary decision regions in nonlinear systems are well &lt;i&gt;&lt;b&gt;approximated&lt;/b&gt;&lt;/i&gt; by continuous deep networks with continuous nonlinearity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When the dimension of the dataset increases, finding a learning algorithm that scales to the learning problem irrespective of translation and geometric irregularities in data, and identifies the underlying structure in spite of noise in the data becomes more complicated. Enter deep learning. Representing the complicated structure of high-dimensional data is what deep nets are good at doing mostly because (i) the presence of local minima in the loss function is not a hindrance to learning for gradient-based algorithms that deep learning algorithms use (e.g. Boltzman machines successfuly learn nonlinear phenomena), and (ii) backpropagation for gradient computation in multi-layered networks with continuous, differentiable, and bounded activation functions can learn nonlinear phenomena well &lt;a href=&quot;#fn2&quot; class=&quot;footnoteRef&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This blog post serves as an addendum to the paper I submitted to &lt;i&gt;American Control Conference (ACC) &lt;/i&gt; last week titled &lt;b&gt;&lt;i&gt;&lt;a href=&quot;http://bibbase.org/service/mendeley/ac6ea3d6-def2-3e3f-b8fd-4db9697c94d6/file/3570398e-42e5-bf5b-64cd-cee1cd135a15/2017-Nonlinear_Systems_Identification_Using_Deep_Dynamic_Neural_Networks.pdf.pdf&quot;&gt;Nonlinear Systems Identification Using Deep Dynamic Neural Networks&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;. Here I briefly go over the model structures I used, describe the datasets and expand a little more on the training procedure on the &lt;i&gt;Database for System Identification&lt;/i&gt; data that I used. The codes are indexed on &lt;a href=&quot;https://github.com/lakehanne/FARNN&quot;&gt;github&lt;/a&gt; and if you do not care about the theory behind my choices in training the respective networks I described in the paper, you are welcome to start trainng your network.&lt;/p&gt;

&lt;h3&gt;Datasets and preliminaries&lt;/h3&gt;

&lt;p&gt;Radiation therapy are by far the best procedure for the treatment of distributed cancer cells. They are effective at shrinking cancer tumor cells using high-precision localization of dosage targets to knock off tumor cells in the human body. Typically, the patient is treated with image-guided radiation therapy whereby a stereoscopic camera observes the patient while they are being treated with electron beams on a 6-DoF couch. The couch is adjusted by a radiation therapist during treatment to correct for involuntary motions and deviation of the patient&amp;#39;s position from planned target(s). The cancer RT treatment is often stopped during treatment to correct for motion deviations using the 6-DoF robot. But the robots used are highly rigid systems that produce just enough torques for motion correction but not enough articulation to account for the non-rigidity of the curvatures of the human body. In a recent work, we tested the ability of inflatable air bladders (IABs) to compensate for the inadequate actuations provided in motion alignment correction systems (e.g. 6-DoF robot couches) used in maskless head and neck cancer radiotherapy. The soft-robot&amp;#39;s precision was evaluated in the motion alignment and correction of these 6-DoF robots in a simulated cancer RT scenario.  We had a mannequin head lying in a supine position on a table that simulated our proposed motion alignment correction set-up during cancer RT. A soft-robot actuator in the form of an inflatable air bladder (IAB) moved the mannequin head based on supplied air pressure. This corrected for non-rigid motions during treatment. The IAB was actuated by current-driven proportional pneumatic valves; the experimental set-up is described in this &lt;a href=&quot;http://arxiv.org/abs/1506.04787&quot;&gt;CASE2015 Paper&lt;/a&gt;, but the change in head motion is now recorded by a more accurate motion capture (mocap) system instead of an RGB-D camera system. The mocap system is capable of measuring head position with less than 1 \(mm \) error. This is a SISO system with input as current (generated from pseudo-random binary sequences) in \(mm \) and outputs as head height in \(mm \). We collected \(10,070\) samples of input-output data offline, and in all experiments, we separate the dataset in a \( 60:40\% \) ratio for training and testing purposes. &lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
&lt;img src=&quot;/imgs/NNDyn/setup.png&quot; align=&quot;center&quot; height=&quot;500px&quot; /&gt;
&lt;div class=&quot;figcaption&quot; align=&quot;middle&quot;&gt;Fig. 1: Experimental Set-up for soft robot system.&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The main idea in system identification is to charcterize the behavior of a plant by an operator \( \mathbb{F} \) that resolves the dynamics of a plant based on input-output datapair \( {u_i(k), y_i(k)} \)  for all \(i = 1, 2, \cdots , N \). Previously, an LTI model generated from a &lt;i&gt;prediction error model&lt;/i&gt; was used in characterizing the system. This was insufficient to approximate the system nonlinearity. So here we propose NN-based Hammerstein block-structured models (Fig. 2).&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
&lt;center&gt;&lt;img src=&quot;/imgs/NNDyn/ham.png&quot;/&gt;&lt;/center&gt;
&lt;div class=&quot;figcaption&quot; align=&quot;center&quot;&gt;Fig 2: The Hammerstein block-structured model.&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In the Hammerstein model above, there are two block structures: one is a nonlinear element (the \(g(\cdot)\) block that models the dynamics from inputs to states while the second structure approximates the linear dynamics from states to the output space. I used deep recurrent network modules to model the nonlinear element while a linear multiulayer network models the \(G(z^{-1}) \) block.   &lt;/p&gt;

&lt;h4&gt;Multilayer Networks&lt;/h4&gt;

&lt;p&gt;These are the &lt;a href=&quot;https://www.researchgate.net/profile/Terrence_Sejnowski/publication/242509302_Learning_and_relearning_in_Boltzmann_machines/links/54a4b00f0cf256bf8bb327cc.pdf&quot;&gt;Rumelhart et al&amp;#39;s&lt;/a&gt; feedforward networks with forward connections only signals between adjoining units. The networks are composed of layers of input and output signals with one or more hidden layers embedded between them. A differentiable, continuous and bounded monotonically increasing function (such as the sigmoid function) is used as the output of each network node (or unit). The relationship between the input and output datapair is usually learned by the backpropagation algorithm (while this is not the only means of training a network, it seems to me by far the most efficient and popular method) . This algorithm uses gradient descent to modify the parameters and thresholds of the network to the end of minimizing the error between the predicted output and actual output of the network. For a detailed treatment of how the backpropagation algorithm works, I recommend &lt;a href=&quot;https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/&quot;&gt;Matt Mazur&amp;#39;s backpropagation example blog post&lt;/a&gt;. Multilayer networks have signal that flow in one direction only (possibly why they are often referred to as feedforward networks in literature). Introduced by Rumelhart-Williams-Hinton in the eighties, joining the weights and biases of the network completely parameterizes the system they are trained on. &lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
&lt;center&gt;&lt;img src=&quot;/imgs/NNDyn/mlp.png&quot; padding-right=&quot;20cm&quot;/&gt;&lt;/center&gt;
&lt;div class=&quot;figcaption&quot; align=&quot;middle&quot;&gt;Fig 3: A simple multilayer network with 2 inputs, 3 nodes in &lt;br&gt;the hidden layer and 2 outputs.&lt;/div&gt;
&lt;/div&gt;

&lt;h4&gt;Recurrent Networks&lt;/h4&gt;

&lt;p&gt;We can think of these as feedforward networks with tapped delay lines. Inspired by the behavior of cells in nature with asynchronous parallel processing i.e. content-addressable memory with phase-space flow of the state of a system &lt;a href=&quot;#fn3&quot; class=&quot;footnoteRef&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;., they have the ability to capture long-term information based on pieces of the information that is available to them. In complex systems composed of several subsystems that interact to yield collective intelligence, the nature of the collective properties is insensitive to the individual behavior of specific sub-elements. These are what RNNs are good at (non-Markovian control). For an expository introduction to Recurrent Neural Networks, I recommend going through Andrej Karpathy&amp;#39;s illuminating blog post on the &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;unreasonable effectiveness of recurrent networks&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;RNNs are the meat of this work and I dwell a little on their mathematical structure here. Assume the following text that is stored in memory, &lt;i&gt;&amp;quot;Funahashi, K. I. (1989). On the approximate realization of continuous mappings by neural networks.&amp;quot;&lt;/i&gt; A content addressable memory should be able to retrieve the entire memory given enough partial information such as &amp;quot;Funahashi, 1989&amp;quot;. It should deal with errors by obtaining this reference through a self-correcting process. Now suppose the text in memory is stochastic in nature such that the equations that describe the content being written to memory is a flow in state space making the model more difficult to describe. Therefore stability becomes relative. But if minimal changes are being made in the letters written into memory, the stochastic effects are small so that the core of local stable points in the phase space remain. To bring the point home, a system whose physical dynamics in phase space is dominated by a substantial amount of locally stable nodes to which it is attracted is in general a content-addressable memory.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
&lt;center&gt;&lt;img src=&quot;/imgs/NNDyn/rnn.png&quot; width=&quot;400px&quot; height=&quot;300px&quot;/&gt;&lt;/center&gt;
&lt;div class=&quot;figcaption&quot; align=&quot;middle&quot;&gt;Fig 4: A simple recurrent neural network.&lt;/div&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;a system whose physical dynamics in phase space is dominated by a substantial amount of locally stable nodes to which it is attarcted is generally referred to as content-addressable memory.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;From Fig. 5, there is a striking disimilarily to the multilayer network depicted ini Fig. 4: that is the self-feedback of neurons in the hidden layer. Through this recurrent nature, neurons in the hidden layer are able to randomly and asynchronously determine if they should fire or not, based on their thresholds. The strong back-coupling of RNNs alllow for modeling delays in discrete-time dynamical systems and asynchronously process information. With a single hidden-layer, for example, such a network can be described by &lt;/p&gt;

&lt;p&gt;\begin{align}
x(k+1) = f(x(k)), \qquad x(0) = x_o
\end{align}&lt;/p&gt;

&lt;p&gt;where \(f(\cdot) \) is a suitably chosen nonlinear, bounded continuous function. If the system under consideration is a continuous-time system, the dynamical system in the feedback path has a diagonal transfer matrix with elements \(1/(s + \alpha) \) along the diagonal &lt;a href=&quot;#fn4&quot; class=&quot;footnoteRef&quot; id=&quot;fnref4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. \(\alpha \) is an unknown parameter that parameterizes the lagged outputs; the system is then represented by the equation&lt;/p&gt;

&lt;p&gt;\begin{align}
\dot{x} = -\alpha x + f(x) + U
\end{align}&lt;/p&gt;

&lt;p&gt;where \( x(t) \in \mathbb{R}^n \) is the system&amp;#39;s state at time t, and the constant vector \(U \in \mathbb{R}^n \) is the input. The gradients&amp;#39; computation in a RNN are commonly computed through &lt;a href=&quot;http://www.cs.cmu.edu/%7Ebhiksha/courses/deeplearning/Fall.2013/pdfs/Werbos.backprop.pdf&quot;&gt;backpropagation thorugh time (BPTT)&lt;/a&gt;, using Real-Time Recurrent Learning (Robinson and Fallside 1987), or other sensitivity analysis methods (Billings, S.A.). &lt;/p&gt;

&lt;h5&gt;Long Short-Term Memory&lt;/h5&gt;

&lt;p&gt;While ordinary recurrent networks are good for finite-delays, when the sequence of recurrence goes back in time by a significant order (e.g.  \(&amp;gt; 500\)), the gradients can become intractable. For example, consider a network node whose output unit \(k\) at time \(t\) is \(y_k(t)\) so that the error computed for node \(k\) is &lt;/p&gt;

&lt;p&gt;\begin{align}
  \epsilon_k(t) = {\sigma&amp;#39;_k} \, [net_k(t))(\hat{y}^k(t) - y_k(t)]
\end{align}&lt;/p&gt;

&lt;p&gt;where 
\begin{align}
  net_i(t) = \sum\limits_{j}^{}w_{ij} \, y^j (t -1)
\end{align}&lt;/p&gt;

&lt;p&gt;and
\begin{align}
  y^i(t) = \sigma_i \, [net_i(t)]
\end{align}&lt;/p&gt;

&lt;p&gt;is the activation of an non-input node \(i\) with activation function \(\sigma_i\). The error signal of a non-output unit \(j\) that is backpropagated is (see Horchreiter&amp;#39;s analysis, 1997),&lt;/p&gt;

&lt;p&gt;\begin{align}
  e_j(t) = \sigma&amp;#39;_j[net_j(t)] \sum\limits_{i}^{}w_{ij} \, e_i (t+1)
\end{align}&lt;/p&gt;

&lt;p&gt;so that the contribution to \(w_{ji}\)&amp;#39;s weight update is \(\alpha e_j(t)y^l(t-1)\), where \(\alpha\) is the step size for the gradient descent function on a neuron in layer \(l\), that is connected to neuron \(j\). Now if the indices of a fully connected neuron range from \(1 \text{ to } n\), then the local error flow from a neuron unit \(u\) to \(v\) that is backpropagated in time for \(q\) time steps to a unit \(v\) is scaled as follows&lt;/p&gt;

&lt;p&gt;\begin{align}
  \dfrac{\partial e_v(t-q)}{\partial e_u(t)} &amp;amp;= \sigma&amp;#39;_v[net_v(t-1)]w_{uv} \qquad \text{ if } q = 1 \text{ or } 
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}    \label{eq:bptterr}
  \dfrac{\partial e_v(t-q)}{\partial e_u(t)} &amp;amp;= \sigma&amp;#39;_v[net_v(t-q)] \sum\limits_{l = 1}^{n} \dfrac{\partial e_l(t-q+1)}{\partial e_u(t)} w_{lv} \qquad \text{ if } q &amp;gt; 1
\end{align}&lt;/p&gt;

&lt;p&gt;If \(l_q = v \text{ and } l_0 = u \) is follows that&lt;/p&gt;

&lt;p&gt;\begin{align}      \label{eq:errorback}
    \dfrac{\partial e_v(t-q)}{\partial e_u(t)} = \sum\limits_{l_1 = 1}^{n} \cdots \sum\limits_{l_{q-1} = 1}^{m} \, \prod\limits_{m=1}^{q} \sigma&amp;#39;_{lm}[net_{lm}(t -m)] w_{l_m} l_{m-1}
\end{align}&lt;/p&gt;

&lt;p&gt;so that if \(|\sigma&amp;#39;_{lm}[net_{lm}(t -m)] w_{l_m} l_{m-1}| &amp;gt; 1.0 \text{ for all} m\), then the product term in \eqref{eq:errorback} is intractable (causing the errors that flow back in time from u to v to be too large). Similarly, if \(|\sigma&amp;#39;_{lm}[net_{lm}(t -m)] w_{l_m} l_{m-1}| &amp;lt; 1.0 \text{ for all } m\), then the product term exponentially decreases with the degree of \(q\) so thatthe error vanishes before reaching \(u\). These two conditions make learning difficult within a reasonable time-frame because (i) the error signals that are backpropagated in time can become infinitely high so that the gradients explode causing oscillating weights in the network or (ii) the errors being temporally backpropagated  could be so small that they cause gradients to vanish so that it becomes a lot more complex to compute gradients with slowly-varying network parameters.&lt;/p&gt;

&lt;p&gt;Because of these issues with remembering long-term contextualization of inputs, RNNs have undergone different transformations in their basic architecture since the first &lt;a href=&quot;https://en.wikipedia.org/wiki/Hopfield_network&quot;&gt;Hopfield Net&lt;/a&gt;. Recent works have sought to improve the vanishing and exploding gradients problem. Of important use to this work are Horchreiter&amp;#39;s &lt;a href=&quot;http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf&quot;&gt;Long Short Term Memory&lt;/a&gt; and Cho&amp;#39;s &lt;a href=&quot;http://arxiv.org/abs/1406.1078&quot;&gt;Gated Recurrent Units&lt;/a&gt; (LSTM). The LSTM network protects error flow from stochastic perturbations by employing &lt;i&gt;multiplicative units&lt;/i&gt; that enforces constant error flows and solves long time-lag issues. For a weight matrix \(W\), the LSTM takes \(O(W)\) operations &lt;a href=&quot;#fn5&quot; class=&quot;footnoteRef&quot; id=&quot;fnref5&quot;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h5&gt;Gated Recurrent Units&lt;/h5&gt;

&lt;p&gt;Like LSTMs, &lt;b&gt;GRUs&lt;/b&gt; were invented to solve the long-term contextualization of delayed signals, albeit with lesser computational complexity compared against LSTMs. The GRU consists of two types of RNN (an encoder and a decoder) acting separately transforming the input sequence into hidden states that are computed in different fashions to maximize the conditional log likelihood &lt;/p&gt;

&lt;p&gt;\begin{align}
  \text{max}_{\theta} \dfrac{1}{N} \sum\limits_{n=1}^{N}log p_\theta (y_n | x_n)
\end{align}&lt;/p&gt;

&lt;p&gt;where \(n\) is the lenth of the sequence, \(\theta\) is the vector of model parameters, and \(p_\theta\) = \(p(y_1, \cdots, y_n | x_1, \cdots, x_n)\). For an input sequence \(\textbf{x} = (x_1, \cdots, x_n)\), the hidden state of the encoder is generated as a nonlinear function represented like so&lt;/p&gt;

&lt;p&gt;\begin{align}
  h_t = f(h_{t-1}, u_t)
\end{align}&lt;/p&gt;

&lt;p&gt;such that the RNN can predict the next element in the sequence by learning the probability distribution over the input sequence. The output at each time step is the conditional distribution \(p(x_k = 1| x_1, \cdots, x_n\). For a \(1-of-K\) encoding, the probability output can be computed for the entire sequence using &lt;/p&gt;

&lt;p&gt;\begin{align}
p(x) = \prod\limits_{k = 1}^{K}p(x_k|x_{k-1}, \cdots , x_1).
\end{align}&lt;/p&gt;

&lt;p&gt;The encoder reads the input sequence and updates the hidden state of the RNN as a summary \(c\) for the whole sequence.&lt;/p&gt;

&lt;p&gt;The decoder uses another simple RNN by predicting the next output sequence \(y_k\) given the hidden state, \(h_k\) but \(y_k\) and \(h_k\) are in turn conditioned on \(y_{k-1}\) and the generated summary from the input sequence, \(\textbf{c}\). In other words, &lt;/p&gt;

&lt;p&gt;\begin{align}
  h_{k} = f(h_{k-1}, y_{k-1}, \textbf{c})
\end{align}&lt;/p&gt;

&lt;p&gt;Again, the conditional distribution of \(y_k\) is given by &lt;/p&gt;

&lt;p&gt;\begin{align}
P(y_k | y_{k-1}, \cdots, y_k, \textbf{c}) = g(\textbf{h_k}, y_{k-1}, \textbf{c})
\end{align}&lt;/p&gt;

&lt;p&gt;We can imagine the encoder-decoder as jointly maximizing the conditional log-likelihood &lt;/p&gt;

&lt;p&gt;\begin{align}
max_\theta \dfrac{1}{N}\sum\limits_{n=1}^{N} log p_\theta(y_n|x_n)
\end{align}&lt;/p&gt;

&lt;p&gt;The hidden unit is composed of two critical gates, as is the case for the LSTM, that adaptively forgets or remebers a long-range sequence. The reset gate is given by &lt;/p&gt;

&lt;p&gt;\begin{align}
r_j = \sigma\left([\textbf{W}_r\textbf{x}]_j + [\textbf{U}_r \textbf{h}_{k-1}]_j\right)
\end{align}&lt;/p&gt;

&lt;p&gt;with \(\sigma\) being the logistic sigmoid function and \([\cdot]_j\) being the \(j-th\) element of the vector. The update gate is computed as&lt;/p&gt;

&lt;p&gt;\begin{align}&lt;br&gt;
  z_j = \sigma\left([\textbf{W}\textbf{x}]_j + [\textbf{U}(\textbf{r} \odot \textbf{h}_{t-1} )]_j\right)
\end{align}&lt;/p&gt;

&lt;p&gt;The activation of the hidden unit \(h_j\) is determined through&lt;/p&gt;

&lt;p&gt;\begin{align}
h_j^(k) = z_j h_j ^ {k -1} + (1 - z_j) \bar{h_j^k}
\end{align}&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;\begin{align}   \label{eq:hidden}
\bar{h_j^k} = \phi\left([\textbf{Wx}_j + [\textbf{U}(\textbf{r \odot \textbf{h}_{t-1}]_j\right)
\end{align}&lt;/p&gt;

&lt;p&gt;Examining \autoref{eq:hidden}, we see that the hidden state ignores past signals that are not relevant to future evolution of the system when the reset gate is 0. The update gate determines how much information from the past hidden states to carry over into the current hidden state (similar to the memory cell in the LSTM). GRUs thus allow a more compact representation of long-term dependency in RNN&amp;#39;s without exposing the RNN state to the exploding or vanishing gradients problem.&lt;/p&gt;

&lt;p&gt;...continued in &lt;a href=&quot;https://lakehanne.github.io/Deep-Nets-Identification-2&quot;&gt;post 2&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Acknowledgement&lt;/h3&gt;

&lt;p&gt;I am grateful to the maintainers of the Torch package which has helped in accelerating the training my models.
I am also grateful to &lt;a href=&quot;https://twitter.com/hugo_larochelle&quot;&gt;Hugo Larochelle&lt;/a&gt; and &lt;a href=&quot;https://cbmm.mit.edu/about/people/kulkarni&quot;&gt;Tejas Kulkarni&lt;/a&gt; for pointing me in the direction of best practices for regularizing the layers of a recurrent neural network during training.  &lt;/p&gt;

&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr&gt;
&lt;h4&gt;References&lt;/h4&gt; 
&lt;ul&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt; Cybenko, G. (1993). Approximation by Superpositions of a Sigmoidal Function. Approximation Theory and Its Applications, 9(3), 17–28. http://doi.org/10.1007/BF02836480&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;
LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2323. http://doi.org/10.1109/5.726791
&lt;a href=&quot;#fnref2&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;
Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences of the United States of America, 79(8), 2554–2558. http://doi.org/10.1073/pnas.79.8.2554
&lt;a href=&quot;#fnref3&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
&lt;li id=&quot;fn4&quot;&gt;&lt;p&gt;
Narendra, K. S. (1992). Identification and Control of Dynamical Systems Using Neural Networks.
&lt;a href=&quot;#fnref4&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul&gt;
&lt;li id=&quot;fn5&quot;&gt;&lt;p&gt;
Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–80. http://doi.org/10.1162/neco.1997.9.8.1735
&lt;a href=&quot;#fnref5&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/section&gt;
</description>
        <pubDate>2016-09-30 07:21:00 -0500</pubDate>
        <link>lakehanne.github.ioDeep-Nets-Identification</link>
        <guid isPermaLink="true">lakehanne.github.ioDeep-Nets-Identification</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Cloning specific folders from git&lt;/center&gt;</title>
        <description>&lt;!--##Table of Contents
###[Sparse Checkout](#sparse-checkout)
###[SVN Checkout](#svn-checkout)
## Directory &amp;&amp; Sub-directory checkout from git repos--&gt;

&lt;p&gt;This is a relatively newie but a goodie.  Have you ever been stuck trying to clone specific folders from a &lt;code&gt;git&lt;/code&gt; repo recently?&lt;/p&gt;

&lt;p&gt;Well, starting from &lt;code&gt;git 1.9&lt;/code&gt;, this feature is now part of git features. For the sake of illustration, let&amp;#39;s say we want to retrieve only the &lt;a href=&quot;https://github.com/PointCloudLibrary/pcl/tree/master/examples&quot;&gt;examples&lt;/a&gt; directory of the point cloud repo commited at the &lt;a href=&quot;https://github.com/PointCloudLibrary&quot;&gt;point cloud library git page&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Sparse-Checkout&lt;/h3&gt;

&lt;p&gt;A new feature called &lt;a href=&quot;https://git-scm.com/docs/git-read-tree/&quot;&gt;sparse checkout&lt;/a&gt; allows us to sparsely populate the working directory by using skip-worktree bit to inform &lt;code&gt;GIT&lt;/code&gt; if the file in the working directory deserves a look. &lt;i&gt;git read-tree&lt;/i&gt; and other merge commands native to &lt;code&gt;git&lt;/code&gt; such as &lt;code&gt;checkout&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, or &lt;code&gt;pull&lt;/code&gt; are useful in maintaining the skip-working tree bitmap and working directory update. &lt;/p&gt;

&lt;p&gt;A paraphrased quote from the manual here:&lt;/p&gt;

&lt;p&gt;&amp;quot;&lt;code&gt;$GIT_DIR/info/sparse-checkout&lt;/code&gt; defines the skip-worktree reference bitmap. When git read-tree needs to update the `working directory, it resets the skip-worktree bit in the index based on this file, which uses the same syntax as .gitignore files. If an entry matches a pattern in this file, skip-worktree will not be set on that entry. Otherwise, skip-worktree will be set.&lt;/p&gt;

&lt;p&gt;Then it compares the new skip-worktree value with the previous one. If skip-worktree turns from set to unset, it will add the corresponding file back. If it turns from unset to set, that file will be removed.&lt;/p&gt;

&lt;p&gt;&amp;quot;While &lt;code&gt;$GIT_DIR/info/sparse-checkout&lt;/code&gt; is usually used to specify what files are in, you can also specify what files are not in, using negate patterns. For example, to remove the file unwanted:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
/*
!unwanted
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Another tricky thing is fully repopulating the working directory when you no longer want sparse checkout. You cannot just disable &amp;quot;sparse checkout&amp;quot; because skip-worktree bits are still in the index and your working directory is still sparsely populated. You should re-populate the working directory with the $GIT_DIR/info/sparse-checkout file content as follows:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
/*
&lt;/code&gt;
&amp;quot;&lt;/p&gt;

&lt;p&gt;So to check out the pcl examples directory for example, we could combine the &lt;code&gt;sparse checkout&lt;/code&gt; and &lt;code&gt;shallow clone&lt;/code&gt; features. By using the &lt;code&gt;shallow clone&lt;/code&gt; feature, we cut off the history and the &lt;code&gt;sparse check out&lt;/code&gt; only pulls files matching the pattern(s) we specify. &lt;/p&gt;

&lt;p&gt;Take a look at the following example:
&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;
$ mkdir pcl-examples
$ cd pcl-examples                               #make a directory we want to copy folders to
$ git init                                      #initialize the empty local repo
$ git remote add origin -f https://github.com/PointCloudLibrary.git     #add the remote origin
$ git config core.sparsecheckout true           #very crucial. this is where we tell git we are checking out specifics
$ echo &amp;quot;examples/*&amp;quot; &amp;gt;&amp;gt; .git/info/sparse-checkout #recursively checkout examples folder
$ git pull --depth=2 origin master          #go only 2 depths down the examples directory
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;

&lt;!--
#### Explanation

The line 
                  &lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$git remote add origin -f https://github.com/PointCloudLibrary.git &lt;/code&gt;&lt;/pre&gt; 

creates an empty repository with the point cloud library github remote, fetches all objects but doesn't check them out.

The files are checked out with the next line's command.

Since we are cloning everything in the examples directory --which, by the way, have a depth of 2-- we pull every darn subdirectory and file under the examples folder by doing:

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ git pull --depth=2 origin master&lt;/code&gt;&lt;/pre&gt; 
--&gt;

&lt;h3&gt;SVN Checkout&lt;/h3&gt;

&lt;p&gt;If you are using &lt;code&gt;svn&lt;/code&gt; instead of git, there is a straightforward way to do this. Simply cd into your working directory and replace the &amp;quot;&lt;code&gt;/tree/master&lt;/code&gt;&amp;quot; path within the &lt;code&gt;url&lt;/code&gt; with &lt;code&gt;trunk&lt;/code&gt;. To clone the subdirectory &lt;code&gt;examples&lt;/code&gt; in the point cloud git repo for example, using &lt;code&gt;svn&lt;/code&gt;, we would do the following in terminal&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ svn checkout https://github.com/PointCloudLibrary/pcl/trunk/examples&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>2016-03-28 07:23:00 -0500</pubDate>
        <link>lakehanne.github.iogit-sparse-checkout</link>
        <guid isPermaLink="true">lakehanne.github.iogit-sparse-checkout</guid>
        
        
        <category>lakehanne.github.io</category>
        
      </item>
    
      <item>
        <title>&lt;center&gt;Cloning specific folders from git&lt;/center&gt;</title>
        <description>&lt;!--##Table of Contents
###[Sparse Checkout](#sparse-checkout)
###[SVN Checkout](#svn-checkout)
## Directory &amp;&amp; Sub-directory checkout from git repos--&gt;

&lt;p&gt;This is a relatively newie but a goodie.  Have you ever been stuck trying to clone specific folders from a &lt;code&gt;git&lt;/code&gt; repo recently?&lt;/p&gt;

&lt;p&gt;Well, starting from &lt;code&gt;git 1.9&lt;/code&gt;, this feature is now part of git features. For the sake of illustration, let&amp;#39;s say we want to retrieve only the &lt;a href=&quot;https://github.com/PointCloudLibrary/pcl/tree/master/examples&quot;&gt;examples&lt;/a&gt; directory of the point cloud repo commited at the &lt;a href=&quot;https://github.com/PointCloudLibrary&quot;&gt;point cloud library git page&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Sparse-Checkout&lt;/h3&gt;

&lt;p&gt;A new feature called &lt;a href=&quot;https://git-scm.com/docs/git-read-tree/&quot;&gt;sparse checkout&lt;/a&gt; allows us to sparsely populate the working directory by using skip-worktree bit to inform &lt;code&gt;GIT&lt;/code&gt; if the file in the working directory deserves a look. &lt;i&gt;git read-tree&lt;/i&gt; and other merge commands native to &lt;code&gt;git&lt;/code&gt; such as &lt;code&gt;checkout&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, or &lt;code&gt;pull&lt;/code&gt; are useful in maintaining the skip-working tree bitmap and working directory update. &lt;/p&gt;

&lt;p&gt;A paraphrased quote from the manual here:&lt;/p&gt;

&lt;p&gt;&amp;quot;&lt;code&gt;$GIT_DIR/info/sparse-checkout&lt;/code&gt; defines the skip-worktree reference bitmap. When git read-tree needs to update the `working directory, it resets the skip-worktree bit in the index based on this file, which uses the same syntax as .gitignore files. If an entry matches a pattern in this file, skip-worktree will not be set on that entry. Otherwise, skip-worktree will be set.&lt;/p&gt;

&lt;p&gt;Then it compares the new skip-worktree value with the previous one. If skip-worktree turns from set to unset, it will add the corresponding file back. If it turns from unset to set, that file will be removed.&lt;/p&gt;

&lt;p&gt;&amp;quot;While &lt;code&gt;$GIT_DIR/info/sparse-checkout&lt;/code&gt; is usually used to specify what files are in, you can also specify what files are not in, using negate patterns. For example, to remove the file unwanted:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
/*
!unwanted
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Another tricky thing is fully repopulating the working directory when you no longer want sparse checkout. You cannot just disable &amp;quot;sparse checkout&amp;quot; because skip-worktree bits are still in the index and your working directory is still sparsely populated. You should re-populate the working directory with the $GIT_DIR/info/sparse-checkout file content as follows:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
/*
&lt;/code&gt;
&amp;quot;&lt;/p&gt;

&lt;p&gt;So to check out the pcl examples directory for example, we could combine the &lt;code&gt;sparse checkout&lt;/code&gt; and &lt;code&gt;shallow clone&lt;/code&gt; features. By using the &lt;code&gt;shallow clone&lt;/code&gt; feature, we cut off the history and the &lt;code&gt;sparse check out&lt;/code&gt; only pulls files matching the pattern(s) we specify. &lt;/p&gt;

&lt;p&gt;Take a look at the following example:
&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;
$ mkdir pcl-examples
$ cd pcl-examples                               #make a directory we want to copy folders to
$ git init                                      #initialize the empty local repo
$ git remote add origin -f https://github.com/PointCloudLibrary.git     #add the remote origin
$ git config core.sparsecheckout true           #very crucial. this is where we tell git we are checking out specifics
$ echo &amp;quot;examples/*&amp;quot; &amp;gt;&amp;gt; .git/info/sparse-checkout #recursively checkout examples folder
$ git pull --depth=2 origin master          #go only 2 depths down the examples directory
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;

&lt;!--
#### Explanation

The line 
                  &lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$git remote add origin -f https://github.com/PointCloudLibrary.git &lt;/code&gt;&lt;/pre&gt; 

creates an empty repository with the point cloud library github remote, fetches all objects but doesn't check them out.

The files are checked out with the next line's command.

Since we are cloning everything in the examples directory --which, by the way, have a depth of 2-- we pull every darn subdirectory and file under the examples folder by doing:

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ git pull --depth=2 origin master&lt;/code&gt;&lt;/pre&gt; 
--&gt;

&lt;h3&gt;SVN Checkout&lt;/h3&gt;

&lt;p&gt;If you are using &lt;code&gt;svn&lt;/code&gt; instead of git, there is a straightforward way to do this. Simply cd into your working directory and replace the &amp;quot;&lt;code&gt;/tree/master&lt;/code&gt;&amp;quot; path within the &lt;code&gt;url&lt;/code&gt; with &lt;code&gt;trunk&lt;/code&gt;. To clone the subdirectory &lt;code&gt;examples&lt;/code&gt; in the point cloud git repo for example, using &lt;code&gt;svn&lt;/code&gt;, we would do the following in terminal&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ svn checkout https://github.com/PointCloudLibrary/pcl/trunk/examples&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>2016-03-28 07:23:00 -0500</pubDate>
        <link>lakehanne.github.iogit-sparse-checkout</link>
        <guid isPermaLink="true">lakehanne.github.iogit-sparse-checkout</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Neural Nets Talk&lt;/center&gt;.</title>
        <description>&lt;!-- Analytics --&gt;

&lt;script&gt;
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-64680332-1', 'auto');
  ga('send', 'pageview');
  ga('send', 'pageview' '/neuraltalk');

&lt;/script&gt;

&lt;p&gt;So I have a lot of free time on my hands this long Thanksgiving weekend. Beside spending time with family and friends, I&amp;#39;ve had to catch up with a few papers on my &lt;code&gt;mendeley database&lt;/code&gt; which I had been postponing to read all along and try out some new &lt;code&gt;deep learning&lt;/code&gt; stuff. Having long wanted to try out Andrej Karpathy&amp;#39;s &lt;a href=&quot;https://github.com/karpathy/neuraltalk&quot;&gt;Neural Talk&lt;/a&gt; &lt;code&gt;recurrent nets&lt;/code&gt; but being unable to due to time constraints, I decided to try out the 0.1 Torch 7 version released nine days ago (&lt;a href=&quot;https://github.com/karpathy/neuraltalk2&quot;&gt;neural talk 2&lt;/a&gt;) based on the &lt;a href=&quot;http://torch.ch/&quot;&gt;Torch&lt;/a&gt; library which is faster and more efficient than the original neural talk. &lt;/p&gt;

&lt;p&gt;Since the release, people have tried out the codes with interesting results such as this funny &lt;a href=&quot;https://vimeo.com/146492001&quot;&gt;neural talk and walk video&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/kcimc&quot;&gt;@kcimc&lt;/a&gt;. Well, what better time to fool around with &amp;quot;automagic&amp;quot; image captioning on my old camera&amp;#39;s sd-card images than when on a holiday at a friend&amp;#39;s?&lt;/p&gt;

&lt;p&gt;Using a mobile laptop running an NVIDIA &lt;code&gt;K1100M GPU&lt;/code&gt;, I updated my &lt;a href=&quot;http://torch.ch/docs/getting-started.html&quot;&gt;torch7&lt;/a&gt; installation and the accompanying &lt;a href=&quot;https://luarocks.org/&quot;&gt;luarocks&lt;/a&gt; dependencies (which include &lt;code&gt;cudnn&lt;/code&gt;, &lt;code&gt;cunn&lt;/code&gt;, &lt;code&gt;cutorch&lt;/code&gt;, &lt;code&gt;image&lt;/code&gt;, &lt;code&gt;nn&lt;/code&gt;, and &lt;code&gt;nngraph&lt;/code&gt;), before I fired up the &lt;code&gt;eval.lua&lt;/code&gt; file on a bunch of images I had on my old camera. &lt;/p&gt;

&lt;p&gt;While it&amp;#39;s far from perfect in prediction (such as saying I was in a suit and tie in front of a kitchen counter when I was actually in front of an hotel in London), I was nonetheless amazed at the power and efficiency of &lt;code&gt;RNN&amp;#39;s&lt;/code&gt;. The pictures below demonstrate the automatic captioning on random images I evaluated the code on.&lt;/p&gt;

&lt;p&gt;If after looking through the goodness of the predictions and your &amp;#39;nerdgasms&amp;#39; are not fired up, I would strongly recommend you seeing a psychiatrist. :)! Can you imagine the possibilities of this in suppy-chain robotics? Or some sort of pick and place tasks for autonomous robots? Neural nets are really interesting and quite efficient at real-world approximations and recontruction. I&amp;#39;ll be posting more results on other interesting work I intend to use this for in the coming few days.&lt;/p&gt;

&lt;p&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;
    &lt;meta charset=&quot;utf-8&quot;&gt;
    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot;&gt;
    &lt;title&gt;neuraltest&lt;/title&gt;
    &lt;script src=&quot;/downloads/neuraltest_files/jquery-1.8.3.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;/downloads/neuraltest_files/d3.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
    &lt;style&gt;
    body {
      color: #333;
      margin: 0;
      padding: 0;
      font-family: &amp;quot;HelveticaNeue-Light&amp;quot;, &amp;quot;Helvetica Neue Light&amp;quot;, &amp;quot;Helvetica Neue&amp;quot;, Helvetica, Arial, &amp;quot;Lucida Grande&amp;quot;, sans-serif; 
      font-weight: 300;
    }
    #wrap {
      margin: 10px;
    }
    .result {
      width:300px;
      margin:10px;
      display: inline-block;
      vertical-align: top;
    }
    .result img {
      width: 100%;
    }
    &lt;/style&gt;&lt;/p&gt;

&lt;p&gt;&lt;/script&gt;
  &lt;/head&gt;
  &lt;body onload=&quot;start()&quot;&gt;
    &lt;div id=&quot;wrap&quot;&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img1.jpg&quot;&gt;&lt;div&gt;a close up of a person holding a red apple&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img2.jpg&quot;&gt;&lt;div&gt;a street sign on the side of the road&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img3.jpg&quot;&gt;&lt;div&gt;a man standing in front of a train station&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img4.jpg&quot;&gt;&lt;div&gt;a group of people standing in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img5.jpg&quot;&gt;&lt;div&gt;a man standing in front of a kitchen counter&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img6.jpg&quot;&gt;&lt;div&gt;a man in a suit and tie standing in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img7.jpg&quot;&gt;&lt;div&gt;a television is on a desk with a computer&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img8.jpg&quot;&gt;&lt;div&gt;a group of people standing around a large room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img9.jpg&quot;&gt;&lt;div&gt;a bedroom with a bed and a desk&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img10.jpg&quot;&gt;&lt;div&gt;a car is driving down a road with a traffic light&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img11.jpg&quot;&gt;&lt;div&gt;a man is standing in front of a tv&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img12.jpg&quot;&gt;&lt;div&gt;a man riding a skateboard down a street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img13.jpg&quot;&gt;&lt;div&gt;a group of people walking down a street&lt;/div&gt;&lt;/div&gt;
    &lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img14.jpg&quot;&gt;&lt;div&gt;a large body of water with a boat on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img15.jpg&quot;&gt;&lt;div&gt;a large building with a clock on the side of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img16.jpg&quot;&gt;&lt;div&gt;a person standing in a kitchen with a refrigerator&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img17.jpg&quot;&gt;&lt;div&gt;a man is flying a kite in a field&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img18.jpg&quot;&gt;&lt;div&gt;a group of people sitting around a table eating food&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img19.jpg&quot;&gt;&lt;div&gt;a dog is laying on the floor next to a window&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img20.jpg&quot;&gt;&lt;div&gt;a person is taking a picture of a boat in the water&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img21.jpg&quot;&gt;&lt;div&gt;a sign that says UNK UNK UNK on the side&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img22.jpg&quot;&gt;&lt;div&gt;a view of a building with a window in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img23.jpg&quot;&gt;&lt;div&gt;a plate of food with a fork on a table&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img24.jpg&quot;&gt;&lt;div&gt;a woman is walking down the street with a suitcase&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img25.jpg&quot;&gt;&lt;div&gt;a man standing in front of a subway train&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img26.jpg&quot;&gt;&lt;div&gt;a man in a suit and tie standing in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img27.jpg&quot;&gt;&lt;div&gt;a crowd of people standing around a large group of people&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img28.jpg&quot;&gt;&lt;div&gt;a view of a city street with a traffic light&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img29.jpg&quot;&gt;&lt;div&gt;a group of people in a field with a kite&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img30.jpg&quot;&gt;&lt;div&gt;a man standing in front of a mirror with a toothbrush&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img31.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img32.jpg&quot;&gt;&lt;div&gt;a man in a suit and tie standing in a store&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img33.jpg&quot;&gt;&lt;div&gt;a group of people walking down a street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img34.jpg&quot;&gt;&lt;div&gt;a person standing in a room with a suitcase&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img35.jpg&quot;&gt;&lt;div&gt;a crowd of people standing around a large plane&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img36.jpg&quot;&gt;&lt;div&gt;a man and a woman standing in front of a wine glass&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img37.jpg&quot;&gt;&lt;div&gt;a group of people sitting in a living room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img38.jpg&quot;&gt;&lt;div&gt;a view of a park with a bench and a tree&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img39.jpg&quot;&gt;&lt;div&gt;a group of people playing a game of frisbee&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img40.jpg&quot;&gt;&lt;div&gt;a refrigerator with a lot of magnets on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img41.jpg&quot;&gt;&lt;div&gt;a group of people standing on a sidewalk&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img42.jpg&quot;&gt;&lt;div&gt;a person standing in front of a window&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img43.jpg&quot;&gt;&lt;div&gt;a view of a river with a boat in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img44.jpg&quot;&gt;&lt;div&gt;a cat is sitting on a bathroom sink&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img45.jpg&quot;&gt;&lt;div&gt;a man standing in front of a crowd of people&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img46.jpg&quot;&gt;&lt;div&gt;a view of a park with a bench in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img47.jpg&quot;&gt;&lt;div&gt;a group of people sitting around a table&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img48.jpg&quot;&gt;&lt;div&gt;a car is parked on the side of the street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img49.jpg&quot;&gt;&lt;div&gt;a view of a bathroom with a sink and a mirror&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img50.jpg&quot;&gt;&lt;div&gt;a man standing in front of a group of people&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img51.jpg&quot;&gt;&lt;div&gt;a young boy wearing a tie and a tie&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img52.jpg&quot;&gt;&lt;div&gt;a view of a traffic light and a building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img53.jpg&quot;&gt;&lt;div&gt;a traffic light with a red light on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img54.jpg&quot;&gt;&lt;div&gt;a crowd of people standing around a large building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img55.jpg&quot;&gt;&lt;div&gt;a car parked on the side of the road&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img56.jpg&quot;&gt;&lt;div&gt;a living room with a tv and a tv&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img57.jpg&quot;&gt;&lt;div&gt;a view of a city bus at a stop light&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img58.jpg&quot;&gt;&lt;div&gt;a woman is standing in front of a mirror&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img59.jpg&quot;&gt;&lt;div&gt;a group of people standing on a street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img60.jpg&quot;&gt;&lt;div&gt;a room filled with lots of furniture and a tv&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img61.jpg&quot;&gt;&lt;div&gt;a view of a train station with a train station&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img62.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img63.jpg&quot;&gt;&lt;div&gt;a large building with a clock tower on top of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img64.jpg&quot;&gt;&lt;div&gt;a man is standing in front of a large mirror&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img65.jpg&quot;&gt;&lt;div&gt;a man is holding a frisbee in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img66.jpg&quot;&gt;&lt;div&gt;a city street with a clock tower in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img67.jpg&quot;&gt;&lt;div&gt;a kitchen with a sink and a refrigerator&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img68.jpg&quot;&gt;&lt;div&gt;a woman holding a pair of scissors in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img69.jpg&quot;&gt;&lt;div&gt;a clock on a tree in a park&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img70.jpg&quot;&gt;&lt;div&gt;a cat sitting on a table next to a pair of scissors&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img71.jpg&quot;&gt;&lt;div&gt;a man is holding a cell phone in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img72.jpg&quot;&gt;&lt;div&gt;a man and a woman sitting on a couch playing a video game&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img73.jpg&quot;&gt;&lt;div&gt;a tree in the middle of a city street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img74.jpg&quot;&gt;&lt;div&gt;a man is holding a glass of wine&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img75.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img76.jpg&quot;&gt;&lt;div&gt;a man and a woman sitting at a table with laptops&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img77.jpg&quot;&gt;&lt;div&gt;a train station with a train on the tracks&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img78.jpg&quot;&gt;&lt;div&gt;a view of a kitchen with a window in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img79.jpg&quot;&gt;&lt;div&gt;a large jetliner sitting on top of an airport tarmac&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img80.jpg&quot;&gt;&lt;div&gt;a view of a boat in the water&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img81.jpg&quot;&gt;&lt;div&gt;a street with a lot of cars on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img82.jpg&quot;&gt;&lt;div&gt;a woman standing in a room with a remote&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img83.jpg&quot;&gt;&lt;div&gt;a group of people standing in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img84.jpg&quot;&gt;&lt;div&gt;a woman is standing in a bathroom with a toilet&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img85.jpg&quot;&gt;&lt;div&gt;a person is walking on a dirt road&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img86.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img87.jpg&quot;&gt;&lt;div&gt;a city street filled with lots of traffic&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img88.jpg&quot;&gt;&lt;div&gt;a person standing in front of a window with a clock on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img89.jpg&quot;&gt;&lt;div&gt;a close up of a keyboard on a table&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img90.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img91.jpg&quot;&gt;&lt;div&gt;a man flying through the air while riding a snowboard&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img92.jpg&quot;&gt;&lt;div&gt;a man holding a cell phone in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img93.jpg&quot;&gt;&lt;div&gt;a man sitting on a toilet in a bathroom&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img94.jpg&quot;&gt;&lt;div&gt;a group of people sitting around a table eating food&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img95.jpg&quot;&gt;&lt;div&gt;a view of a kitchen with a refrigerator and a stove&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img96.jpg&quot;&gt;&lt;div&gt;a car driving down a street next to a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img97.jpg&quot;&gt;&lt;div&gt;a person standing in front of a refrigerator&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img98.jpg&quot;&gt;&lt;div&gt;a man standing in front of a train station&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img99.jpg&quot;&gt;&lt;div&gt;a boat that is sitting in the water&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img100.jpg&quot;&gt;&lt;div&gt;a cat is sitting on the floor in a bathroom&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img101.jpg&quot;&gt;&lt;div&gt;a man is holding a wii remote in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img102.jpg&quot;&gt;&lt;div&gt;a group of people standing on top of a lush green field&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img103.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img104.jpg&quot;&gt;&lt;div&gt;a traffic light on a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img105.jpg&quot;&gt;&lt;div&gt;a group of people standing on a platform&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img106.jpg&quot;&gt;&lt;div&gt;a red and yellow train on the tracks&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img107.jpg&quot;&gt;&lt;div&gt;a picture of a person in a mirror&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img108.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img109.jpg&quot;&gt;&lt;div&gt;a traffic light on a pole on a city street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img110.jpg&quot;&gt;&lt;div&gt;a traffic light on a pole on a city street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img111.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img112.jpg&quot;&gt;&lt;div&gt;a kitchen with a stove and a sink&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img113.jpg&quot;&gt;&lt;div&gt;a person standing in front of a window with a clock on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img114.jpg&quot;&gt;&lt;div&gt;a car parked on the side of the road&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img115.jpg&quot;&gt;&lt;div&gt;a picture of a man in a suit and tie&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img116.jpg&quot;&gt;&lt;div&gt;a large jetliner sitting on top of an airport runway&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img117.jpg&quot;&gt;&lt;div&gt;a city street at night with a traffic light&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img118.jpg&quot;&gt;&lt;div&gt;a red and white car parked in a parking lot&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img119.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img120.jpg&quot;&gt;&lt;div&gt;a young boy is sitting on a bench&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img121.jpg&quot;&gt;&lt;div&gt;a city street with a large building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img122.jpg&quot;&gt;&lt;div&gt;a building with a clock on the side of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img123.jpg&quot;&gt;&lt;div&gt;a woman holding a cell phone in her hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img124.jpg&quot;&gt;&lt;div&gt;a table with a vase of flowers on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img125.jpg&quot;&gt;&lt;div&gt;a group of people standing in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img126.jpg&quot;&gt;&lt;div&gt;a man is standing in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img127.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img128.jpg&quot;&gt;&lt;div&gt;a cat is sitting on the floor in front of a tv&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img129.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img130.jpg&quot;&gt;&lt;div&gt;a bathroom with a sink and a toilet&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img131.jpg&quot;&gt;&lt;div&gt;a view of a city street from a window&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img132.jpg&quot;&gt;&lt;div&gt;a sign that says UNK UNK UNK and UNK&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img133.jpg&quot;&gt;&lt;div&gt;a group of people standing around a group of sheep&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img134.jpg&quot;&gt;&lt;div&gt;a street sign in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img135.jpg&quot;&gt;&lt;div&gt;a view of a traffic light and a building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img136.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img137.jpg&quot;&gt;&lt;div&gt;a large building with a clock on the side of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img138.jpg&quot;&gt;&lt;div&gt;a white and black sheep in a field&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img139.jpg&quot;&gt;&lt;div&gt;a train traveling down a train track next to a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img140.jpg&quot;&gt;&lt;div&gt;a view of a traffic light and a building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img141.jpg&quot;&gt;&lt;div&gt;a city street with a traffic light on the side of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img142.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img143.jpg&quot;&gt;&lt;div&gt;a group of people standing on a sidewalk&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img144.jpg&quot;&gt;&lt;div&gt;a view of a field with a mountain in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img145.jpg&quot;&gt;&lt;div&gt;a person standing in front of a window in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img146.jpg&quot;&gt;&lt;div&gt;a large building with a clock on the front&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img147.jpg&quot;&gt;&lt;div&gt;a view of a traffic light and a building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img148.jpg&quot;&gt;&lt;div&gt;a city street filled with lots of traffic&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img149.jpg&quot;&gt;&lt;div&gt;a man standing in front of a mirror in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img150.jpg&quot;&gt;&lt;div&gt;a view of a lake with a boat in the distance&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img151.jpg&quot;&gt;&lt;div&gt;a view of a mountain range from a plane&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img152.jpg&quot;&gt;&lt;div&gt;a hotel room with a bed and a table&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img153.jpg&quot;&gt;&lt;div&gt;a man holding a cell phone in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img154.jpg&quot;&gt;&lt;div&gt;a person is standing in front of a building&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;/body&gt;&lt;/html&gt;&lt;/p&gt;
</description>
        <pubDate>2015-11-22 17:39:00 -0600</pubDate>
        <link>lakehanne.github.ioneuraltalk</link>
        <guid isPermaLink="true">lakehanne.github.ioneuraltalk</guid>
        
        
        <category>lakehanne.github.io</category>
        
      </item>
    
      <item>
        <title>&lt;center&gt;Neural Nets Talk&lt;/center&gt;.</title>
        <description>&lt;!-- Analytics --&gt;

&lt;script&gt;
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-64680332-1', 'auto');
  ga('send', 'pageview');
  ga('send', 'pageview' '/neuraltalk');

&lt;/script&gt;

&lt;p&gt;So I have a lot of free time on my hands this long Thanksgiving weekend. Beside spending time with family and friends, I&amp;#39;ve had to catch up with a few papers on my &lt;code&gt;mendeley database&lt;/code&gt; which I had been postponing to read all along and try out some new &lt;code&gt;deep learning&lt;/code&gt; stuff. Having long wanted to try out Andrej Karpathy&amp;#39;s &lt;a href=&quot;https://github.com/karpathy/neuraltalk&quot;&gt;Neural Talk&lt;/a&gt; &lt;code&gt;recurrent nets&lt;/code&gt; but being unable to due to time constraints, I decided to try out the 0.1 Torch 7 version released nine days ago (&lt;a href=&quot;https://github.com/karpathy/neuraltalk2&quot;&gt;neural talk 2&lt;/a&gt;) based on the &lt;a href=&quot;http://torch.ch/&quot;&gt;Torch&lt;/a&gt; library which is faster and more efficient than the original neural talk. &lt;/p&gt;

&lt;p&gt;Since the release, people have tried out the codes with interesting results such as this funny &lt;a href=&quot;https://vimeo.com/146492001&quot;&gt;neural talk and walk video&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/kcimc&quot;&gt;@kcimc&lt;/a&gt;. Well, what better time to fool around with &amp;quot;automagic&amp;quot; image captioning on my old camera&amp;#39;s sd-card images than when on a holiday at a friend&amp;#39;s?&lt;/p&gt;

&lt;p&gt;Using a mobile laptop running an NVIDIA &lt;code&gt;K1100M GPU&lt;/code&gt;, I updated my &lt;a href=&quot;http://torch.ch/docs/getting-started.html&quot;&gt;torch7&lt;/a&gt; installation and the accompanying &lt;a href=&quot;https://luarocks.org/&quot;&gt;luarocks&lt;/a&gt; dependencies (which include &lt;code&gt;cudnn&lt;/code&gt;, &lt;code&gt;cunn&lt;/code&gt;, &lt;code&gt;cutorch&lt;/code&gt;, &lt;code&gt;image&lt;/code&gt;, &lt;code&gt;nn&lt;/code&gt;, and &lt;code&gt;nngraph&lt;/code&gt;), before I fired up the &lt;code&gt;eval.lua&lt;/code&gt; file on a bunch of images I had on my old camera. &lt;/p&gt;

&lt;p&gt;While it&amp;#39;s far from perfect in prediction (such as saying I was in a suit and tie in front of a kitchen counter when I was actually in front of an hotel in London), I was nonetheless amazed at the power and efficiency of &lt;code&gt;RNN&amp;#39;s&lt;/code&gt;. The pictures below demonstrate the automatic captioning on random images I evaluated the code on.&lt;/p&gt;

&lt;p&gt;If after looking through the goodness of the predictions and your &amp;#39;nerdgasms&amp;#39; are not fired up, I would strongly recommend you seeing a psychiatrist. :)! Can you imagine the possibilities of this in suppy-chain robotics? Or some sort of pick and place tasks for autonomous robots? Neural nets are really interesting and quite efficient at real-world approximations and recontruction. I&amp;#39;ll be posting more results on other interesting work I intend to use this for in the coming few days.&lt;/p&gt;

&lt;p&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;
    &lt;meta charset=&quot;utf-8&quot;&gt;
    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot;&gt;
    &lt;title&gt;neuraltest&lt;/title&gt;
    &lt;script src=&quot;/downloads/neuraltest_files/jquery-1.8.3.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;/downloads/neuraltest_files/d3.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
    &lt;style&gt;
    body {
      color: #333;
      margin: 0;
      padding: 0;
      font-family: &amp;quot;HelveticaNeue-Light&amp;quot;, &amp;quot;Helvetica Neue Light&amp;quot;, &amp;quot;Helvetica Neue&amp;quot;, Helvetica, Arial, &amp;quot;Lucida Grande&amp;quot;, sans-serif; 
      font-weight: 300;
    }
    #wrap {
      margin: 10px;
    }
    .result {
      width:300px;
      margin:10px;
      display: inline-block;
      vertical-align: top;
    }
    .result img {
      width: 100%;
    }
    &lt;/style&gt;&lt;/p&gt;

&lt;p&gt;&lt;/script&gt;
  &lt;/head&gt;
  &lt;body onload=&quot;start()&quot;&gt;
    &lt;div id=&quot;wrap&quot;&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img1.jpg&quot;&gt;&lt;div&gt;a close up of a person holding a red apple&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img2.jpg&quot;&gt;&lt;div&gt;a street sign on the side of the road&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img3.jpg&quot;&gt;&lt;div&gt;a man standing in front of a train station&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img4.jpg&quot;&gt;&lt;div&gt;a group of people standing in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img5.jpg&quot;&gt;&lt;div&gt;a man standing in front of a kitchen counter&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img6.jpg&quot;&gt;&lt;div&gt;a man in a suit and tie standing in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img7.jpg&quot;&gt;&lt;div&gt;a television is on a desk with a computer&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img8.jpg&quot;&gt;&lt;div&gt;a group of people standing around a large room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img9.jpg&quot;&gt;&lt;div&gt;a bedroom with a bed and a desk&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img10.jpg&quot;&gt;&lt;div&gt;a car is driving down a road with a traffic light&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img11.jpg&quot;&gt;&lt;div&gt;a man is standing in front of a tv&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img12.jpg&quot;&gt;&lt;div&gt;a man riding a skateboard down a street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img13.jpg&quot;&gt;&lt;div&gt;a group of people walking down a street&lt;/div&gt;&lt;/div&gt;
    &lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img14.jpg&quot;&gt;&lt;div&gt;a large body of water with a boat on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img15.jpg&quot;&gt;&lt;div&gt;a large building with a clock on the side of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img16.jpg&quot;&gt;&lt;div&gt;a person standing in a kitchen with a refrigerator&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img17.jpg&quot;&gt;&lt;div&gt;a man is flying a kite in a field&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img18.jpg&quot;&gt;&lt;div&gt;a group of people sitting around a table eating food&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img19.jpg&quot;&gt;&lt;div&gt;a dog is laying on the floor next to a window&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img20.jpg&quot;&gt;&lt;div&gt;a person is taking a picture of a boat in the water&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img21.jpg&quot;&gt;&lt;div&gt;a sign that says UNK UNK UNK on the side&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img22.jpg&quot;&gt;&lt;div&gt;a view of a building with a window in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img23.jpg&quot;&gt;&lt;div&gt;a plate of food with a fork on a table&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img24.jpg&quot;&gt;&lt;div&gt;a woman is walking down the street with a suitcase&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img25.jpg&quot;&gt;&lt;div&gt;a man standing in front of a subway train&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img26.jpg&quot;&gt;&lt;div&gt;a man in a suit and tie standing in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img27.jpg&quot;&gt;&lt;div&gt;a crowd of people standing around a large group of people&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img28.jpg&quot;&gt;&lt;div&gt;a view of a city street with a traffic light&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img29.jpg&quot;&gt;&lt;div&gt;a group of people in a field with a kite&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img30.jpg&quot;&gt;&lt;div&gt;a man standing in front of a mirror with a toothbrush&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img31.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img32.jpg&quot;&gt;&lt;div&gt;a man in a suit and tie standing in a store&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img33.jpg&quot;&gt;&lt;div&gt;a group of people walking down a street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img34.jpg&quot;&gt;&lt;div&gt;a person standing in a room with a suitcase&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img35.jpg&quot;&gt;&lt;div&gt;a crowd of people standing around a large plane&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img36.jpg&quot;&gt;&lt;div&gt;a man and a woman standing in front of a wine glass&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img37.jpg&quot;&gt;&lt;div&gt;a group of people sitting in a living room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img38.jpg&quot;&gt;&lt;div&gt;a view of a park with a bench and a tree&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img39.jpg&quot;&gt;&lt;div&gt;a group of people playing a game of frisbee&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img40.jpg&quot;&gt;&lt;div&gt;a refrigerator with a lot of magnets on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img41.jpg&quot;&gt;&lt;div&gt;a group of people standing on a sidewalk&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img42.jpg&quot;&gt;&lt;div&gt;a person standing in front of a window&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img43.jpg&quot;&gt;&lt;div&gt;a view of a river with a boat in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img44.jpg&quot;&gt;&lt;div&gt;a cat is sitting on a bathroom sink&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img45.jpg&quot;&gt;&lt;div&gt;a man standing in front of a crowd of people&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img46.jpg&quot;&gt;&lt;div&gt;a view of a park with a bench in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img47.jpg&quot;&gt;&lt;div&gt;a group of people sitting around a table&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img48.jpg&quot;&gt;&lt;div&gt;a car is parked on the side of the street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img49.jpg&quot;&gt;&lt;div&gt;a view of a bathroom with a sink and a mirror&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img50.jpg&quot;&gt;&lt;div&gt;a man standing in front of a group of people&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img51.jpg&quot;&gt;&lt;div&gt;a young boy wearing a tie and a tie&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img52.jpg&quot;&gt;&lt;div&gt;a view of a traffic light and a building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img53.jpg&quot;&gt;&lt;div&gt;a traffic light with a red light on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img54.jpg&quot;&gt;&lt;div&gt;a crowd of people standing around a large building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img55.jpg&quot;&gt;&lt;div&gt;a car parked on the side of the road&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img56.jpg&quot;&gt;&lt;div&gt;a living room with a tv and a tv&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img57.jpg&quot;&gt;&lt;div&gt;a view of a city bus at a stop light&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img58.jpg&quot;&gt;&lt;div&gt;a woman is standing in front of a mirror&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img59.jpg&quot;&gt;&lt;div&gt;a group of people standing on a street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img60.jpg&quot;&gt;&lt;div&gt;a room filled with lots of furniture and a tv&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img61.jpg&quot;&gt;&lt;div&gt;a view of a train station with a train station&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img62.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img63.jpg&quot;&gt;&lt;div&gt;a large building with a clock tower on top of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img64.jpg&quot;&gt;&lt;div&gt;a man is standing in front of a large mirror&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img65.jpg&quot;&gt;&lt;div&gt;a man is holding a frisbee in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img66.jpg&quot;&gt;&lt;div&gt;a city street with a clock tower in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img67.jpg&quot;&gt;&lt;div&gt;a kitchen with a sink and a refrigerator&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img68.jpg&quot;&gt;&lt;div&gt;a woman holding a pair of scissors in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img69.jpg&quot;&gt;&lt;div&gt;a clock on a tree in a park&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img70.jpg&quot;&gt;&lt;div&gt;a cat sitting on a table next to a pair of scissors&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img71.jpg&quot;&gt;&lt;div&gt;a man is holding a cell phone in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img72.jpg&quot;&gt;&lt;div&gt;a man and a woman sitting on a couch playing a video game&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img73.jpg&quot;&gt;&lt;div&gt;a tree in the middle of a city street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img74.jpg&quot;&gt;&lt;div&gt;a man is holding a glass of wine&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img75.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img76.jpg&quot;&gt;&lt;div&gt;a man and a woman sitting at a table with laptops&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img77.jpg&quot;&gt;&lt;div&gt;a train station with a train on the tracks&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img78.jpg&quot;&gt;&lt;div&gt;a view of a kitchen with a window in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img79.jpg&quot;&gt;&lt;div&gt;a large jetliner sitting on top of an airport tarmac&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img80.jpg&quot;&gt;&lt;div&gt;a view of a boat in the water&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img81.jpg&quot;&gt;&lt;div&gt;a street with a lot of cars on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img82.jpg&quot;&gt;&lt;div&gt;a woman standing in a room with a remote&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img83.jpg&quot;&gt;&lt;div&gt;a group of people standing in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img84.jpg&quot;&gt;&lt;div&gt;a woman is standing in a bathroom with a toilet&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img85.jpg&quot;&gt;&lt;div&gt;a person is walking on a dirt road&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img86.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img87.jpg&quot;&gt;&lt;div&gt;a city street filled with lots of traffic&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img88.jpg&quot;&gt;&lt;div&gt;a person standing in front of a window with a clock on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img89.jpg&quot;&gt;&lt;div&gt;a close up of a keyboard on a table&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img90.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img91.jpg&quot;&gt;&lt;div&gt;a man flying through the air while riding a snowboard&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img92.jpg&quot;&gt;&lt;div&gt;a man holding a cell phone in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img93.jpg&quot;&gt;&lt;div&gt;a man sitting on a toilet in a bathroom&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img94.jpg&quot;&gt;&lt;div&gt;a group of people sitting around a table eating food&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img95.jpg&quot;&gt;&lt;div&gt;a view of a kitchen with a refrigerator and a stove&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img96.jpg&quot;&gt;&lt;div&gt;a car driving down a street next to a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img97.jpg&quot;&gt;&lt;div&gt;a person standing in front of a refrigerator&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img98.jpg&quot;&gt;&lt;div&gt;a man standing in front of a train station&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img99.jpg&quot;&gt;&lt;div&gt;a boat that is sitting in the water&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img100.jpg&quot;&gt;&lt;div&gt;a cat is sitting on the floor in a bathroom&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img101.jpg&quot;&gt;&lt;div&gt;a man is holding a wii remote in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img102.jpg&quot;&gt;&lt;div&gt;a group of people standing on top of a lush green field&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img103.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img104.jpg&quot;&gt;&lt;div&gt;a traffic light on a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img105.jpg&quot;&gt;&lt;div&gt;a group of people standing on a platform&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img106.jpg&quot;&gt;&lt;div&gt;a red and yellow train on the tracks&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img107.jpg&quot;&gt;&lt;div&gt;a picture of a person in a mirror&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img108.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img109.jpg&quot;&gt;&lt;div&gt;a traffic light on a pole on a city street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img110.jpg&quot;&gt;&lt;div&gt;a traffic light on a pole on a city street&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img111.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img112.jpg&quot;&gt;&lt;div&gt;a kitchen with a stove and a sink&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img113.jpg&quot;&gt;&lt;div&gt;a person standing in front of a window with a clock on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img114.jpg&quot;&gt;&lt;div&gt;a car parked on the side of the road&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img115.jpg&quot;&gt;&lt;div&gt;a picture of a man in a suit and tie&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img116.jpg&quot;&gt;&lt;div&gt;a large jetliner sitting on top of an airport runway&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img117.jpg&quot;&gt;&lt;div&gt;a city street at night with a traffic light&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img118.jpg&quot;&gt;&lt;div&gt;a red and white car parked in a parking lot&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img119.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img120.jpg&quot;&gt;&lt;div&gt;a young boy is sitting on a bench&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img121.jpg&quot;&gt;&lt;div&gt;a city street with a large building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img122.jpg&quot;&gt;&lt;div&gt;a building with a clock on the side of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img123.jpg&quot;&gt;&lt;div&gt;a woman holding a cell phone in her hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img124.jpg&quot;&gt;&lt;div&gt;a table with a vase of flowers on it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img125.jpg&quot;&gt;&lt;div&gt;a group of people standing in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img126.jpg&quot;&gt;&lt;div&gt;a man is standing in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img127.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img128.jpg&quot;&gt;&lt;div&gt;a cat is sitting on the floor in front of a tv&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img129.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img130.jpg&quot;&gt;&lt;div&gt;a bathroom with a sink and a toilet&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img131.jpg&quot;&gt;&lt;div&gt;a view of a city street from a window&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img132.jpg&quot;&gt;&lt;div&gt;a sign that says UNK UNK UNK and UNK&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img133.jpg&quot;&gt;&lt;div&gt;a group of people standing around a group of sheep&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img134.jpg&quot;&gt;&lt;div&gt;a street sign in front of a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img135.jpg&quot;&gt;&lt;div&gt;a view of a traffic light and a building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img136.jpg&quot;&gt;&lt;div&gt;a view of a city street at night&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img137.jpg&quot;&gt;&lt;div&gt;a large building with a clock on the side of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img138.jpg&quot;&gt;&lt;div&gt;a white and black sheep in a field&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img139.jpg&quot;&gt;&lt;div&gt;a train traveling down a train track next to a building&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img140.jpg&quot;&gt;&lt;div&gt;a view of a traffic light and a building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img141.jpg&quot;&gt;&lt;div&gt;a city street with a traffic light on the side of it&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img142.jpg&quot;&gt;&lt;div&gt;a view of a city street at night time&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img143.jpg&quot;&gt;&lt;div&gt;a group of people standing on a sidewalk&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img144.jpg&quot;&gt;&lt;div&gt;a view of a field with a mountain in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img145.jpg&quot;&gt;&lt;div&gt;a person standing in front of a window in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img146.jpg&quot;&gt;&lt;div&gt;a large building with a clock on the front&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img147.jpg&quot;&gt;&lt;div&gt;a view of a traffic light and a building in the background&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img148.jpg&quot;&gt;&lt;div&gt;a city street filled with lots of traffic&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img149.jpg&quot;&gt;&lt;div&gt;a man standing in front of a mirror in a room&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img150.jpg&quot;&gt;&lt;div&gt;a view of a lake with a boat in the distance&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img151.jpg&quot;&gt;&lt;div&gt;a view of a mountain range from a plane&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img152.jpg&quot;&gt;&lt;div&gt;a hotel room with a bed and a table&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img153.jpg&quot;&gt;&lt;div&gt;a man holding a cell phone in his hand&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;result&quot;&gt;&lt;img src=&quot;/downloads/neuraltest_files/img154.jpg&quot;&gt;&lt;div&gt;a person is standing in front of a building&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;/body&gt;&lt;/html&gt;&lt;/p&gt;
</description>
        <pubDate>2015-11-22 17:39:00 -0600</pubDate>
        <link>lakehanne.github.ioneuraltalk</link>
        <guid isPermaLink="true">lakehanne.github.ioneuraltalk</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Optimal Control: Model Predictive Controllers.&lt;/center&gt;</title>
        <description>&lt;!-- Google Tag Manager --&gt;

&lt;noscript&gt;&lt;iframe src=&quot;//www.googletagmanager.com/ns.html?id=GTM-WXGQX2&quot;
height=&quot;0&quot; width=&quot;0&quot; style=&quot;display:none;visibility:hidden&quot;&gt;&lt;/iframe&gt;&lt;/noscript&gt;

&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WXGQX2');&lt;/script&gt;
&lt;!-- End Google Tag Manager --&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;!--Mathjax Parser --&gt;

&lt;script type=&quot;text/javascript&quot; async
  src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;TABLE BORDER=&quot;5&quot;    WIDTH=&quot;100%&quot;   CELLPADDING=&quot;1&quot; CELLSPACING=&quot;2&quot;&gt;

    &lt;TR&gt;
      &lt;TH COLSPAN=&quot;4&quot;&gt;&lt;BR&gt;&lt;H3&gt;Table of Notations and Abbreviations&lt;/H3&gt;
      &lt;/TH&gt;
    &lt;/TR&gt;

    &lt;TR&gt;
      &lt;TH&gt;Notation&lt;/TH&gt;
      &lt;TH&gt;Meaning&lt;/TH&gt;
      &lt;TH&gt;Abbreviation&lt;/TH&gt;
      &lt;TH&gt;Meaning&lt;/TH&gt;
    &lt;/TR&gt;

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$z^{-1}$&lt;/TD&gt;
      &lt;TD&gt;Unit delay operator&lt;/TD&gt;  
      &lt;TD&gt;MFD&lt;/TD&gt;
      &lt;TD&gt;Matrix Fraction Description&lt;/TD&gt;   
    &lt;/TR&gt;

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$x_{\rm \rightarrow}$&lt;/TD&gt;
      &lt;TD&gt;Future values of x&lt;/TD&gt;  
      &lt;TD&gt;LTI&lt;/TD&gt;
      &lt;TD&gt;Linear Time Invariant&lt;/TD&gt;    
    &lt;/TR&gt;   

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$x_{\rm \leftarrow}$&lt;/TD&gt;
      &lt;TD&gt;Past values of x&lt;/TD&gt; 
      &lt;TD&gt;FSR&lt;/TD&gt;
      &lt;TD&gt;Finite Step Response&lt;/TD&gt;     
    &lt;/TR&gt;  

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$\Delta = 1 - z^{-1}$&lt;/TD&gt;
      &lt;TD&gt;Differencing Operator&lt;/TD&gt;  
      &lt;TD&gt;FIR&lt;/TD&gt;
      &lt;TD&gt;Finite Impulse Response&lt;/TD&gt;  
    &lt;/TR&gt;  

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$z^{-1}$&lt;/TD&gt;
      &lt;TD&gt;Backward shift operator (z-transforms)&lt;/TD&gt;   
      &lt;TD&gt;TFM&lt;/TD&gt;
      &lt;TD&gt;Transfer Function Model&lt;/TD&gt;
    &lt;/TR&gt;  

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$n_y$&lt;/TD&gt;
      &lt;TD&gt;Output Prediction Horizon&lt;/TD&gt;    
      &lt;TD&gt;SISO&lt;/TD&gt;
      &lt;TD&gt;Single-Input Single Output&lt;/TD&gt;
    &lt;/TR&gt; 

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$n_u$&lt;/TD&gt;
      &lt;TD&gt;Input Horizon&lt;/TD&gt;  
      &lt;TD&gt;MIMO&lt;/TD&gt;
      &lt;TD&gt;Multiple-Input Multiple Output&lt;/TD&gt;
    &lt;/TR&gt;   

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$u(k-i)$&lt;/TD&gt;
      &lt;TD&gt;Past input&lt;/TD&gt; 
      &lt;TD&gt;IMC&lt;/TD&gt;
      &lt;TD&gt;Internal Model Control&lt;/TD&gt;
    &lt;/TR&gt; 

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$y(k-i)$&lt;/TD&gt;
      &lt;TD&gt;Past output&lt;/TD&gt;
      &lt;TD&gt;IM&lt;/TD&gt;
      &lt;TD&gt;Independednt Model&lt;/TD&gt;
    &lt;/TR&gt;      
&lt;/TABLE&gt;

&lt;h3&gt;&lt;center&gt;&lt;strong&gt;Optimal Controllers: An Overview&lt;/strong&gt;&lt;/center&gt;&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;If you are already familiar with LQ methods, you can skip this section and go straight to &lt;a href=&quot;#Model-Predictive-Controllers&quot;&gt;&lt;strong&gt;Model Predictive Controllers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Optimal controllers belong to the class of controllers that minimize a cost function with respect to a &lt;u&gt;&lt;b&gt;predicted&lt;/u&gt;&lt;/b&gt; control law over a given &lt;u&gt;&lt;b&gt;prediction horizon&lt;/u&gt;&lt;/b&gt;. They generate a desired &lt;u&gt;&lt;b&gt;output control&lt;/u&gt;&lt;/b&gt; sequence from which the optimal &lt;u&gt;&lt;b&gt;&lt;em&gt;control law&lt;/em&gt;&lt;/u&gt;&lt;/b&gt; may be determined. &lt;/p&gt;

&lt;!-- &lt;br&gt;&lt;/br&gt; --&gt;

&lt;p&gt;Take your car cruise speed control for an example. The vehicle dynamics of the car is &lt;em&gt;&lt;em&gt;modeled&lt;/em&gt;&lt;/em&gt; into the cruise control system. The road curvature ahead of you gets &lt;em&gt;&lt;em&gt;predicted&lt;/em&gt;&lt;/em&gt; online as you drive and &lt;u&gt;&lt;em&gt;control sequences&lt;/em&gt;&lt;/u&gt; are generated based on an &lt;em&gt;&lt;em&gt;internal model&lt;/em&gt;&lt;/em&gt; &lt;em&gt;(&lt;em&gt;prediction&lt;/em&gt;)&lt;/em&gt; of the slope of the road/road curvature using past observations; other disturbances (such as rain, friction between tire and road) are modeled into the prediction ideally. At time \(k + 1\), only the first element (&lt;b&gt;&lt;em&gt;control action&lt;/em&gt;&lt;/b&gt;) within the generated control sequence is used as a &lt;em&gt;feedback&lt;/em&gt; control mechanism for your car. At the next &lt;b&gt;&lt;em&gt;sampling time instant&lt;/em&gt;&lt;/b&gt;, new predictions are made based on the prediction horizon and a new control sequence is generated online. Again, the chosen control action is the first element in the control sequence such  that we are constantly choosing the best among a host of control actions using our anticipated &lt;b&gt;&lt;em&gt;prediction&lt;/em&gt;&lt;/b&gt; of the road curvature and other environmental variables to give the desired &lt;b&gt;&lt;em&gt;performance&lt;/em&gt;&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
Optimal controllers are mostly used in discrete processes (&lt;a href=&quot;#Gawthrop&quot;&gt;others&lt;/a&gt; have postulated continuous time models for such controllers but the vast majority of commercial users of optimal controllers are in discrete time) and are very useful for processes where the setpoint is known ahead of time. Typically, a criterion function is used to determine how well the controller is tracking a desired trajectory. An example model predictive controller (MPC) criterion function, in its basic form, is given as  &lt;/p&gt;

&lt;p&gt;&lt;a name = eqn:cost-function&gt;&lt;/a&gt;
\begin{equation}            \label{eqn:cost function}
  J  = \sum\nolimits_{i=n_w}^{n_y} [\hat{y}(k+i) - r(k + i)]^2 
\end{equation}  &lt;/p&gt;

&lt;p&gt;where \(i\) is typically taken as 1, \(n_y\) is the prediction horizon, \(\hat{y}(k+i)\) is the prediction and \(r(k+i)\) is the desired trajectory (or setpoint).&lt;/p&gt;

&lt;p&gt;The controller output sequence, \(\Delta u \), is obtained by minimizing \(J\) over the prediction horizon, \(n_y\), with respect to \(\Delta u \), i.e.,&lt;/p&gt;

&lt;p&gt;\begin{equation}
  \Delta u  = \text{arg} \min_{\rm \Delta u} J \label{eqn:min J} 
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\Delta u \) is the future control sequence.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
If equation \eqref{eqn:cost function} is &lt;strong&gt;&lt;em&gt;well-posed&lt;/em&gt;&lt;/strong&gt;&lt;a href=&quot;#footnote-1&quot;&gt;\(^{1}\)&lt;/a&gt;, then \(\Delta u \) would be optimal with respect to the criterion function and hence &lt;a href=&quot;#eqn:carima&quot;&gt;eliminate offset in trajectory tracking&lt;/a&gt;. &lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
  &lt;img src=&quot;/downloads/MPC/MPCConcept.jpg&quot; width=&quot;50%&quot; height=&quot;350&quot;, border=&quot;0&quot; style=&quot;float:left;&quot;&gt;
  &lt;img src=&quot;/downloads/MPC/LQG.jpg&quot; width=&quot;50%&quot; height=&quot;350&quot;  border=&quot;0&quot; style=&quot;float:right;&quot;&gt;  
  &lt;div class=&quot;figcaption&quot; align=&quot;left&quot;&gt;Fig.1.0.0. Tracking by a Model Predictive Controller. &lt;div class=&quot;figcaption&quot; align=&quot;right&quot;&gt;Fig. 1.0.1. Reference Tracking by an LQ Controller&lt;/a&gt;&lt;/a&gt;.
  &lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;!--![MPCControl.jpg](/downloads/MPC/MPCConcept.jpg){: .center-image }--&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
It becomes obvious from equation \eqref{eqn:cost function} that minimizing the criterion function is an optimization problem. One useful approach in typical problems is to make \(J\) &lt;b&gt;&lt;em&gt;quadratic&lt;/em&gt;&lt;/b&gt; such that the model becomes linear; &lt;u&gt;if there are no contraints, the solution to the quadratic minimization problem is analytic&lt;/u&gt;.&lt;/p&gt;

&lt;h4&gt;&lt;center&gt;&lt;strong&gt;Linear Quadratic (LQ) Controllers&lt;/strong&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;Linear Quadratic Controllers are indeed predictive controllers except for the infinite horizon which they employ in minimizing the criterion function. The criterion function is minimized only once resulting in an optimal controller output sequence from which the controller output is selected.
Similar to model predictive controllers, they are in general discrete time controllers. For a linear problem such as&lt;/p&gt;

&lt;p&gt;&lt;a name=eqn:state-model&gt;&lt;/a&gt;
\begin{gather} \label{eqn:state-model}
  x(k+1) = A \, x(k) + B \, u(k)  \nonumber \newline 
  y(k) = C^T \, x(k)              \nonumber
\end{gather}&lt;/p&gt;

&lt;p&gt;The cost function is typically constructed as &lt;/p&gt;

&lt;p&gt;&lt;a name=eqn:LQ-cost&gt;&lt;/a&gt;
\begin{equation}  \label{eqn:LQ-cost}
J = \sum_{k=0}^n x^T(k)\,Q\,x(k) + R \, u(k)^T \, u(k) + 2 x(k)^T \, N \, u(k)
\end{equation}  &lt;/p&gt;

&lt;p&gt;where \(n\) is the terminal sampling instant, \(Q\) is a symmetric, positive semi-definite matrix that weights the \(n\)-states of the matrix. \(N\) specifies a matrix of appropriate dimensions that penalizes the cross-product between the input and state vectors, while \(R\) is a symmetric, positive defiite weighting matrix on the control vector , \(u\). &lt;/p&gt;

&lt;p&gt;Other variants of equation \eqref{eqn:LQ-cost} exist where instead of weighting the states of the system, we instead weight the output of the system by constructing the cost function as&lt;/p&gt;

&lt;p&gt;&lt;a name=eqn:LQ-costy&gt;&lt;/a&gt;
\begin{equation}   \label{eqn:LQ-costy}
  J = \sum_{k=0}^n y^T(k) \,Q \,y(k) + R\, u(k)^T \, u(k) +  2 x(k)^T \, N \, u(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;In practice, it is typical to set the eigen values of the \(Q\)-matrix to one while adjusting the eigenvalue(s) of the \(R\) matrix until one obtains the desired result. \(N\) is typically set to an appropriate matrix of zeros. This is no doubt not the only way of tuning the LQ controller as fine control would most likely require weighting the eigen-values of the Q-matrix differently (this is from my experience when tuning LQ controllers).&lt;/p&gt;

&lt;p&gt;If we model disturbances into the system&amp;#39;s states, the optimization problem becomes a stochastic optimization problem that must be solved. But the separation theorem applies such that we can construct a state estimator which asymptotically tracks the internal states from observed outputs using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Algebraic_Riccati_equation&quot;&gt;algebraic Riccati equation&lt;/a&gt; given as &lt;/p&gt;

&lt;p&gt;\begin{equation}    \label{eqn:Riccati}
  A^T P A -(A^T P B + N)(R + B^T P B)^{-1}(B^T P A + N) + Q.
\end{equation}&lt;/p&gt;

&lt;p&gt;\(P\) is an unknown \(n \times n\) symmetric matrix and \(A\), \(B\), \(Q\), and \(R\) are known coefficient matrices as in equations \eqref{eqn:LQ-cost} and \eqref{eqn:LQ-costy}. We find an optimal control law by solving the minimization of the deterministic LQ problem, equation \eqref{eqn:LQ-cost} which we then feed into the states. &lt;/p&gt;

&lt;p&gt;The optimal controller gains, \(K\), are determined from the equation&lt;/p&gt;

&lt;p&gt;\begin{equation}
  K_{lqr} = R^{-1}(B^T \, P + N^T)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(P\) is the solution to the algebraic Riccati equation \eqref{eqn:Riccati}.&lt;/p&gt;

&lt;p&gt;In classical control, we are accustomed to solving a control design problem by determining design parameters based on a given set of design specifications (e.g. desired overshoot, gain margin, settiling time, e.t.c). Discrete LQ controllers pose a significant difficulty with respect to solving such problems as it is challenging to solve the criterion function in terms of these design parameters since discrete state space matrices hardly translate to any physical meaning. If you use the &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00207728808964057&quot;&gt;orthogonal least squares algorithm&lt;/a&gt; or &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00207178908559767&quot;&gt;forward regression orthogonal least squares&lt;/a&gt; in identifying your physical system, you might be able to make some sense of your model though. &lt;/p&gt;

&lt;p&gt;It turns out that if the cost function is quadratic in the parameters and the state space matrices are in  &lt;strong&gt;&lt;em&gt;continuous time&lt;/em&gt;&lt;/strong&gt;, the weighting matrices would no longer correspond to the artificial, \(A\), \(B\), and \(C\) matrices (which is indeed what they are in the discrete state space).&lt;/p&gt;

&lt;p&gt;&lt;a name=Model-Predictive-Controllers&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;&lt;center&gt;Model Predictive Controllers (MPC)&lt;/strong&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;A key thing about LQ closed loop problems is that we can pretty much guarantee closed loop stability if the prediction horizon, \(N \rightarrow \infty\). Model Predictive Controllers, on the other hand, employ a &lt;a href=&quot;http://www.cds.caltech.edu/%7Emurray/books/AM08/pdf/obc08-rhc_30Jan08.pdf&quot;&gt;receding (which is a finite) horizon concept&lt;/a&gt; in establishing the control sequence and closed-loop stability is generally not guaranteed. &lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;http://www.mdpi.com/energies/energies-08-01505/article_deploy/html/images/energies-08-01505-g005-1024.png&quot; width=&quot;60%&quot; height=&quot;450&quot; align=&quot;middle&quot;&gt;  
  &lt;div class=&quot;figcaption&quot; align=&quot;left&quot;&gt;Fig. 1. Receding Horizon Concept. &lt;i&gt;Courtesy, &lt;a href=&quot;http://www.mdpi.com/1996-1073/8/2/1505&quot;&gt; MDPI &lt;/a&gt;&lt;/i&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;!-- ![Receding Horizon Concept](http://www.mdpi.com/energies/energies-08-01505/article_deploy/html/images/energies-08-01505-g005-1024.png) --&gt;

&lt;p&gt;Basically, for inifinite horizon controllers such as LQ methods, this means there is no model mismatch between the predictor model and the plant. In practice, this is tough to achieve as disturbances play a large role in virtually all real-world systems. Typically in state-space or transfer function control model structures, we&amp;#39;ll add an integrator in the feedback loop to correct for offset errors at steady state. This would not be optimal in zeroing steady-state errors in an MPC approach. One way of avoiding mismatch is to include an integrator in the prediction model as an &lt;strong&gt;&lt;em&gt;internal model&lt;/em&gt;&lt;/strong&gt; of the disturbance. This could be a &lt;strong&gt;CARIMA&lt;/strong&gt;  (&lt;strong&gt;C&lt;/strong&gt;ontrolled &lt;strong&gt;A&lt;/strong&gt;uto &lt;strong&gt;R&lt;/strong&gt;egressive &lt;strong&gt;I&lt;/strong&gt;ntegral &lt;strong&gt;M&lt;/strong&gt;oving &lt;strong&gt;A&lt;/strong&gt;verage) model, for example. A typical &lt;strong&gt;CARIMA&lt;/strong&gt; model takes the polynomial form&lt;/p&gt;

&lt;p&gt;&lt;a name=eqn:carima&gt;&lt;/a&gt;
\begin{equation}  \label{eqn:carima}
  A(z^{-1})\,y(k) = B(z^{-1})\,u(k) + \dfrac{C(z^{-1})}{1-z^{-1}}\,e(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(A\), \(B\), and \(C\) are polynomials in the backward shift operator \(z^{-1}\) given by&lt;/p&gt;

&lt;p&gt;\begin{equation} \label{eqn:poly}
  A(z^{-1}) = 1 + a_1\,z^{-1} + a_2 + \, z^{-2} + \cdots + a_{n_a}z^-{n_a}  \nonumber
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
  B(z^{-1}) = b_1 + b_2\, z^{-1} + b_3 \, z^{-2} + \cdots + b_{n_b}z^-{n_b}  \nonumber
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
  C(z^{-1}) = c_0 + c_1\, z^{-1} + c_2 \, z^{-2} + \cdots + c_{n_b}z^-{n_c},
\end{equation}&lt;/p&gt;

&lt;p&gt;and \(y(k)\), \(u(k)\) and \(e(k)\), for \(k = 1, 2\), \(\cdots \)  are respectively the plant output, input and integrated noise term in the model. &lt;/p&gt;

&lt;p&gt;MPC&amp;#39;s are generally good and better than traditional PID controllers if correctly implemented in that they handle disturbances typically well and can anticipate future disturbances thereby making the control action more effective as a result. This is because of an &lt;i&gt;&lt;strong&gt;internal model&lt;/strong&gt;&lt;/i&gt; of the plant that allows them to anticipate the future &amp;quot;behavior&amp;quot; of a plant&amp;#39;s output and mitigate such errors before the plant reaches the &lt;i&gt;&amp;quot;future time&amp;quot;&lt;/i&gt;. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.sheffield.ac.uk/acse/staff/jar&quot;&gt;Rossiter&lt;/a&gt; gives a classic analogy in the way human beings cross a road. It is not sufficient that a road is not busy with passing cars on it. As you cross, you constantly look at ahead (your prediction horizon) to anticipate oncoming vehicles and update your movement (=control action) based on your prediction (based on past observations).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Accurate prediction over a selected horizon is critical to a successful MPC implementation&lt;/strong&gt;. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Below are a brief overview of common models employed in MPC algorithms. Readers are referred to System Identification texts e.g., &lt;a href=&quot;#box-jenkins&quot;&gt;Box and Jenkins&lt;/a&gt;,  &lt;a href=&quot;#billings&quot;&gt;Billings&lt;/a&gt;, or &lt;a href=&quot;#Ljung&quot;&gt;Ljung&lt;/a&gt; where data modeling and system identification methods are elucidated in details.&lt;/p&gt;

&lt;!-- &lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
  &lt;img src=&quot;/downloads/MPC/lrpc.jpg&quot; width=&quot;75%&quot; height=&quot;450&quot;, border=&quot;0&quot; style=&quot;float:center;&quot;&gt; 
  &lt;div class=&quot;figcaption&quot; align=&quot;center&quot;&gt;Fig.2.1. Receding Horizon Concept (Copyright: Mahdi Mahfouf, University of Sheffield, 2011 - 2012). 
&lt;/div&gt;
&lt;/div&gt; --&gt;

&lt;h4&gt;&lt;center&gt;&lt;strong&gt;Linear Models in Predictive Control&lt;/strong&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;An essential part of an MPC design is the internal model of the plant. If we want a high-fidelity control, we would want to develop a high fidelity model. For optimal control, our controller would only be as good as our model. If our understanding of the plant is faulty and this transfers to the model, we would typically have model mismatch between plant and predictor and our implementation of the MPC may be worse than using an infinite horizon control.&lt;/p&gt;

&lt;p&gt;We briefly discuss the common classical models that have been accepted as a standard by the control community in the past two-some decades. Linear models obey the superposition principle. Put differently, their internal structure can be approximated by a linear function in the proximity of the desired operating point. In general, linear system identification belong in two large categories: parametric and non-parametric methods. I will only touch upon parametric methods as these are the most commonly employed models in MPC approaches.&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Autoregressive Moving Average with Exogenous Input Model (ARMAX)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;The general structure of the linear finite-horizon system can be written as &lt;/p&gt;

&lt;p&gt;\begin{equation} \label{eqn:ARMAX}
  A(z^{-1})\,y(k) = \dfrac{B(z^{-1})}{F(z^{-1})}\,u(k) + \dfrac{C(z^{-1})}{D(z^{-1})}\,e(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;with \(A(z^{-1}), \, B(z^{-1}), \, C(z^{-1}) \) defined as in equation &lt;a href=&quot;#eqn:poly&quot;&gt;\eqref{eqn:poly}&lt;/a&gt; and the \(F(z^{-1}), \text{ and } D(z^{-1})\) polynomials defined as&lt;/p&gt;

&lt;p&gt;\begin{equation}
  D(z^{-1}) = 1 + d_1\,z^{-1} + d_2 + \, z^{-2} + \cdots + d_{n_d}z^{-n_d}  \nonumber
\end{equation} &lt;/p&gt;

&lt;p&gt;\begin{equation}
  F(z^{-1}) = 1 + f_1\,z^{-1} + f_2 + \, z^{-2} + \cdots + f_{n_f}z^{-n_f} 
\end{equation}&lt;/p&gt;

&lt;p&gt;The autoregressive part is given by the component \(A(z^{-1}) \, y(k)\), while the noise term is modeled as a moving average regression model in \(\dfrac{C(z^{-1})}{D(z^{-1})}\,e(k)\); the exogenous component is given by \(\dfrac{B(z^{-1})}{F(z^{-1})}\,u(k)\). &lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Autoregressive Model (AR)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If in equation \eqref{eqn:ARMAX},  \(B(z^{-1}) = 0\) and \(C(z^{-1}) = D(z^{-1}) = 1\) then  we have an AR model,&lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k) = -a_1 \, y(k-1) -  a_2 \,  y(k-2) - \cdots - a_{n_a} \, y(k-{n_a})
\end{equation}&lt;/p&gt;

&lt;p&gt;Alternatively, if \(A(z^{-1}) = 1\) , \(B(z^{-1}) = 0, \,  \text{ and } \, C(z^{-1}) = 1 \) then we end up with the regression&lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k) = - d_1 \, y(k-1) - d_2 \, y(k-2) - \cdots - d_{n_d} y(k - n_d) + e(k) 
\end{equation}&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Moving Average Model (MA)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If \(A(z^{-1}) = 1\), \(B(z^{-1}) = 0\) and \(D(z^{-1}) = 1\), then we have the following moving average noise model&lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k) = e(k)+ c_1e(k-1) + \cdots + c_{n_c}e(k-n_c)
\end{equation}&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Autoregressive  with Exogenous Input Model (ARX)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If \(F(z^{-1}) = 1\), and \(C(z^{-1}) = D(z^{-1}) =1\), we obtain the ARX structure
\begin{equation}
\begin{split}
  y(k) &amp;amp; = -a_1 \, y(k-1) - \cdots - a_{n_a} \, y(k-{n_a}) + b_1u(k-1) \
       &amp;amp;+\cdots + b_{n_b}u(k-n_b) + e(k)
       \end{split} 
\end{equation}&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Autoregressive Moving Average Model (ARMA)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;Setting \(B(z^{-1}) = 0\), and \(D(z^{-1}) = 1\), we obtain the ARMA model,
\begin{equation}
\begin{split}
  y(k) &amp;amp; = -a_1 \, y(k-1) - \cdots - a_{n_a} \, y(k-{n_a}) + c_1eu(k-1) \
       &amp;amp;+\cdots + c_{n_c}e(k-n_c)
       \end{split} 
\end{equation}&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Finite Impulse Response Model (FIR)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If in equation \eqref{eqn:ARMAX}, \(A(z^{-1}) = F(z^{-1}) = 1 \text{ and } C(z^{-1}) = 0\), we have an FIR model. This can be rewritten as &lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k+i) = \sum_{j=0}^{n_y - 1} \, h_j\, u(k-j+i-1)
\end{equation}&lt;/p&gt;

&lt;p&gt;for predictions of the process output at \(t = k+i\) with \(i \geq 1\). &lt;/p&gt;

&lt;p&gt;The FIR model has the advantage of being simple to construct in that no complex calculations are required of the model and model assumptions are required. They are arguably the most commonly used in commercial MPC packages. However, it does come up short for unstable systems and it also requires a lot of parameters to estimate an FIR model.&lt;/p&gt;

&lt;p&gt;My &lt;a href=&quot;https://github.com/SeRViCE-Lab/Matlab-Files/blob/master/ident_data/&quot;&gt;Github repo&lt;/a&gt; has various examples where typical &lt;a href=&quot;https://github.com/SeRViCE-Lab/Matlab-Files/blob/master/ident_data/Filtered%20GWN/carimaFWGN.m&quot;&gt;ARX, ARMAX, Impulse Response and AR models&lt;/a&gt;, are identified from finite data. &lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Box-Jenkins (BJ) Model&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;The BJ Model is obtained by setting 
\begin{equation} \label{eqn:BJ}
  y(k) = \dfrac{B(z^{-1})}{F(z^{-1})}\,u(k) + \dfrac{C(z^{-1})}{D(z^{-1})}\,e(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;a name=fsr&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Finite Step Response Model (FSR)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;\begin{equation}
  y(k) = \sum_{j=0}^{n_s - 1} \, s_j\, \Delta u(k-j-1)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\Delta\) is the differencing operator (\(\Delta = 1 - q^{-1}\)) and \(q^{-1}\) is the backward shift operator. \(n_s\) is the number of step response elements \(s_j\) used in predicting \(y\).&lt;/p&gt;

&lt;h3&gt;&lt;center&gt;&lt;strong&gt;Predictions in System Models&lt;/strong&gt;&lt;/center&gt;&lt;/h3&gt;

&lt;h5&gt;&lt;center&gt;&lt;strong&gt;&lt;u&gt;Background&lt;/u&gt;&lt;/strong&gt;&lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;In principle, MPCs are long-range predictive controllers(LRPC). Contrary to classical control laws, they are potent if there is a process dead-time or if the setpoint is well-known ahead of time. They were introduced by &lt;a href=&quot;#richalet&quot;&gt;Richalet&lt;/a&gt; and &lt;a href=&quot;#cutler&quot;&gt;Cutler&lt;/a&gt; in the 1970&amp;#39;s and &amp;#39;80&amp;#39;s. As mentioned previously, predictions constitute the bulwark of model predictive controllers. Matter-of-factly, the prediction that this family of controllers have in their structure explains their robust performance when correctly implemented in a typical process. The basic algorithm involves:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;At a current time, \(k\), we predict an output, \(\hat{y}_k\), over a certain output horizon, \(n_y\), based on a mathematical model of the plant dynamics. The predicted output is a function of future possible control scenarios.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From the proposed scenarios, the strategy that delivers the best control action to bring the current process output to the setpoint is chosen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The chosen control law is applied to the real process input only at the present time \(k\).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above procedure is repeated at the next sampling instant leading to an updated control action with correctness based on latest measurements. In literature, we refer to this as the &lt;strong&gt;&lt;em&gt;receding horizon concept&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Other model-based controllers include pole-placement methods and Linear Quadratic methods. When there are no contraints on the control law and the setpoint is not complex, LQ-, pole-placement and predictive controllers generally yield an equivalent result.&lt;/p&gt;

&lt;p&gt;We will assume the model of the plant has been correctly estimated as mentioned in the previous post. The model to be controlled (= &lt;em&gt;plant&lt;/em&gt;) is used in predicting the process output over a defined prediction horizon, $n_y$. Typically, we would want to use an \(i\)-step ahead prediction based on our understanding of the system dynamics. The prediction horizon needs to be carefully selected such the computed optimal control law is not equivalent to a linear quadratic controller. Typically this happens when \(n_y - n_u\) is greater than the open-loop settling time and \(n_u\) is \(\geq\) 5. For large \(n_y - n_u\) and large \(n_u\), &lt;a href=&quot;#GPCs&quot;&gt;Generalized Predictive Controllers&lt;/a&gt; give an almost equal control to an optimal controller with the same weights. With \(n_y - n_u\) and \(n_u\) small, the resulting control law may be severely &lt;a href=&quot;#Rossiter&quot;&gt;suboptimal&lt;/a&gt;. Model predictive controllers are implemented in discrete time since control decisions are made discretely ( = instanteneously). Continuous time systems are sampled to obtain a discrete-time equivalent. The rule of thumb is that the sampling time should be \(10 - 20\) times faster than the dominant transient response of the system (e.g. rise time or settling time). &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It is important not to sample at a faster rate than the dominant transient response of the system; otherwise, the high frequency gains within the system will not be picked up by the model. In other words, we should sample fast enough to pick up disturbances, but no faster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Model Predictive Controllers are easy to implement when the model is linear and discrete but they do find applications in nonlinear processes as well. It behooves that a nonlinear model of the process would then be employed to design the controller in such an instance. Nonlinear models will be treated in a future post. Nonlinear systems that use model predictive control tend to have a more rigorous underpinning.&lt;/p&gt;

&lt;p&gt;Within the framework of model predictive controllers, there are several variants in literature. Among these variants are &lt;a name=GPCs&gt;&lt;/a&gt;Clarke&amp;#39;s Generalized Predictive Controllers (GPCs), &lt;a href=&quot;#cutler&quot;&gt;Dynamic Matrix Control&lt;/a&gt;, Extended Predictive Self-Adaptive Control (EPSAC), Predictive Functional Control (PFC), Ydstie&amp;#39;s Extended Horizon Adaptive Control (EHAC), and Unified Predictive Control (UPC) et cet&amp;#39;era. MPCs find applications in nonminimum phase systems, time-delay systems and unstable processes. I will briefly do a once-over on MAC, DMC and GPC with transfer function models and come back full circle to describe GPC with state-space models since these are generally straightforward to code and has less mathematical labyrinths.&lt;/p&gt;

&lt;h4&gt;&lt;center&gt;&lt;u&gt;&lt;strong&gt;Prediction in Richalet&amp;#39;s Model Algorithm Control (MAC)&lt;/strong&gt;&lt;/u&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;Generally, these make use of impulse response function models. Suppose we denote the output of a discrete LTI system by a discrete impulse response \(h(j)\) as in&lt;/p&gt;

&lt;p&gt;\begin{equation}
  H(z^{-1}) = A^{-1}(z^{-1}) \, B^(z^{-1})
\end{equation} &lt;/p&gt;

&lt;p&gt;it follows that the output can be written as a function of \(h(t)\) as follows&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:impulse}
  y(t) = \sum_{j=1}^{n=\infty} \, h(j) \, u(t-j)
\end{equation} &lt;/p&gt;

&lt;p&gt;where \(h(j)\) are the respective coefficients of the impulse response. If we assume a stable and causal system, for an \(n\) terminal sampling instant, equation \eqref{eqn:impulse} becomes&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:impulse-lim}
  y(t) = \sum_{j=1}^{n} \, h(j) \, u(t-j)
\end{equation} &lt;/p&gt;

&lt;p&gt;such that we can compute the recursive form of equation \eqref{eqn:impulse-lim} as&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:impulse-rec}
  y_{mdl}(t + k) = \tilde{h}^T \, \tilde{u}(t + k) = \tilde{u}^T(t + k)\, \tilde{h}
\end{equation} &lt;/p&gt;

&lt;p&gt;where, \[\tilde{h}^T = h(1), \, h(2), \, h(3), \, \ldots, \, h(n) \]
and \[\tilde{u}^T(t + k) = u(t + k - 1), \, u(t + k - 2), \, u(t + k -3), \, \ldots, \, u(t + k - n) \].&lt;/p&gt;

&lt;p&gt;The cost function is given by&lt;/p&gt;

&lt;p&gt;\begin{gather}  \label{eqn:mac-cost}
  J_{MAC} &amp;amp;= e^T \, e + \beta^2 \Delta u^T \Delta u&lt;br&gt;
\end{gather}&lt;/p&gt;

&lt;p&gt;where  \(e = y_r - y_{mdl}\), and \(\beta\) is a penalty function for the input variable \(u(t)\).&lt;/p&gt;

&lt;div class=&quot;boxed&quot;&gt;
&lt;u&gt;&lt;b&gt;MAC Algorithm Summary&lt;/b&gt;&lt;/u&gt;
&lt;ul&gt;
  &lt;li&gt;Define a reference trajectory, $y_{r}$, that $y(k)$ should track.&lt;/li&gt;
  &lt;li&gt;Tune output predictions based on an $FIR$ model order to deal with model disturbances and uncertainties. &lt;/li&gt;
  &lt;li&gt;Use $\beta$ to penalize the control law.&lt;/li&gt;
  &lt;li&gt;Formulate the output predictions using equation \eqref{eqn:impulse-rec} with the assumption that the plant is stable and causal.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;h4&gt;&lt;center&gt;&lt;strong&gt;&lt;u&gt;Prediction in Cutler&amp;#39;s Dynamic Matrix Control&lt;/u&gt;&lt;/strong&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;Here, a &lt;a href=&quot;http://lakehanne.github.io/Optimal-Controllers-MPC/#fsr&quot;&gt;finite step response model&lt;/a&gt; is employed in the prediction. It is essentially composed of three tuning factors viz., the prediction horizon, \(n_y\), control weighting factor, \(\beta\), and the control horizon, \(n_u\).&lt;/p&gt;

&lt;div class=&quot;boxed&quot;&gt;
&lt;u&gt;&lt;b&gt;DMC Assumption&lt;/b&gt;&lt;/u&gt;
&lt;ul&gt;
  &lt;li&gt;The process is stable and causal process&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;h4&gt;&lt;center&gt;&lt;u&gt;&lt;strong&gt;GPC: Prediction of the plant output described by a transfer function&lt;/strong&gt;&lt;/u&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;It seems to me that most literature is filled with prediction of the output described by a transfer function model. I assume this is due to their relative easiness of computation compared against FIR and FSR models and their applicability to unstable processes. Most papers out of Europe tend to favor transfer function methods while US researchers typically prefer state-space methods. Typically, &lt;a href=&quot;http://mathworld.wolfram.com/DiophantineEquation.html&quot;&gt;&lt;strong&gt;Diophantine identity equations&lt;/strong&gt;&lt;/a&gt; are used to form the regressor model. In my opinion, this is royally complex and more often than not obscures the detail of what is being solved. My advice is one should generally stay out of Diophantine models when possible and only use them when necessary. I will briefly expand on this treatment and focus on MPC treatments using recursive state space models when describing GPC algorithms.&lt;/p&gt;

&lt;p&gt;The transfer function model is of the form:&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:tf_model}
  y(k) = \dfrac{z^{-d}B(z^{-1})}{A(z^{-1})}u(k-1)
\end{equation}&lt;/p&gt;

&lt;p&gt;with \(A(z^{-1})\) and \(B(z^{-1})\) defined by the polynomial expansions. A close inspection of equation \eqref{eqn:tf_model} reveals that the transfer function model subsumes both the FIR (i.e. \(A\) = 1 and the coefficients of the \(B\) polynomial are the impulse response elements)  and FSR models (i.e. A = 1 and the coeeficients of the \(B\) polynomial are the step response coefficients \(b_0 = s_0; \, b_j = s_j - s_{j-1} \forall j \geq 1 \).  &lt;/p&gt;

&lt;p&gt;Transfer function models have the good properties of using greedy polynomial orders and variables in representing linear processes but an assumption about the model order has to be made. &lt;/p&gt;

&lt;h5&gt;&lt;center&gt;&lt;u&gt;&lt;strong&gt;Output predictions in TFM with Diophantine identities&lt;/strong&gt;&lt;/u&gt;&lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;Another way of writing equation \eqref{eqn:tf_model} is by forming an \(i\)-step ahead predictor as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:tf_isa}
  y(k + i) = \dfrac{z^{-d}B(z^{-1})}{A(z^{-1})}u(k + i -1)
\end{equation}&lt;/p&gt;

&lt;p&gt;It follows from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_control#Certainty_equivalence&quot;&gt;certainty equivalence principle&lt;/a&gt; that if we substitute the delay, \(d, \text{ and } B, \, A \) polynomials with their estimates (i.e. \(\hat{d}, \hat{B}, \, \text{and} \hat{A} \)), the optimal control solution obtained would be the same as the system&amp;#39;s delay, \(d, \text{ and } B, \text{ and } A \) polynomials. Let&amp;#39;s form the predicted output estimate as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}   \label{eqn:tf_pred}
  \hat{y}(k + i) = \dfrac{z^{-\hat{d}}\hat{B}(z^{-1})}{\hat{A}(z^{-1})}u(k + i -1)
\end{equation}&lt;/p&gt;

&lt;p&gt;We could rearrange the above equation as&lt;/p&gt;

&lt;p&gt;\begin{split}&lt;br&gt;
  \hat{y}(k + i) = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - \hat{y}(k + i)[\hat{A}(z^{-1}) -1] \nonumber 
\end{split}&lt;/p&gt;

&lt;p&gt;Since \(A(z^{-1}) = 1 + a_1\,z^{-1} + a_2 + \, z^{-2} + \cdots + a_{n_a}z^{-n_a} \), by definition, we can write &lt;/p&gt;

&lt;p&gt;\begin{align}&lt;br&gt;
  \hat{y}(k + i) &amp;amp;= z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - \hat{y}(k + i)(a_1\,z^{-1} + a_2 z^{-2} + \cdots + a_{n_a}z^{-n_a}) \nonumber \
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
    \qquad \qquad &amp;amp;= z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - a_1 \hat{y}(k + i -1) - a_2 \hat{y}(k + i -2) - \cdots - a_{n_a} \hat{y}(k + i - n_a)    \nonumber
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
    &amp;amp; = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - (a_1 + a_2 z^{-1} + \cdots + a_{n_a}z^{-n_a + 1}) \hat{y}(k + i - 1) \nonumber
\end{align}&lt;/p&gt;

&lt;p&gt;such that we can write &lt;/p&gt;

&lt;p&gt;\begin{align}  \label{eqn:tf_isa2}
    \hat{y}(k + i) &amp;amp; = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - z(\hat{A} - 1) \hat{y}(k + i - 1) 
\end{align}&lt;/p&gt;

&lt;p&gt;from equation \eqref{eqn:tf_pred}, where \(z(\hat{A} - 1) = a_1 + a_2 z^{-1} + \, z^{-1} + \cdots + a_{n_a}z^{-n_a + 1}\).&lt;/p&gt;

&lt;p&gt;The equation above is the \(i\)-step ahead predictor using the estimates and runs independently of the process. However, it is a fact of life that disturbances and uncertainties get a vote in any prediction model of a physical/chemical process. Therefore equation \eqref{eqn:tf_isa2} is not well-posed. To avoid prediction errors, we could replace \(\hat{y}(k)\) with \(y(k) \) in the equation and rearrange the model a bit further. Let&amp;#39;s introduce the Diophantine identity equation &lt;/p&gt;

&lt;p&gt;&lt;a name=Dioph&gt;&lt;/a&gt;
&lt;div class=&quot;boxed&quot;&gt;
&lt;b&gt;&lt;center&gt;Diophantine Identity Equation&lt;/center&gt;&lt;/b&gt;
&lt;ul&gt;
\begin{equation}&lt;br&gt;
    \dfrac{1}{\hat{A}} = E_i + z^{-i}\dfrac{F_i}{\hat{A}}
\end{equation}
&lt;/ul&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;where degree of \(E_i \leq i -1\) and \(F_i\) being of degree \(n_A -1\).&lt;/p&gt;

&lt;p&gt;Multiplying out equation \eqref{eqn:tf_pred} with \(E_i\) gives&lt;/p&gt;

&lt;p&gt;\begin{align}
    E_i \hat{A}(z^{-1}) \hat{y}(k+i) &amp;amp; = z^{-\hat{d}} E_iB(z^{-1}) u(k+i-1)
\end{align}&lt;/p&gt;

&lt;p&gt;Rearranging the Diophantine equation and substituting \(E_i \hat{A}(z^{-1}) = 1 - z^{-i}F_i \) into the above equation, we find that&lt;/p&gt;

&lt;p&gt;\begin{align}   \label{eqn:rigged}
    \hat{y}(k+i) &amp;amp; = z^{-\hat{d}} E_iB(z^{-1}) u(k+i-1) + F_i \underbrace{\hat{y}(k)}_{\textbf{replace with \(y(k)\)}}.
\end{align}&lt;/p&gt;

&lt;p&gt;But&lt;/p&gt;

&lt;p&gt;\begin{equation}
    E_i\hat{B}(z^{-1}) = \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} - \dfrac{q^{-i} \hat{B}(z^{-1}) F_i}{\hat{A(z^{-1})}},
\end{equation}&lt;/p&gt;

&lt;p&gt;if we multiply the &lt;a href=&quot;#Dioph&quot;&gt;Diophantine equation&lt;/a&gt; by \(\hat{B}(z^{-1})\) such that &lt;/p&gt;

&lt;p&gt;\begin{align}  \label{eqn:tf_isasep}
    \hat{y}(k+i) &amp;amp; = z^{-\hat{d}} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} u(k+i-1) + F_i y(k) - z^{-d} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} F_i u(k -1)   \nonumber
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
\qquad \qquad \quad &amp;amp; = \underbrace{z^{-\hat{d}} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} u(k+i-1)} + \underbrace{F_i (y(k) - \hat{y}(k) ) }_{\textbf{correction } }
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
\quad  &amp;amp;  \textbf{prediction} \nonumber 
\end{align}&lt;/p&gt;

&lt;p&gt;We see that equation \eqref{eqn:tf_isasep} nicely separates the output predictor into a prediction part (&lt;i&gt;from the past input&lt;/i&gt;) and a correction part (based on error between model and prediction at time, \(k\) ). Essentially, we have manipulated the equation \(\eqref(eqn:rigged)\) such that we have a correcting term in the prediction of the output by subsituting \(y(k)\) in place of \(\hat{y}(k)\) in equation \eqref(eqn:rigged) to negate issue of model-plant mismatch.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
&lt;a name=box-jenkins&gt;&lt;/a&gt;[Box and Jenkins]: Box, G.E.P. and Jenkins, G.M., &amp;#39;&lt;em&gt;Time Series Analysis:Forecasting and Control. San Francisco, CA&lt;/em&gt;&amp;#39;, Holden Day, 1970.&lt;/p&gt;

&lt;p&gt;&lt;a name=billings&gt;&lt;/a&gt;[Billings] :  Stephen A. Billings. &amp;#39;&lt;em&gt;Nonlinear System Identification. NARMAX Methods in the Time, Frequency and Spatio-Temporal Domains&lt;/em&gt;&amp;#39;. John Wiley &amp;amp; Sons Ltd, Chichester, West Sussex, United Kingdom,  2013.&lt;/p&gt;

&lt;p&gt;&lt;a name=cutler&gt;&lt;/a&gt;&lt;a href=&quot;https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004232009&quot;&gt;[C. R. Cutler, B. L. Ramaker]: Dynamic matrix control -- A computer control algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=richalet&gt;&lt;/a&gt; [J. Richalet, A. Rault, J.L. Testud and J. papon]: &amp;#39;Model Predictive Heuristic Control: Applications to Industrial Processes&amp;#39;, &lt;em&gt;Automatica&lt;/em&gt;, Vol. 14. No. 5, pp. 413 - 428, 1978.&lt;/p&gt;

&lt;p&gt;&lt;a name=Rossiter&gt;&lt;/a&gt;   &lt;a href=&quot;https://www.crcpress.com/Model-Based-Predictive-Control-A-Practical-Approach/Rossiter/9780849312915&quot;&gt;[J.A. Rossiter]:   &amp;#39;Model Predictive Control: A Practical Approach&amp;#39;. CRC Press LLC, Florida, USA. 2003&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&lt;a name=Gawthrop&gt;&lt;/a&gt;[Gawthrop] : &amp;#39;&lt;em&gt;Continuous-Time Self-Tuning Control&lt;/em&gt;&amp;#39; Vols I and II, Tannton, Research Studies Press, 1990.&lt;/p&gt;

&lt;p&gt;&lt;a name=Ljung&gt;&lt;/a&gt;[Ljung] : L. Ljung. &amp;#39;&lt;em&gt;System Identification Theory for the User&lt;/em&gt;&amp;#39;. 2nd Edition. Upper Saddle River, NJ, USA. Prentice Hall, 1999.&lt;/p&gt;

&lt;p&gt;&lt;a name=Soeterboek&gt;&lt;/a&gt; &lt;a href=&quot;http://repository.tudelft.nl/view/ir/uuid%3A18a07849-f43c-4d98-afb2-0b8a3a2e8b20/&quot;&gt;[Ronald Soeterboek]. &amp;#39;Predictive Control: A Unified Approach&amp;#39;. Prentice Hall International (UK) Limited, 1992&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
&lt;p id=&quot;footnote-1&quot;&gt;[1] A well-posed \(J\) should have an unbiased predictions in steady state. That is the minimizing argument of \(J\) must be consistent with an offset-free tracking. This implies the predicted control sequence to maintain zerto tracking error must be zero. The minimum of \(J = 0\) in steady state is equivalent to \(r_{\rm \rightarrow}\) \(- \, y_{\rm \rightarrow} = 0\) and \(u_{k+i} - u_{k+i-1} = 0\).&lt;/p&gt;

&lt;!-- (Note that \\(x\_{\rm \rightarrow}\\) denotes future values of \\(x\\) ).  --&gt;
</description>
        <pubDate>2015-11-03 04:21:00 -0600</pubDate>
        <link>lakehanne.github.ioOptimal-Controllers-MPC</link>
        <guid isPermaLink="true">lakehanne.github.ioOptimal-Controllers-MPC</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Optimal Control: Model Predictive Controllers.&lt;/center&gt;</title>
        <description>&lt;!-- Google Tag Manager --&gt;

&lt;noscript&gt;&lt;iframe src=&quot;//www.googletagmanager.com/ns.html?id=GTM-WXGQX2&quot;
height=&quot;0&quot; width=&quot;0&quot; style=&quot;display:none;visibility:hidden&quot;&gt;&lt;/iframe&gt;&lt;/noscript&gt;

&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WXGQX2');&lt;/script&gt;
&lt;!-- End Google Tag Manager --&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;!--Mathjax Parser --&gt;

&lt;script type=&quot;text/javascript&quot; async
  src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;TABLE BORDER=&quot;5&quot;    WIDTH=&quot;100%&quot;   CELLPADDING=&quot;1&quot; CELLSPACING=&quot;2&quot;&gt;

    &lt;TR&gt;
      &lt;TH COLSPAN=&quot;4&quot;&gt;&lt;BR&gt;&lt;H3&gt;Table of Notations and Abbreviations&lt;/H3&gt;
      &lt;/TH&gt;
    &lt;/TR&gt;

    &lt;TR&gt;
      &lt;TH&gt;Notation&lt;/TH&gt;
      &lt;TH&gt;Meaning&lt;/TH&gt;
      &lt;TH&gt;Abbreviation&lt;/TH&gt;
      &lt;TH&gt;Meaning&lt;/TH&gt;
    &lt;/TR&gt;

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$z^{-1}$&lt;/TD&gt;
      &lt;TD&gt;Unit delay operator&lt;/TD&gt;  
      &lt;TD&gt;MFD&lt;/TD&gt;
      &lt;TD&gt;Matrix Fraction Description&lt;/TD&gt;   
    &lt;/TR&gt;

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$x_{\rm \rightarrow}$&lt;/TD&gt;
      &lt;TD&gt;Future values of x&lt;/TD&gt;  
      &lt;TD&gt;LTI&lt;/TD&gt;
      &lt;TD&gt;Linear Time Invariant&lt;/TD&gt;    
    &lt;/TR&gt;   

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$x_{\rm \leftarrow}$&lt;/TD&gt;
      &lt;TD&gt;Past values of x&lt;/TD&gt; 
      &lt;TD&gt;FSR&lt;/TD&gt;
      &lt;TD&gt;Finite Step Response&lt;/TD&gt;     
    &lt;/TR&gt;  

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$\Delta = 1 - z^{-1}$&lt;/TD&gt;
      &lt;TD&gt;Differencing Operator&lt;/TD&gt;  
      &lt;TD&gt;FIR&lt;/TD&gt;
      &lt;TD&gt;Finite Impulse Response&lt;/TD&gt;  
    &lt;/TR&gt;  

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$z^{-1}$&lt;/TD&gt;
      &lt;TD&gt;Backward shift operator (z-transforms)&lt;/TD&gt;   
      &lt;TD&gt;TFM&lt;/TD&gt;
      &lt;TD&gt;Transfer Function Model&lt;/TD&gt;
    &lt;/TR&gt;  

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$n_y$&lt;/TD&gt;
      &lt;TD&gt;Output Prediction Horizon&lt;/TD&gt;    
      &lt;TD&gt;SISO&lt;/TD&gt;
      &lt;TD&gt;Single-Input Single Output&lt;/TD&gt;
    &lt;/TR&gt; 

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$n_u$&lt;/TD&gt;
      &lt;TD&gt;Input Horizon&lt;/TD&gt;  
      &lt;TD&gt;MIMO&lt;/TD&gt;
      &lt;TD&gt;Multiple-Input Multiple Output&lt;/TD&gt;
    &lt;/TR&gt;   

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$u(k-i)$&lt;/TD&gt;
      &lt;TD&gt;Past input&lt;/TD&gt; 
      &lt;TD&gt;IMC&lt;/TD&gt;
      &lt;TD&gt;Internal Model Control&lt;/TD&gt;
    &lt;/TR&gt; 

    &lt;TR ALIGN=&quot;CENTER&quot;&gt;
      &lt;TD&gt;$y(k-i)$&lt;/TD&gt;
      &lt;TD&gt;Past output&lt;/TD&gt;
      &lt;TD&gt;IM&lt;/TD&gt;
      &lt;TD&gt;Independednt Model&lt;/TD&gt;
    &lt;/TR&gt;      
&lt;/TABLE&gt;

&lt;h3&gt;&lt;center&gt;&lt;strong&gt;Optimal Controllers: An Overview&lt;/strong&gt;&lt;/center&gt;&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;If you are already familiar with LQ methods, you can skip this section and go straight to &lt;a href=&quot;#Model-Predictive-Controllers&quot;&gt;&lt;strong&gt;Model Predictive Controllers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Optimal controllers belong to the class of controllers that minimize a cost function with respect to a &lt;u&gt;&lt;b&gt;predicted&lt;/u&gt;&lt;/b&gt; control law over a given &lt;u&gt;&lt;b&gt;prediction horizon&lt;/u&gt;&lt;/b&gt;. They generate a desired &lt;u&gt;&lt;b&gt;output control&lt;/u&gt;&lt;/b&gt; sequence from which the optimal &lt;u&gt;&lt;b&gt;&lt;em&gt;control law&lt;/em&gt;&lt;/u&gt;&lt;/b&gt; may be determined. &lt;/p&gt;

&lt;!-- &lt;br&gt;&lt;/br&gt; --&gt;

&lt;p&gt;Take your car cruise speed control for an example. The vehicle dynamics of the car is &lt;em&gt;&lt;em&gt;modeled&lt;/em&gt;&lt;/em&gt; into the cruise control system. The road curvature ahead of you gets &lt;em&gt;&lt;em&gt;predicted&lt;/em&gt;&lt;/em&gt; online as you drive and &lt;u&gt;&lt;em&gt;control sequences&lt;/em&gt;&lt;/u&gt; are generated based on an &lt;em&gt;&lt;em&gt;internal model&lt;/em&gt;&lt;/em&gt; &lt;em&gt;(&lt;em&gt;prediction&lt;/em&gt;)&lt;/em&gt; of the slope of the road/road curvature using past observations; other disturbances (such as rain, friction between tire and road) are modeled into the prediction ideally. At time \(k + 1\), only the first element (&lt;b&gt;&lt;em&gt;control action&lt;/em&gt;&lt;/b&gt;) within the generated control sequence is used as a &lt;em&gt;feedback&lt;/em&gt; control mechanism for your car. At the next &lt;b&gt;&lt;em&gt;sampling time instant&lt;/em&gt;&lt;/b&gt;, new predictions are made based on the prediction horizon and a new control sequence is generated online. Again, the chosen control action is the first element in the control sequence such  that we are constantly choosing the best among a host of control actions using our anticipated &lt;b&gt;&lt;em&gt;prediction&lt;/em&gt;&lt;/b&gt; of the road curvature and other environmental variables to give the desired &lt;b&gt;&lt;em&gt;performance&lt;/em&gt;&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
Optimal controllers are mostly used in discrete processes (&lt;a href=&quot;#Gawthrop&quot;&gt;others&lt;/a&gt; have postulated continuous time models for such controllers but the vast majority of commercial users of optimal controllers are in discrete time) and are very useful for processes where the setpoint is known ahead of time. Typically, a criterion function is used to determine how well the controller is tracking a desired trajectory. An example model predictive controller (MPC) criterion function, in its basic form, is given as  &lt;/p&gt;

&lt;p&gt;&lt;a name = eqn:cost-function&gt;&lt;/a&gt;
\begin{equation}            \label{eqn:cost function}
  J  = \sum\nolimits_{i=n_w}^{n_y} [\hat{y}(k+i) - r(k + i)]^2 
\end{equation}  &lt;/p&gt;

&lt;p&gt;where \(i\) is typically taken as 1, \(n_y\) is the prediction horizon, \(\hat{y}(k+i)\) is the prediction and \(r(k+i)\) is the desired trajectory (or setpoint).&lt;/p&gt;

&lt;p&gt;The controller output sequence, \(\Delta u \), is obtained by minimizing \(J\) over the prediction horizon, \(n_y\), with respect to \(\Delta u \), i.e.,&lt;/p&gt;

&lt;p&gt;\begin{equation}
  \Delta u  = \text{arg} \min_{\rm \Delta u} J \label{eqn:min J} 
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\Delta u \) is the future control sequence.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
If equation \eqref{eqn:cost function} is &lt;strong&gt;&lt;em&gt;well-posed&lt;/em&gt;&lt;/strong&gt;&lt;a href=&quot;#footnote-1&quot;&gt;\(^{1}\)&lt;/a&gt;, then \(\Delta u \) would be optimal with respect to the criterion function and hence &lt;a href=&quot;#eqn:carima&quot;&gt;eliminate offset in trajectory tracking&lt;/a&gt;. &lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
  &lt;img src=&quot;/downloads/MPC/MPCConcept.jpg&quot; width=&quot;50%&quot; height=&quot;350&quot;, border=&quot;0&quot; style=&quot;float:left;&quot;&gt;
  &lt;img src=&quot;/downloads/MPC/LQG.jpg&quot; width=&quot;50%&quot; height=&quot;350&quot;  border=&quot;0&quot; style=&quot;float:right;&quot;&gt;  
  &lt;div class=&quot;figcaption&quot; align=&quot;left&quot;&gt;Fig.1.0.0. Tracking by a Model Predictive Controller. &lt;div class=&quot;figcaption&quot; align=&quot;right&quot;&gt;Fig. 1.0.1. Reference Tracking by an LQ Controller&lt;/a&gt;&lt;/a&gt;.
  &lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;!--![MPCControl.jpg](/downloads/MPC/MPCConcept.jpg){: .center-image }--&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
It becomes obvious from equation \eqref{eqn:cost function} that minimizing the criterion function is an optimization problem. One useful approach in typical problems is to make \(J\) &lt;b&gt;&lt;em&gt;quadratic&lt;/em&gt;&lt;/b&gt; such that the model becomes linear; &lt;u&gt;if there are no contraints, the solution to the quadratic minimization problem is analytic&lt;/u&gt;.&lt;/p&gt;

&lt;h4&gt;&lt;center&gt;&lt;strong&gt;Linear Quadratic (LQ) Controllers&lt;/strong&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;Linear Quadratic Controllers are indeed predictive controllers except for the infinite horizon which they employ in minimizing the criterion function. The criterion function is minimized only once resulting in an optimal controller output sequence from which the controller output is selected.
Similar to model predictive controllers, they are in general discrete time controllers. For a linear problem such as&lt;/p&gt;

&lt;p&gt;&lt;a name=eqn:state-model&gt;&lt;/a&gt;
\begin{gather} \label{eqn:state-model}
  x(k+1) = A \, x(k) + B \, u(k)  \nonumber \newline 
  y(k) = C^T \, x(k)              \nonumber
\end{gather}&lt;/p&gt;

&lt;p&gt;The cost function is typically constructed as &lt;/p&gt;

&lt;p&gt;&lt;a name=eqn:LQ-cost&gt;&lt;/a&gt;
\begin{equation}  \label{eqn:LQ-cost}
J = \sum_{k=0}^n x^T(k)\,Q\,x(k) + R \, u(k)^T \, u(k) + 2 x(k)^T \, N \, u(k)
\end{equation}  &lt;/p&gt;

&lt;p&gt;where \(n\) is the terminal sampling instant, \(Q\) is a symmetric, positive semi-definite matrix that weights the \(n\)-states of the matrix. \(N\) specifies a matrix of appropriate dimensions that penalizes the cross-product between the input and state vectors, while \(R\) is a symmetric, positive defiite weighting matrix on the control vector , \(u\). &lt;/p&gt;

&lt;p&gt;Other variants of equation \eqref{eqn:LQ-cost} exist where instead of weighting the states of the system, we instead weight the output of the system by constructing the cost function as&lt;/p&gt;

&lt;p&gt;&lt;a name=eqn:LQ-costy&gt;&lt;/a&gt;
\begin{equation}   \label{eqn:LQ-costy}
  J = \sum_{k=0}^n y^T(k) \,Q \,y(k) + R\, u(k)^T \, u(k) +  2 x(k)^T \, N \, u(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;In practice, it is typical to set the eigen values of the \(Q\)-matrix to one while adjusting the eigenvalue(s) of the \(R\) matrix until one obtains the desired result. \(N\) is typically set to an appropriate matrix of zeros. This is no doubt not the only way of tuning the LQ controller as fine control would most likely require weighting the eigen-values of the Q-matrix differently (this is from my experience when tuning LQ controllers).&lt;/p&gt;

&lt;p&gt;If we model disturbances into the system&amp;#39;s states, the optimization problem becomes a stochastic optimization problem that must be solved. But the separation theorem applies such that we can construct a state estimator which asymptotically tracks the internal states from observed outputs using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Algebraic_Riccati_equation&quot;&gt;algebraic Riccati equation&lt;/a&gt; given as &lt;/p&gt;

&lt;p&gt;\begin{equation}    \label{eqn:Riccati}
  A^T P A -(A^T P B + N)(R + B^T P B)^{-1}(B^T P A + N) + Q.
\end{equation}&lt;/p&gt;

&lt;p&gt;\(P\) is an unknown \(n \times n\) symmetric matrix and \(A\), \(B\), \(Q\), and \(R\) are known coefficient matrices as in equations \eqref{eqn:LQ-cost} and \eqref{eqn:LQ-costy}. We find an optimal control law by solving the minimization of the deterministic LQ problem, equation \eqref{eqn:LQ-cost} which we then feed into the states. &lt;/p&gt;

&lt;p&gt;The optimal controller gains, \(K\), are determined from the equation&lt;/p&gt;

&lt;p&gt;\begin{equation}
  K_{lqr} = R^{-1}(B^T \, P + N^T)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(P\) is the solution to the algebraic Riccati equation \eqref{eqn:Riccati}.&lt;/p&gt;

&lt;p&gt;In classical control, we are accustomed to solving a control design problem by determining design parameters based on a given set of design specifications (e.g. desired overshoot, gain margin, settiling time, e.t.c). Discrete LQ controllers pose a significant difficulty with respect to solving such problems as it is challenging to solve the criterion function in terms of these design parameters since discrete state space matrices hardly translate to any physical meaning. If you use the &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00207728808964057&quot;&gt;orthogonal least squares algorithm&lt;/a&gt; or &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00207178908559767&quot;&gt;forward regression orthogonal least squares&lt;/a&gt; in identifying your physical system, you might be able to make some sense of your model though. &lt;/p&gt;

&lt;p&gt;It turns out that if the cost function is quadratic in the parameters and the state space matrices are in  &lt;strong&gt;&lt;em&gt;continuous time&lt;/em&gt;&lt;/strong&gt;, the weighting matrices would no longer correspond to the artificial, \(A\), \(B\), and \(C\) matrices (which is indeed what they are in the discrete state space).&lt;/p&gt;

&lt;p&gt;&lt;a name=Model-Predictive-Controllers&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;&lt;center&gt;Model Predictive Controllers (MPC)&lt;/strong&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;A key thing about LQ closed loop problems is that we can pretty much guarantee closed loop stability if the prediction horizon, \(N \rightarrow \infty\). Model Predictive Controllers, on the other hand, employ a &lt;a href=&quot;http://www.cds.caltech.edu/%7Emurray/books/AM08/pdf/obc08-rhc_30Jan08.pdf&quot;&gt;receding (which is a finite) horizon concept&lt;/a&gt; in establishing the control sequence and closed-loop stability is generally not guaranteed. &lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;http://www.mdpi.com/energies/energies-08-01505/article_deploy/html/images/energies-08-01505-g005-1024.png&quot; width=&quot;60%&quot; height=&quot;450&quot; align=&quot;middle&quot;&gt;  
  &lt;div class=&quot;figcaption&quot; align=&quot;left&quot;&gt;Fig. 1. Receding Horizon Concept. &lt;i&gt;Courtesy, &lt;a href=&quot;http://www.mdpi.com/1996-1073/8/2/1505&quot;&gt; MDPI &lt;/a&gt;&lt;/i&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;!-- ![Receding Horizon Concept](http://www.mdpi.com/energies/energies-08-01505/article_deploy/html/images/energies-08-01505-g005-1024.png) --&gt;

&lt;p&gt;Basically, for inifinite horizon controllers such as LQ methods, this means there is no model mismatch between the predictor model and the plant. In practice, this is tough to achieve as disturbances play a large role in virtually all real-world systems. Typically in state-space or transfer function control model structures, we&amp;#39;ll add an integrator in the feedback loop to correct for offset errors at steady state. This would not be optimal in zeroing steady-state errors in an MPC approach. One way of avoiding mismatch is to include an integrator in the prediction model as an &lt;strong&gt;&lt;em&gt;internal model&lt;/em&gt;&lt;/strong&gt; of the disturbance. This could be a &lt;strong&gt;CARIMA&lt;/strong&gt;  (&lt;strong&gt;C&lt;/strong&gt;ontrolled &lt;strong&gt;A&lt;/strong&gt;uto &lt;strong&gt;R&lt;/strong&gt;egressive &lt;strong&gt;I&lt;/strong&gt;ntegral &lt;strong&gt;M&lt;/strong&gt;oving &lt;strong&gt;A&lt;/strong&gt;verage) model, for example. A typical &lt;strong&gt;CARIMA&lt;/strong&gt; model takes the polynomial form&lt;/p&gt;

&lt;p&gt;&lt;a name=eqn:carima&gt;&lt;/a&gt;
\begin{equation}  \label{eqn:carima}
  A(z^{-1})\,y(k) = B(z^{-1})\,u(k) + \dfrac{C(z^{-1})}{1-z^{-1}}\,e(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(A\), \(B\), and \(C\) are polynomials in the backward shift operator \(z^{-1}\) given by&lt;/p&gt;

&lt;p&gt;\begin{equation} \label{eqn:poly}
  A(z^{-1}) = 1 + a_1\,z^{-1} + a_2 + \, z^{-2} + \cdots + a_{n_a}z^-{n_a}  \nonumber
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
  B(z^{-1}) = b_1 + b_2\, z^{-1} + b_3 \, z^{-2} + \cdots + b_{n_b}z^-{n_b}  \nonumber
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
  C(z^{-1}) = c_0 + c_1\, z^{-1} + c_2 \, z^{-2} + \cdots + c_{n_b}z^-{n_c},
\end{equation}&lt;/p&gt;

&lt;p&gt;and \(y(k)\), \(u(k)\) and \(e(k)\), for \(k = 1, 2\), \(\cdots \)  are respectively the plant output, input and integrated noise term in the model. &lt;/p&gt;

&lt;p&gt;MPC&amp;#39;s are generally good and better than traditional PID controllers if correctly implemented in that they handle disturbances typically well and can anticipate future disturbances thereby making the control action more effective as a result. This is because of an &lt;i&gt;&lt;strong&gt;internal model&lt;/strong&gt;&lt;/i&gt; of the plant that allows them to anticipate the future &amp;quot;behavior&amp;quot; of a plant&amp;#39;s output and mitigate such errors before the plant reaches the &lt;i&gt;&amp;quot;future time&amp;quot;&lt;/i&gt;. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.sheffield.ac.uk/acse/staff/jar&quot;&gt;Rossiter&lt;/a&gt; gives a classic analogy in the way human beings cross a road. It is not sufficient that a road is not busy with passing cars on it. As you cross, you constantly look at ahead (your prediction horizon) to anticipate oncoming vehicles and update your movement (=control action) based on your prediction (based on past observations).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Accurate prediction over a selected horizon is critical to a successful MPC implementation&lt;/strong&gt;. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Below are a brief overview of common models employed in MPC algorithms. Readers are referred to System Identification texts e.g., &lt;a href=&quot;#box-jenkins&quot;&gt;Box and Jenkins&lt;/a&gt;,  &lt;a href=&quot;#billings&quot;&gt;Billings&lt;/a&gt;, or &lt;a href=&quot;#Ljung&quot;&gt;Ljung&lt;/a&gt; where data modeling and system identification methods are elucidated in details.&lt;/p&gt;

&lt;!-- &lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
  &lt;img src=&quot;/downloads/MPC/lrpc.jpg&quot; width=&quot;75%&quot; height=&quot;450&quot;, border=&quot;0&quot; style=&quot;float:center;&quot;&gt; 
  &lt;div class=&quot;figcaption&quot; align=&quot;center&quot;&gt;Fig.2.1. Receding Horizon Concept (Copyright: Mahdi Mahfouf, University of Sheffield, 2011 - 2012). 
&lt;/div&gt;
&lt;/div&gt; --&gt;

&lt;h4&gt;&lt;center&gt;&lt;strong&gt;Linear Models in Predictive Control&lt;/strong&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;An essential part of an MPC design is the internal model of the plant. If we want a high-fidelity control, we would want to develop a high fidelity model. For optimal control, our controller would only be as good as our model. If our understanding of the plant is faulty and this transfers to the model, we would typically have model mismatch between plant and predictor and our implementation of the MPC may be worse than using an infinite horizon control.&lt;/p&gt;

&lt;p&gt;We briefly discuss the common classical models that have been accepted as a standard by the control community in the past two-some decades. Linear models obey the superposition principle. Put differently, their internal structure can be approximated by a linear function in the proximity of the desired operating point. In general, linear system identification belong in two large categories: parametric and non-parametric methods. I will only touch upon parametric methods as these are the most commonly employed models in MPC approaches.&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Autoregressive Moving Average with Exogenous Input Model (ARMAX)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;The general structure of the linear finite-horizon system can be written as &lt;/p&gt;

&lt;p&gt;\begin{equation} \label{eqn:ARMAX}
  A(z^{-1})\,y(k) = \dfrac{B(z^{-1})}{F(z^{-1})}\,u(k) + \dfrac{C(z^{-1})}{D(z^{-1})}\,e(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;with \(A(z^{-1}), \, B(z^{-1}), \, C(z^{-1}) \) defined as in equation &lt;a href=&quot;#eqn:poly&quot;&gt;\eqref{eqn:poly}&lt;/a&gt; and the \(F(z^{-1}), \text{ and } D(z^{-1})\) polynomials defined as&lt;/p&gt;

&lt;p&gt;\begin{equation}
  D(z^{-1}) = 1 + d_1\,z^{-1} + d_2 + \, z^{-2} + \cdots + d_{n_d}z^{-n_d}  \nonumber
\end{equation} &lt;/p&gt;

&lt;p&gt;\begin{equation}
  F(z^{-1}) = 1 + f_1\,z^{-1} + f_2 + \, z^{-2} + \cdots + f_{n_f}z^{-n_f} 
\end{equation}&lt;/p&gt;

&lt;p&gt;The autoregressive part is given by the component \(A(z^{-1}) \, y(k)\), while the noise term is modeled as a moving average regression model in \(\dfrac{C(z^{-1})}{D(z^{-1})}\,e(k)\); the exogenous component is given by \(\dfrac{B(z^{-1})}{F(z^{-1})}\,u(k)\). &lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Autoregressive Model (AR)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If in equation \eqref{eqn:ARMAX},  \(B(z^{-1}) = 0\) and \(C(z^{-1}) = D(z^{-1}) = 1\) then  we have an AR model,&lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k) = -a_1 \, y(k-1) -  a_2 \,  y(k-2) - \cdots - a_{n_a} \, y(k-{n_a})
\end{equation}&lt;/p&gt;

&lt;p&gt;Alternatively, if \(A(z^{-1}) = 1\) , \(B(z^{-1}) = 0, \,  \text{ and } \, C(z^{-1}) = 1 \) then we end up with the regression&lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k) = - d_1 \, y(k-1) - d_2 \, y(k-2) - \cdots - d_{n_d} y(k - n_d) + e(k) 
\end{equation}&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Moving Average Model (MA)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If \(A(z^{-1}) = 1\), \(B(z^{-1}) = 0\) and \(D(z^{-1}) = 1\), then we have the following moving average noise model&lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k) = e(k)+ c_1e(k-1) + \cdots + c_{n_c}e(k-n_c)
\end{equation}&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Autoregressive  with Exogenous Input Model (ARX)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If \(F(z^{-1}) = 1\), and \(C(z^{-1}) = D(z^{-1}) =1\), we obtain the ARX structure
\begin{equation}
\begin{split}
  y(k) &amp;amp; = -a_1 \, y(k-1) - \cdots - a_{n_a} \, y(k-{n_a}) + b_1u(k-1) \
       &amp;amp;+\cdots + b_{n_b}u(k-n_b) + e(k)
       \end{split} 
\end{equation}&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Autoregressive Moving Average Model (ARMA)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;Setting \(B(z^{-1}) = 0\), and \(D(z^{-1}) = 1\), we obtain the ARMA model,
\begin{equation}
\begin{split}
  y(k) &amp;amp; = -a_1 \, y(k-1) - \cdots - a_{n_a} \, y(k-{n_a}) + c_1eu(k-1) \
       &amp;amp;+\cdots + c_{n_c}e(k-n_c)
       \end{split} 
\end{equation}&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Finite Impulse Response Model (FIR)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If in equation \eqref{eqn:ARMAX}, \(A(z^{-1}) = F(z^{-1}) = 1 \text{ and } C(z^{-1}) = 0\), we have an FIR model. This can be rewritten as &lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k+i) = \sum_{j=0}^{n_y - 1} \, h_j\, u(k-j+i-1)
\end{equation}&lt;/p&gt;

&lt;p&gt;for predictions of the process output at \(t = k+i\) with \(i \geq 1\). &lt;/p&gt;

&lt;p&gt;The FIR model has the advantage of being simple to construct in that no complex calculations are required of the model and model assumptions are required. They are arguably the most commonly used in commercial MPC packages. However, it does come up short for unstable systems and it also requires a lot of parameters to estimate an FIR model.&lt;/p&gt;

&lt;p&gt;My &lt;a href=&quot;https://github.com/SeRViCE-Lab/Matlab-Files/blob/master/ident_data/&quot;&gt;Github repo&lt;/a&gt; has various examples where typical &lt;a href=&quot;https://github.com/SeRViCE-Lab/Matlab-Files/blob/master/ident_data/Filtered%20GWN/carimaFWGN.m&quot;&gt;ARX, ARMAX, Impulse Response and AR models&lt;/a&gt;, are identified from finite data. &lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Box-Jenkins (BJ) Model&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;The BJ Model is obtained by setting 
\begin{equation} \label{eqn:BJ}
  y(k) = \dfrac{B(z^{-1})}{F(z^{-1})}\,u(k) + \dfrac{C(z^{-1})}{D(z^{-1})}\,e(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;a name=fsr&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h5&gt;&lt;center&gt; &lt;strong&gt;Finite Step Response Model (FSR)&lt;/strong&gt; &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;\begin{equation}
  y(k) = \sum_{j=0}^{n_s - 1} \, s_j\, \Delta u(k-j-1)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\Delta\) is the differencing operator (\(\Delta = 1 - q^{-1}\)) and \(q^{-1}\) is the backward shift operator. \(n_s\) is the number of step response elements \(s_j\) used in predicting \(y\).&lt;/p&gt;

&lt;h3&gt;&lt;center&gt;&lt;strong&gt;Predictions in System Models&lt;/strong&gt;&lt;/center&gt;&lt;/h3&gt;

&lt;h5&gt;&lt;center&gt;&lt;strong&gt;&lt;u&gt;Background&lt;/u&gt;&lt;/strong&gt;&lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;In principle, MPCs are long-range predictive controllers(LRPC). Contrary to classical control laws, they are potent if there is a process dead-time or if the setpoint is well-known ahead of time. They were introduced by &lt;a href=&quot;#richalet&quot;&gt;Richalet&lt;/a&gt; and &lt;a href=&quot;#cutler&quot;&gt;Cutler&lt;/a&gt; in the 1970&amp;#39;s and &amp;#39;80&amp;#39;s. As mentioned previously, predictions constitute the bulwark of model predictive controllers. Matter-of-factly, the prediction that this family of controllers have in their structure explains their robust performance when correctly implemented in a typical process. The basic algorithm involves:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;At a current time, \(k\), we predict an output, \(\hat{y}_k\), over a certain output horizon, \(n_y\), based on a mathematical model of the plant dynamics. The predicted output is a function of future possible control scenarios.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From the proposed scenarios, the strategy that delivers the best control action to bring the current process output to the setpoint is chosen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The chosen control law is applied to the real process input only at the present time \(k\).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above procedure is repeated at the next sampling instant leading to an updated control action with correctness based on latest measurements. In literature, we refer to this as the &lt;strong&gt;&lt;em&gt;receding horizon concept&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Other model-based controllers include pole-placement methods and Linear Quadratic methods. When there are no contraints on the control law and the setpoint is not complex, LQ-, pole-placement and predictive controllers generally yield an equivalent result.&lt;/p&gt;

&lt;p&gt;We will assume the model of the plant has been correctly estimated as mentioned in the previous post. The model to be controlled (= &lt;em&gt;plant&lt;/em&gt;) is used in predicting the process output over a defined prediction horizon, $n_y$. Typically, we would want to use an \(i\)-step ahead prediction based on our understanding of the system dynamics. The prediction horizon needs to be carefully selected such the computed optimal control law is not equivalent to a linear quadratic controller. Typically this happens when \(n_y - n_u\) is greater than the open-loop settling time and \(n_u\) is \(\geq\) 5. For large \(n_y - n_u\) and large \(n_u\), &lt;a href=&quot;#GPCs&quot;&gt;Generalized Predictive Controllers&lt;/a&gt; give an almost equal control to an optimal controller with the same weights. With \(n_y - n_u\) and \(n_u\) small, the resulting control law may be severely &lt;a href=&quot;#Rossiter&quot;&gt;suboptimal&lt;/a&gt;. Model predictive controllers are implemented in discrete time since control decisions are made discretely ( = instanteneously). Continuous time systems are sampled to obtain a discrete-time equivalent. The rule of thumb is that the sampling time should be \(10 - 20\) times faster than the dominant transient response of the system (e.g. rise time or settling time). &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It is important not to sample at a faster rate than the dominant transient response of the system; otherwise, the high frequency gains within the system will not be picked up by the model. In other words, we should sample fast enough to pick up disturbances, but no faster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Model Predictive Controllers are easy to implement when the model is linear and discrete but they do find applications in nonlinear processes as well. It behooves that a nonlinear model of the process would then be employed to design the controller in such an instance. Nonlinear models will be treated in a future post. Nonlinear systems that use model predictive control tend to have a more rigorous underpinning.&lt;/p&gt;

&lt;p&gt;Within the framework of model predictive controllers, there are several variants in literature. Among these variants are &lt;a name=GPCs&gt;&lt;/a&gt;Clarke&amp;#39;s Generalized Predictive Controllers (GPCs), &lt;a href=&quot;#cutler&quot;&gt;Dynamic Matrix Control&lt;/a&gt;, Extended Predictive Self-Adaptive Control (EPSAC), Predictive Functional Control (PFC), Ydstie&amp;#39;s Extended Horizon Adaptive Control (EHAC), and Unified Predictive Control (UPC) et cet&amp;#39;era. MPCs find applications in nonminimum phase systems, time-delay systems and unstable processes. I will briefly do a once-over on MAC, DMC and GPC with transfer function models and come back full circle to describe GPC with state-space models since these are generally straightforward to code and has less mathematical labyrinths.&lt;/p&gt;

&lt;h4&gt;&lt;center&gt;&lt;u&gt;&lt;strong&gt;Prediction in Richalet&amp;#39;s Model Algorithm Control (MAC)&lt;/strong&gt;&lt;/u&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;Generally, these make use of impulse response function models. Suppose we denote the output of a discrete LTI system by a discrete impulse response \(h(j)\) as in&lt;/p&gt;

&lt;p&gt;\begin{equation}
  H(z^{-1}) = A^{-1}(z^{-1}) \, B^(z^{-1})
\end{equation} &lt;/p&gt;

&lt;p&gt;it follows that the output can be written as a function of \(h(t)\) as follows&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:impulse}
  y(t) = \sum_{j=1}^{n=\infty} \, h(j) \, u(t-j)
\end{equation} &lt;/p&gt;

&lt;p&gt;where \(h(j)\) are the respective coefficients of the impulse response. If we assume a stable and causal system, for an \(n\) terminal sampling instant, equation \eqref{eqn:impulse} becomes&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:impulse-lim}
  y(t) = \sum_{j=1}^{n} \, h(j) \, u(t-j)
\end{equation} &lt;/p&gt;

&lt;p&gt;such that we can compute the recursive form of equation \eqref{eqn:impulse-lim} as&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:impulse-rec}
  y_{mdl}(t + k) = \tilde{h}^T \, \tilde{u}(t + k) = \tilde{u}^T(t + k)\, \tilde{h}
\end{equation} &lt;/p&gt;

&lt;p&gt;where, \[\tilde{h}^T = h(1), \, h(2), \, h(3), \, \ldots, \, h(n) \]
and \[\tilde{u}^T(t + k) = u(t + k - 1), \, u(t + k - 2), \, u(t + k -3), \, \ldots, \, u(t + k - n) \].&lt;/p&gt;

&lt;p&gt;The cost function is given by&lt;/p&gt;

&lt;p&gt;\begin{gather}  \label{eqn:mac-cost}
  J_{MAC} &amp;amp;= e^T \, e + \beta^2 \Delta u^T \Delta u&lt;br&gt;
\end{gather}&lt;/p&gt;

&lt;p&gt;where  \(e = y_r - y_{mdl}\), and \(\beta\) is a penalty function for the input variable \(u(t)\).&lt;/p&gt;

&lt;div class=&quot;boxed&quot;&gt;
&lt;u&gt;&lt;b&gt;MAC Algorithm Summary&lt;/b&gt;&lt;/u&gt;
&lt;ul&gt;
  &lt;li&gt;Define a reference trajectory, $y_{r}$, that $y(k)$ should track.&lt;/li&gt;
  &lt;li&gt;Tune output predictions based on an $FIR$ model order to deal with model disturbances and uncertainties. &lt;/li&gt;
  &lt;li&gt;Use $\beta$ to penalize the control law.&lt;/li&gt;
  &lt;li&gt;Formulate the output predictions using equation \eqref{eqn:impulse-rec} with the assumption that the plant is stable and causal.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;h4&gt;&lt;center&gt;&lt;strong&gt;&lt;u&gt;Prediction in Cutler&amp;#39;s Dynamic Matrix Control&lt;/u&gt;&lt;/strong&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;Here, a &lt;a href=&quot;http://lakehanne.github.io/Optimal-Controllers-MPC/#fsr&quot;&gt;finite step response model&lt;/a&gt; is employed in the prediction. It is essentially composed of three tuning factors viz., the prediction horizon, \(n_y\), control weighting factor, \(\beta\), and the control horizon, \(n_u\).&lt;/p&gt;

&lt;div class=&quot;boxed&quot;&gt;
&lt;u&gt;&lt;b&gt;DMC Assumption&lt;/b&gt;&lt;/u&gt;
&lt;ul&gt;
  &lt;li&gt;The process is stable and causal process&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;h4&gt;&lt;center&gt;&lt;u&gt;&lt;strong&gt;GPC: Prediction of the plant output described by a transfer function&lt;/strong&gt;&lt;/u&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;It seems to me that most literature is filled with prediction of the output described by a transfer function model. I assume this is due to their relative easiness of computation compared against FIR and FSR models and their applicability to unstable processes. Most papers out of Europe tend to favor transfer function methods while US researchers typically prefer state-space methods. Typically, &lt;a href=&quot;http://mathworld.wolfram.com/DiophantineEquation.html&quot;&gt;&lt;strong&gt;Diophantine identity equations&lt;/strong&gt;&lt;/a&gt; are used to form the regressor model. In my opinion, this is royally complex and more often than not obscures the detail of what is being solved. My advice is one should generally stay out of Diophantine models when possible and only use them when necessary. I will briefly expand on this treatment and focus on MPC treatments using recursive state space models when describing GPC algorithms.&lt;/p&gt;

&lt;p&gt;The transfer function model is of the form:&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:tf_model}
  y(k) = \dfrac{z^{-d}B(z^{-1})}{A(z^{-1})}u(k-1)
\end{equation}&lt;/p&gt;

&lt;p&gt;with \(A(z^{-1})\) and \(B(z^{-1})\) defined by the polynomial expansions. A close inspection of equation \eqref{eqn:tf_model} reveals that the transfer function model subsumes both the FIR (i.e. \(A\) = 1 and the coefficients of the \(B\) polynomial are the impulse response elements)  and FSR models (i.e. A = 1 and the coeeficients of the \(B\) polynomial are the step response coefficients \(b_0 = s_0; \, b_j = s_j - s_{j-1} \forall j \geq 1 \).  &lt;/p&gt;

&lt;p&gt;Transfer function models have the good properties of using greedy polynomial orders and variables in representing linear processes but an assumption about the model order has to be made. &lt;/p&gt;

&lt;h5&gt;&lt;center&gt;&lt;u&gt;&lt;strong&gt;Output predictions in TFM with Diophantine identities&lt;/strong&gt;&lt;/u&gt;&lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;Another way of writing equation \eqref{eqn:tf_model} is by forming an \(i\)-step ahead predictor as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:tf_isa}
  y(k + i) = \dfrac{z^{-d}B(z^{-1})}{A(z^{-1})}u(k + i -1)
\end{equation}&lt;/p&gt;

&lt;p&gt;It follows from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_control#Certainty_equivalence&quot;&gt;certainty equivalence principle&lt;/a&gt; that if we substitute the delay, \(d, \text{ and } B, \, A \) polynomials with their estimates (i.e. \(\hat{d}, \hat{B}, \, \text{and} \hat{A} \)), the optimal control solution obtained would be the same as the system&amp;#39;s delay, \(d, \text{ and } B, \text{ and } A \) polynomials. Let&amp;#39;s form the predicted output estimate as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}   \label{eqn:tf_pred}
  \hat{y}(k + i) = \dfrac{z^{-\hat{d}}\hat{B}(z^{-1})}{\hat{A}(z^{-1})}u(k + i -1)
\end{equation}&lt;/p&gt;

&lt;p&gt;We could rearrange the above equation as&lt;/p&gt;

&lt;p&gt;\begin{split}&lt;br&gt;
  \hat{y}(k + i) = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - \hat{y}(k + i)[\hat{A}(z^{-1}) -1] \nonumber 
\end{split}&lt;/p&gt;

&lt;p&gt;Since \(A(z^{-1}) = 1 + a_1\,z^{-1} + a_2 + \, z^{-2} + \cdots + a_{n_a}z^{-n_a} \), by definition, we can write &lt;/p&gt;

&lt;p&gt;\begin{align}&lt;br&gt;
  \hat{y}(k + i) &amp;amp;= z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - \hat{y}(k + i)(a_1\,z^{-1} + a_2 z^{-2} + \cdots + a_{n_a}z^{-n_a}) \nonumber \
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
    \qquad \qquad &amp;amp;= z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - a_1 \hat{y}(k + i -1) - a_2 \hat{y}(k + i -2) - \cdots - a_{n_a} \hat{y}(k + i - n_a)    \nonumber
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
    &amp;amp; = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - (a_1 + a_2 z^{-1} + \cdots + a_{n_a}z^{-n_a + 1}) \hat{y}(k + i - 1) \nonumber
\end{align}&lt;/p&gt;

&lt;p&gt;such that we can write &lt;/p&gt;

&lt;p&gt;\begin{align}  \label{eqn:tf_isa2}
    \hat{y}(k + i) &amp;amp; = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - z(\hat{A} - 1) \hat{y}(k + i - 1) 
\end{align}&lt;/p&gt;

&lt;p&gt;from equation \eqref{eqn:tf_pred}, where \(z(\hat{A} - 1) = a_1 + a_2 z^{-1} + \, z^{-1} + \cdots + a_{n_a}z^{-n_a + 1}\).&lt;/p&gt;

&lt;p&gt;The equation above is the \(i\)-step ahead predictor using the estimates and runs independently of the process. However, it is a fact of life that disturbances and uncertainties get a vote in any prediction model of a physical/chemical process. Therefore equation \eqref{eqn:tf_isa2} is not well-posed. To avoid prediction errors, we could replace \(\hat{y}(k)\) with \(y(k) \) in the equation and rearrange the model a bit further. Let&amp;#39;s introduce the Diophantine identity equation &lt;/p&gt;

&lt;p&gt;&lt;a name=Dioph&gt;&lt;/a&gt;
&lt;div class=&quot;boxed&quot;&gt;
&lt;b&gt;&lt;center&gt;Diophantine Identity Equation&lt;/center&gt;&lt;/b&gt;
&lt;ul&gt;
\begin{equation}&lt;br&gt;
    \dfrac{1}{\hat{A}} = E_i + z^{-i}\dfrac{F_i}{\hat{A}}
\end{equation}
&lt;/ul&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;where degree of \(E_i \leq i -1\) and \(F_i\) being of degree \(n_A -1\).&lt;/p&gt;

&lt;p&gt;Multiplying out equation \eqref{eqn:tf_pred} with \(E_i\) gives&lt;/p&gt;

&lt;p&gt;\begin{align}
    E_i \hat{A}(z^{-1}) \hat{y}(k+i) &amp;amp; = z^{-\hat{d}} E_iB(z^{-1}) u(k+i-1)
\end{align}&lt;/p&gt;

&lt;p&gt;Rearranging the Diophantine equation and substituting \(E_i \hat{A}(z^{-1}) = 1 - z^{-i}F_i \) into the above equation, we find that&lt;/p&gt;

&lt;p&gt;\begin{align}   \label{eqn:rigged}
    \hat{y}(k+i) &amp;amp; = z^{-\hat{d}} E_iB(z^{-1}) u(k+i-1) + F_i \underbrace{\hat{y}(k)}_{\textbf{replace with \(y(k)\)}}.
\end{align}&lt;/p&gt;

&lt;p&gt;But&lt;/p&gt;

&lt;p&gt;\begin{equation}
    E_i\hat{B}(z^{-1}) = \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} - \dfrac{q^{-i} \hat{B}(z^{-1}) F_i}{\hat{A(z^{-1})}},
\end{equation}&lt;/p&gt;

&lt;p&gt;if we multiply the &lt;a href=&quot;#Dioph&quot;&gt;Diophantine equation&lt;/a&gt; by \(\hat{B}(z^{-1})\) such that &lt;/p&gt;

&lt;p&gt;\begin{align}  \label{eqn:tf_isasep}
    \hat{y}(k+i) &amp;amp; = z^{-\hat{d}} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} u(k+i-1) + F_i y(k) - z^{-d} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} F_i u(k -1)   \nonumber
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
\qquad \qquad \quad &amp;amp; = \underbrace{z^{-\hat{d}} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} u(k+i-1)} + \underbrace{F_i (y(k) - \hat{y}(k) ) }_{\textbf{correction } }
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
\quad  &amp;amp;  \textbf{prediction} \nonumber 
\end{align}&lt;/p&gt;

&lt;p&gt;We see that equation \eqref{eqn:tf_isasep} nicely separates the output predictor into a prediction part (&lt;i&gt;from the past input&lt;/i&gt;) and a correction part (based on error between model and prediction at time, \(k\) ). Essentially, we have manipulated the equation \(\eqref(eqn:rigged)\) such that we have a correcting term in the prediction of the output by subsituting \(y(k)\) in place of \(\hat{y}(k)\) in equation \eqref(eqn:rigged) to negate issue of model-plant mismatch.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
&lt;a name=box-jenkins&gt;&lt;/a&gt;[Box and Jenkins]: Box, G.E.P. and Jenkins, G.M., &amp;#39;&lt;em&gt;Time Series Analysis:Forecasting and Control. San Francisco, CA&lt;/em&gt;&amp;#39;, Holden Day, 1970.&lt;/p&gt;

&lt;p&gt;&lt;a name=billings&gt;&lt;/a&gt;[Billings] :  Stephen A. Billings. &amp;#39;&lt;em&gt;Nonlinear System Identification. NARMAX Methods in the Time, Frequency and Spatio-Temporal Domains&lt;/em&gt;&amp;#39;. John Wiley &amp;amp; Sons Ltd, Chichester, West Sussex, United Kingdom,  2013.&lt;/p&gt;

&lt;p&gt;&lt;a name=cutler&gt;&lt;/a&gt;&lt;a href=&quot;https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004232009&quot;&gt;[C. R. Cutler, B. L. Ramaker]: Dynamic matrix control -- A computer control algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=richalet&gt;&lt;/a&gt; [J. Richalet, A. Rault, J.L. Testud and J. papon]: &amp;#39;Model Predictive Heuristic Control: Applications to Industrial Processes&amp;#39;, &lt;em&gt;Automatica&lt;/em&gt;, Vol. 14. No. 5, pp. 413 - 428, 1978.&lt;/p&gt;

&lt;p&gt;&lt;a name=Rossiter&gt;&lt;/a&gt;   &lt;a href=&quot;https://www.crcpress.com/Model-Based-Predictive-Control-A-Practical-Approach/Rossiter/9780849312915&quot;&gt;[J.A. Rossiter]:   &amp;#39;Model Predictive Control: A Practical Approach&amp;#39;. CRC Press LLC, Florida, USA. 2003&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&lt;a name=Gawthrop&gt;&lt;/a&gt;[Gawthrop] : &amp;#39;&lt;em&gt;Continuous-Time Self-Tuning Control&lt;/em&gt;&amp;#39; Vols I and II, Tannton, Research Studies Press, 1990.&lt;/p&gt;

&lt;p&gt;&lt;a name=Ljung&gt;&lt;/a&gt;[Ljung] : L. Ljung. &amp;#39;&lt;em&gt;System Identification Theory for the User&lt;/em&gt;&amp;#39;. 2nd Edition. Upper Saddle River, NJ, USA. Prentice Hall, 1999.&lt;/p&gt;

&lt;p&gt;&lt;a name=Soeterboek&gt;&lt;/a&gt; &lt;a href=&quot;http://repository.tudelft.nl/view/ir/uuid%3A18a07849-f43c-4d98-afb2-0b8a3a2e8b20/&quot;&gt;[Ronald Soeterboek]. &amp;#39;Predictive Control: A Unified Approach&amp;#39;. Prentice Hall International (UK) Limited, 1992&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/br&gt;
&lt;p id=&quot;footnote-1&quot;&gt;[1] A well-posed \(J\) should have an unbiased predictions in steady state. That is the minimizing argument of \(J\) must be consistent with an offset-free tracking. This implies the predicted control sequence to maintain zerto tracking error must be zero. The minimum of \(J = 0\) in steady state is equivalent to \(r_{\rm \rightarrow}\) \(- \, y_{\rm \rightarrow} = 0\) and \(u_{k+i} - u_{k+i-1} = 0\).&lt;/p&gt;

&lt;!-- (Note that \\(x\_{\rm \rightarrow}\\) denotes future values of \\(x\\) ).  --&gt;
</description>
        <pubDate>2015-11-03 04:21:00 -0600</pubDate>
        <link>lakehanne.github.ioOptimal-Controllers-MPC</link>
        <guid isPermaLink="true">lakehanne.github.ioOptimal-Controllers-MPC</guid>
        
        
        <category>lakehanne.github.io</category>
        
      </item>
    
      <item>
        <title>&lt;center&gt;Mathematical Modeling of Robots (Introduction).&lt;/center&gt;</title>
        <description>&lt;!-- Analytics --&gt;

&lt;script&gt;
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-64680332-1', 'auto');
  ga('send', 'pageview');
    ga('send', 'pageview' '/robots-modeling');

&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;
  src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;!-- Google Tag Manager --&gt;

&lt;noscript&gt;&lt;iframe src=&quot;//www.googletagmanager.com/ns.html?id=GTM-N9TQBW&quot;
height=&quot;0&quot; width=&quot;0&quot; style=&quot;display:none;visibility:hidden&quot;&gt;&lt;/iframe&gt;&lt;/noscript&gt;

&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-N9TQBW');&lt;/script&gt;

&lt;!-- End Google Tag Manager --&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;This is a lecture I delivered to UT Dallas EE Senior students on the first day of class in Fall 2015 reposted from &lt;a href=&quot;http://service-lab.github.io/Lecture-1/&quot;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;The Three Laws of Robotics\(^1\).&lt;/strong&gt;&lt;/center&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A robot may not injure a human being or, through inaction, allow a human being to come to harm.&lt;/li&gt;
&lt;li&gt;A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.&lt;/li&gt;
&lt;li&gt;A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ## Instructor: [Olalekan Ogunmolu](http://github.com/lakehanne).
    Date of Lecture: August 25, 2015.
    Venue:           ECSN 2.126, UT Dallas, Richardson, Texas. --&gt;

&lt;h2&gt;Table of Contents:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#objectives&quot;&gt;Course Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#topics&quot;&gt;Topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#policy&quot;&gt;Grading Policy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#maths&quot;&gt;Mathematical Modeling of Robots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#common-robot-arm-configurations&quot;&gt;Common Robot Arm Configurations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#add&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name='objectives'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;1. Course Objectives&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Students will learn and utilize the mathematical representation of rigid body motions,
including homogeneous transformations, to solve for position and orientation and
velocities of objects. They will apply this by programming physical robotics systems.&lt;/li&gt;
&lt;li&gt;Students will formulate and solve forward and inverse kinematic equations and write
programs to solve these equations and carry them out on physical robots.&lt;/li&gt;
&lt;li&gt;Students will formulate and solve velocity kinematics equations and equations and write
programs to solve these equations and carry them out on physical robots.&lt;/li&gt;
&lt;li&gt;Student will learn the mathematical procedures for robot motion planning and trajectory
generation and execute such algorithms in simulation and programming robots.&lt;/li&gt;
&lt;li&gt;Students will expound the mathematical theory and physical implementation of common
robot sensors including rotary encoders and cameras, as well as write programs to read
and process data from such sensors to send control commands to robots.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name='topics'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;2. Topics&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Introduction: Historical development of robots; basic terminology and structure; robots in
automated manufacturing&lt;/li&gt;
&lt;li&gt;Rigid Motions and Homogeneous Transformation: Rotations and their composition;
Euler angles; roll-pitch-yaw; homogeneous transformations&lt;/li&gt;
&lt;li&gt;Forward Kinematics: Common robot configurations; Denavit-Hartenberg convention; Amatrices;
T-matrices&lt;/li&gt;
&lt;li&gt;Inverse kinematics: Planar mechanisms; geometric approaches; spherical wrist&lt;/li&gt;
&lt;li&gt;Velocity kinematics: Angular velocity and acceleration; The Jacobian; singular
configurations; singular values; pseudoinverse; manipulability&lt;/li&gt;
&lt;li&gt;Motion planning: Configuration space; artificial potential fields; randomized methods;
collision detection&lt;/li&gt;
&lt;li&gt;Trajectory generation: Joint space interpolation; polynomial splines; trapezoidal velocity
profiles; minimum time trajectories&lt;/li&gt;
&lt;li&gt;Vision-based control: The geometry of image formation; feature extraction; feature
tracking; the image Jacobian; visual servo control&lt;/li&gt;
&lt;li&gt;Advanced Topics (one or more of the following depending on time and class interest):
Lagrangian dynamics; path planning; mobile robots; force sensing and force control; &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;policy&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;3. Grading Policy&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Labs: 25%&lt;/li&gt;
&lt;li&gt;Homeworks: 25%&lt;/li&gt;
&lt;li&gt;Exam 1: 25%&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exam 2:  25%&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Targeted grade ranges:  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                - A: 90-100%

                - B: 80-89%

                - C: 70-79%

                - D: 60-69%

                - F: &amp;lt;60%
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a mixed class. Graduate and undergraduate students will be graded and
curved separately.&lt;/p&gt;

&lt;p&gt;There will be two exams given during the semester. In the event of an excused
absence (illness, job-related travel, holy day absence, etc.), students
&lt;u&gt;&lt;strong&gt;must&lt;/strong&gt;&lt;/u&gt; inform the instructor ahead of time and provide proper documentation of
the conflict. An accommodation will be attempted for verifiable problems.
Homework assignments will be collected graded and discussed in class (as time
permits). &lt;/p&gt;

&lt;p&gt;Homework will be collected at the beginning of the class period when
it is due. Homework that is not reasonably neat and readable, or not bound,
will be marked down. &lt;u &gt;&lt;strong&gt;Late Homework will not be accepted&lt;/strong&gt;&lt;/u&gt;. It may not be
returned to you until a week after it is due, which means you may not have it
back for a problem-solving session, or to use in studying for an exam. &lt;u &gt;&lt;strong&gt;If you
want to have it available at these times, you will have to make a photocopy
of it before you turn it in.&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name='maths'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;4.  Mathematical Modeling of Robots&lt;/h2&gt;

&lt;h3&gt;4.1.  DEFINITION OF A ROBOT&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;A robot is a reprogrammable, multifunctional manipulator designed to move material, parts, tools, or specialized devices through variable programmed motions for the performance of a variety of tasks.&lt;/em&gt;    &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        -- Spong, Hutchinson and Vidyasagar, Robot Modeling and Control (2006)    
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;

&lt;p&gt;Basically, a robot should be able to &lt;code&gt;sense&lt;/code&gt;, &lt;code&gt;move&lt;/code&gt; and &lt;code&gt;act intelligently&lt;/code&gt;. Put difeerently, a robot should have sensors on it to enable it to be environmentally-aware. It should be equipped with mechanical parts such as electric motors to make it able to navigate its environment. Lastly, it needs some &amp;#39;smarts&amp;#39; (otherwise referred to as &lt;code&gt;reprogrammability&lt;/code&gt; in engineering literature) to make it intelligent. The reporgrammability aspect of the definition is very important because it gives a robot &lt;code&gt;adaptibility&lt;/code&gt; and &lt;code&gt;usefulness&lt;/code&gt; making the robot &lt;code&gt;reconfigurable&lt;/code&gt;.   &lt;/p&gt;

&lt;h3&gt;4.2.  ROBOT MANIPULATORS&lt;/h3&gt;

&lt;p&gt;Robot manipulators are made up of &lt;u&gt; &lt;strong&gt;links&lt;/strong&gt;&lt;/u&gt;, \(i\), which are connected by &lt;u&gt;&lt;strong&gt;joints&lt;/strong&gt;&lt;/u&gt;, \(j_i\), to form a &lt;u&gt;&lt;strong&gt;kinematic chain&lt;/strong&gt;&lt;/u&gt;. Joints are typically rotary (henceforth referred to as &lt;em&gt;revolute&lt;/em&gt;) or linear (henceforth referred to as &lt;em&gt;prismatic&lt;/em&gt;) with a simple &lt;code&gt;end effector&lt;/code&gt; for manipulating work pieces. Applications vary from simple tasks such as pick and place operations to navigating complex environments (&lt;a href=&quot;https://www.youtube.com/watch?v=_luhn7TLfWU&quot;&gt;MIT Cheetah Robot&lt;/a&gt;) and medical robots such as the &lt;a href=&quot;http://www.davincisurgery.com/da-vinci-surgery/da-vinci-surgical-system/&quot;&gt;Davinci Surgical System&lt;/a&gt; among others.&lt;/p&gt;

&lt;p&gt;A &lt;u&gt;&lt;strong&gt;revolute joint&lt;/strong&gt;&lt;/u&gt; is akin to a hinge and allows relative rotation between two links. A &lt;u&gt;&lt;strong&gt;prismatic&lt;/strong&gt;&lt;/u&gt; joint allows a linear relative motion between the adjoint links. We will denote the revolute joints by \(R\) and the prismatic joints by \(P\).&lt;/p&gt;

&lt;p&gt;An example of a revolute/ joint is depicted in the figure below:&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/Lec1/Revolute.png&quot; width=&quot;49%&quot; height=&quot;250&quot; align=&quot;middle&quot;&gt;  
  &lt;img src=&quot;/assets/Lec1/Prismatic.png&quot; width=&quot;49%&quot; height=&quot;250&quot; style=&quot;border-left: 1px solid black;&quot;&gt;
  &lt;div class=&quot;figcaption&quot; align=&quot;right&quot;&gt;Fig. 1. An example of a revolute joint (left) and a prismatic joint (right). &lt;i&gt;Courtesy, &lt;a href=&quot;https://code.google.com/p/impsim/wiki/jmanual_jointRevolute&quot;&gt;  impsim. &lt;/a&gt;&lt;/i&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The angle \(\Phi\) in the left figure is defined as the joint angle and it connects links &lt;strong&gt;LK1&lt;/strong&gt; and &lt;strong&gt;LK2&lt;/strong&gt;. Similarly, the displacement \(\Phi\) in the right figure connects  links &lt;strong&gt;LK1&lt;/strong&gt; and &lt;strong&gt;LK2&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;&lt;u&gt;Rotations follow the right hand rule&lt;/u&gt;. Essentially, this means if we have three orthonormal vectors \(x\), \(y\), and \(z\) \(\in\) \(\mathcal{R}^3\) which define a coordinate frame, they must satisfy the mathematical relation, 
&lt;center&gt;\(z\) = \(x\) \(\times\) \(y\).&lt;/center&gt;
We will denote the axis of rotation of a revolute joint or the axis of displacement of a prismatic joint as \(z^i\) if the joint connects links \(i\) and \(i+1\). \(\Phi\) is referred to as the &lt;strong&gt;joint variable&lt;/strong&gt; for a revolute or prismatic joint as the case may be.&lt;/p&gt;

&lt;h3&gt;Things to remember&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;There are two types of joints: prismatic and revolute joints.&lt;/li&gt;
&lt;li&gt;\(\Phi\) is referred to the joint variable.&lt;/li&gt;
&lt;li&gt;A revolute joint will allow rotation between two links.&lt;/li&gt;
&lt;li&gt;A prismatic joint will allow displacement between two links.&lt;/li&gt;
&lt;li&gt;The axis of rotation for a typical joint, \(j_i\) that connects links \(i\) and \(i+1\) is \(z^i\).&lt;/li&gt;
&lt;li&gt;Angles are measured in a clockwise manner so that if an angle along a directed axis is positive if it represents a clockwise rotation about the direction from which we are viewing. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A three link arm with 2 revolute joints for example is referred to as an &lt;strong&gt;RRP&lt;/strong&gt; arm, where the R&amp;#39;s stand for &lt;em&gt;revolute&lt;/em&gt; and the P stands for &lt;em&gt;prismatic&lt;/em&gt;. An example of an RRP robotic arm is the SCARA robot which we shall deal with shortly.&lt;/p&gt;

&lt;h3&gt;4.3.  Taxonomy of Robot Manipulators&lt;/h3&gt;

&lt;h4&gt;4.3.1. Configuration&lt;/h4&gt;

&lt;p&gt;The configuration of a manipulator is a complete description of the location of every point on the robot manipulator.
When we know the set of all possible configurations, we say we know the &lt;strong&gt;configuration space&lt;/strong&gt; of the robot. The configuration space will correspond mathematically to knowing the set of all possible \(\theta_i\) for a revolute joint or \(d_i\) for a prismatic robot where \(\theta_i\) denotes the respective joint angles and \(d_i\) denotes the respective displacements. The set of angles \(\theta_i\) for a revolute joint is naturally associated with a unit circle in the plane denoted by \(\mathcal{S}^1\). It is typical for you to see revolute joints written as \(\theta_i\) \(\in\)  \(\mathcal{S}^1\) in literature. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt; For a single revolute joint arm, the configuration space is \(\mathcal{S}^1\), which geometrically is a one-dimensional sphere (or 1-D sphere).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 2:&lt;/strong&gt; A two-revolute joint arm will have \(\mathcal{S}^1\) \(\times\) \(\mathcal{S}^1\) configuration space. This is visually equivalent to moving an outer circle arrount an inner concentric circle. This is called a torus (donut-shaped).&lt;/p&gt;

&lt;p&gt;&lt;center&gt; &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/c/c6/Simple_Torus.svg&quot; alt=&quot;Torus&quot;&gt;&lt;/center&gt;
&lt;center&gt; Fig 2. An example of a Torus. &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 3:&lt;/strong&gt; For a one revolute and one prismatic joint, the configuration space is \(\mathcal{S}^1\) \(\times\) \(\mathcal{R}\) which is geometrically the equivalent of a cylinder.&lt;/p&gt;

&lt;h4&gt;4.3.2.  Degrees of freedom&lt;/h4&gt;

&lt;p&gt;The number of degrees of freedom of a robot is the minimum number of parameters required to specify the configuration of a robot. Generally, this is equivalent to the size of the configuration space. Typically, the number of joints for a robotic manipulator will tell us about how many degrees of freedom it has. The &lt;a href=&quot;http://www.robai.com/robots/robot/cyton-epsilon-300/&quot;&gt;cyton arm robot&lt;/a&gt; which we will use in most of the labs that accompany this class has seven joints and hence seven degrees of freedom. A rigid object in three-dimensional space will have six degrees of freedom(DOF) because it will have 3 dedicated to orientation and 3 dedicated to translation. When the DOF is lesser than 6, the robot is said to be &lt;u&gt;underactuated&lt;/u&gt;. An example of underactuated robots include quadrotors with four rotors. When a manipulator has more than 6 DOF, we say such a robot is &lt;u&gt;kinematically redundant&lt;/u&gt;.&lt;/p&gt;

&lt;h4&gt;4.3.3.  The State&lt;/h4&gt;

&lt;p&gt;The state includes the geometry of all inputs, all disturbances, velocities, forces and et cetera that determine the current and future response of the manipulator.&lt;/p&gt;

&lt;h4&gt;4.3.4.  The State Space&lt;/h4&gt;

&lt;p&gt;The state space is the set of all possible states of a manipulator. &lt;/p&gt;

&lt;h4&gt;4.3.5.  The Workspace&lt;/h4&gt;

&lt;p&gt;The volume of the motion traversed by the end-effector as the manipulator executes possible motions is called the workspace. Often separated into the &lt;b&gt;reachable workspace&lt;/b&gt; and &lt;b&gt;dexterous workspace&lt;/b&gt; depending on how many of the possible set of points an end-effector can reach based. The &lt;strong&gt;reachable workspace&lt;/strong&gt; is the set of all possible points the manipulator can reach while the &lt;strong&gt;dexterous workspace&lt;/strong&gt; is the set of points the manipulator can reach based on an arbitrary orientation of the end effector.&lt;/p&gt;

&lt;h2&gt;4.4.  Accuracy and Repeatability&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;accuracy&lt;/strong&gt; of a manipulator is a measure of how close the manipulator can reach a specific point \(x, y, z\) in the state space. There is no absolute correct way to measure the accuracy of a robot. One method is to use position encoders in joints at known locations. This would use the assumed geometry of the manipulator and its rigidity to infer the end-effector position from the measured joint positions. It becomes apparent therefore that accuracy is affected by computational errors, machining accuracy during the manipulator construction, gear backlash among other things.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Repeatability&lt;/strong&gt; is a measure of how close a manipulator can return to a previously taught point. The &lt;u&gt;resolution of the controller&lt;/u&gt; will affect a manipulator&amp;#39;s repeatability. The resolution is the smallest degree of increase in manipulator motion that a robot can sense and is the ratio of the total distance travelled and \(2^n\) where \(n\) is the number of bits of the encoder accuracy. Therefore a prismatic axis will have a greater resolution compared to a revolute joint (a straight line is shorter than an arc length).&lt;/p&gt;

&lt;p&gt;&lt;a name='common-robot-arm-configurations'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;5. Common Robot Arm Configurations&lt;/h2&gt;

&lt;p&gt;Following the prismatic and revolute joint taxonomy, there are many possiblities in the way a manipulator arm can be designed. This section discusses the common design attributes of the typical arrangements.&lt;/p&gt;

&lt;h3&gt;5.1.  Articulated Manipulator (RRR)&lt;/h3&gt;

&lt;p&gt;This is otherwise called the &lt;strong&gt;anthromorphic&lt;/strong&gt; manipulator named out of its anthropomorphic characteristics. It has three revolute joints with each axis designated as the &lt;strong&gt;waist&lt;/strong&gt; (\(z_0\)), &lt;strong&gt;shoulder&lt;/strong&gt; (\(z_1\)), and &lt;strong&gt;elbow&lt;/strong&gt; (\(z_2\)). More often than not, the joint axis \(z_2\) will be parallel to \(z_1\) while both \(z_2\) and \(z_1\) will be perpendicular to \(z_0\). The revolute manipulator has a considerably large degree of freedom of movement in a compact space.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;/assets/Lec1/RRR.png&quot; alt=&quot;Articulated Robot rm&quot;&gt;&lt;/center&gt;
&lt;center&gt;Fig 5.1. Symbolic representation of a six-DOF elbow manipulator with links and joints similar to those of a a human joint/limbs which explains why it is called an anthroppmorphic robot. Photo courtesy of [\(^2\)].&lt;/center&gt;&lt;/p&gt;

&lt;h3&gt;5.2.  The Spherical Manipulator (RRP)&lt;/h3&gt;

&lt;p&gt;If we replace the elbow or last joint in the &lt;strong&gt;RRR&lt;/strong&gt; manipulator with a prismatic joint, we would be left with what is called the &lt;strong&gt;spherical manipulator&lt;/strong&gt;. Spherical joints are capable of arbitrary rotations and the name derives from the fact that the &amp;quot;joint coordinates coincide with the spherical coordinates of the end effector relative to a coordinate frame located at the shoulder joint&amp;quot;\(1\). Passive spherical joints is consists of a &lt;em&gt;ball and socket&lt;/em&gt; joint. This does not work adequately if the joint is to exert forces and torques and hence actuated spherical joints are often constructed such that three revolute joints are combined with motors to the end of making their axes intersect at a point\(^3\).&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;/assets/Lec1/Spherical.png&quot; alt=&quot;Spherical Robot rm&quot;&gt;&lt;/center&gt;
&lt;center&gt;Fig 5.2. Symbolic representation of a spherical arm (left); The Stanford Arm (right) is an example of a spherical manipulator. Photo courtesy of [\(^2\)].&lt;/center&gt;&lt;/p&gt;

&lt;h3&gt;5.3.  The SCARA Manipulator (RRP)&lt;/h3&gt;

&lt;p&gt;Short for &lt;strong&gt;S&lt;/strong&gt;elective &lt;strong&gt;Co&lt;/strong&gt;mpliant &lt;strong&gt;A&lt;/strong&gt;rticulated &lt;strong&gt;R&lt;/strong&gt;obot for &lt;strong&gt;A&lt;/strong&gt;ssembly. Introduced in 1979 in Japan and the United States. Typically used for pick and place operations.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;/assets/Lec1/SCARA.png&quot; alt=&quot;The Adept One robot&quot;&gt;&lt;/center&gt;
&lt;center&gt;Fig.5.3.  Symbolic representation of the Adept One Robot. Photo Courtesy of [\(^3\)].&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;It is a bit different from the Spherical Manipulators in that its \(z_0\), \(z_1\) and \(z_2\) are all parallel to one another.&lt;/p&gt;

&lt;h3&gt;5.4.  The Cylindrical Manipulator (RPP)&lt;/h3&gt;

&lt;p&gt;The cylindrical manipulator has two independent degrees of freedom and is typically a combination of a revolute and two prismatic joints such that their axes &lt;strong&gt;intersect&lt;/strong&gt;. An example of the cylindrical joint is shown in fig 5.4.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;http://icosym-nt.cvut.cz/odl/partners/fuh/EXAMPLES/eqs5/stan_FM.gif&quot; width=&quot;60%&quot; height=&quot;500&quot; align=&quot;middle&quot;&gt; 
  &lt;div class=&quot;figcaption&quot; align=&quot;middle&quot;&gt;Fig.5.4.  Symbolic representation of the Cylindrical Robot. Photo Courtesy of [4]&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3&gt;5.5.  The Cartesian Manipulator (PPP)&lt;/h3&gt;

&lt;p&gt;We say a manipulator is cartesian of its first three joints are prismatic. The variables of the joints are the cartesian Cartesian coordinates with respect to the base. They find uses in sealant application to materials as in table-top assembly, as gantry robots e.g. for cargo transfer etc. Its axes are coincident to a Cartesuab coordinator&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://prime.jsc.nasa.gov/ROV/images/cartesian.GIF&quot; alt=&quot;An example cartesian robot manipulator&quot;&gt;&lt;/center&gt;
&lt;center&gt;Fig. 5.5. An example cartesian robot manipulator.&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name='summary'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this class, we have covered the mathematical basis of robotics and built a foundation upon which we shall build the next several modules. Please go through the material posted on elearning and chapter 1 of Dr. Spong&amp;#39;s book [\(^2\)] to get familiarized all the more with the topics we have discussed so far. &lt;/p&gt;

&lt;p&gt;We&amp;#39;ll see you in the next class.&lt;/p&gt;

&lt;p&gt;&lt;a name='add'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;1.  &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics&quot;&gt;Asimov, Isaac (circa 1950) I, Robot.&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;2.  Robot Modeling and Control, Mark W. Spong, Seth Hutchinson and M. Vidyasagar. John Wiley &amp;amp; Sons Inc. 2006.&lt;/li&gt;
&lt;li&gt;3.  A Mathematical Introduction to Robotic Manipulation, Richard Murray, Zexiang Li and S. Shankar Sastry. CRC Press. 1994.&lt;/li&gt;
&lt;li&gt;4.  &lt;a href=&quot;http://icosym-nt.cvut.cz/odl/partners/fuh/EXAMPLES/eqs5/stan_FM.gif&quot;&gt;Dynast&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
&lt;a href=&quot;https://twitter.com/share&quot; class=&quot;twitter-share-button&quot; data-via=&quot;patmeansnoble&quot;&gt;Tweet&lt;/a&gt;
&lt;script&gt;!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');&lt;/script&gt;
--&gt;
</description>
        <pubDate>2015-08-24 00:00:00 -0500</pubDate>
        <link>lakehanne.github.iorobots-modeling</link>
        <guid isPermaLink="true">lakehanne.github.iorobots-modeling</guid>
        
        
        <category>lakehanne.github.io</category>
        
      </item>
    
      <item>
        <title>&lt;center&gt;Mathematical Modeling of Robots (Introduction).&lt;/center&gt;</title>
        <description>&lt;!-- Analytics --&gt;

&lt;script&gt;
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-64680332-1', 'auto');
  ga('send', 'pageview');
    ga('send', 'pageview' '/robots-modeling');

&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;
  src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;!-- Google Tag Manager --&gt;

&lt;noscript&gt;&lt;iframe src=&quot;//www.googletagmanager.com/ns.html?id=GTM-N9TQBW&quot;
height=&quot;0&quot; width=&quot;0&quot; style=&quot;display:none;visibility:hidden&quot;&gt;&lt;/iframe&gt;&lt;/noscript&gt;

&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-N9TQBW');&lt;/script&gt;

&lt;!-- End Google Tag Manager --&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;This is a lecture I delivered to UT Dallas EE Senior students on the first day of class in Fall 2015 reposted from &lt;a href=&quot;http://service-lab.github.io/Lecture-1/&quot;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;The Three Laws of Robotics\(^1\).&lt;/strong&gt;&lt;/center&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A robot may not injure a human being or, through inaction, allow a human being to come to harm.&lt;/li&gt;
&lt;li&gt;A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.&lt;/li&gt;
&lt;li&gt;A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ## Instructor: [Olalekan Ogunmolu](http://github.com/lakehanne).
    Date of Lecture: August 25, 2015.
    Venue:           ECSN 2.126, UT Dallas, Richardson, Texas. --&gt;

&lt;h2&gt;Table of Contents:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#objectives&quot;&gt;Course Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#topics&quot;&gt;Topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#policy&quot;&gt;Grading Policy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#maths&quot;&gt;Mathematical Modeling of Robots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#common-robot-arm-configurations&quot;&gt;Common Robot Arm Configurations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#add&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name='objectives'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;1. Course Objectives&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Students will learn and utilize the mathematical representation of rigid body motions,
including homogeneous transformations, to solve for position and orientation and
velocities of objects. They will apply this by programming physical robotics systems.&lt;/li&gt;
&lt;li&gt;Students will formulate and solve forward and inverse kinematic equations and write
programs to solve these equations and carry them out on physical robots.&lt;/li&gt;
&lt;li&gt;Students will formulate and solve velocity kinematics equations and equations and write
programs to solve these equations and carry them out on physical robots.&lt;/li&gt;
&lt;li&gt;Student will learn the mathematical procedures for robot motion planning and trajectory
generation and execute such algorithms in simulation and programming robots.&lt;/li&gt;
&lt;li&gt;Students will expound the mathematical theory and physical implementation of common
robot sensors including rotary encoders and cameras, as well as write programs to read
and process data from such sensors to send control commands to robots.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name='topics'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;2. Topics&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Introduction: Historical development of robots; basic terminology and structure; robots in
automated manufacturing&lt;/li&gt;
&lt;li&gt;Rigid Motions and Homogeneous Transformation: Rotations and their composition;
Euler angles; roll-pitch-yaw; homogeneous transformations&lt;/li&gt;
&lt;li&gt;Forward Kinematics: Common robot configurations; Denavit-Hartenberg convention; Amatrices;
T-matrices&lt;/li&gt;
&lt;li&gt;Inverse kinematics: Planar mechanisms; geometric approaches; spherical wrist&lt;/li&gt;
&lt;li&gt;Velocity kinematics: Angular velocity and acceleration; The Jacobian; singular
configurations; singular values; pseudoinverse; manipulability&lt;/li&gt;
&lt;li&gt;Motion planning: Configuration space; artificial potential fields; randomized methods;
collision detection&lt;/li&gt;
&lt;li&gt;Trajectory generation: Joint space interpolation; polynomial splines; trapezoidal velocity
profiles; minimum time trajectories&lt;/li&gt;
&lt;li&gt;Vision-based control: The geometry of image formation; feature extraction; feature
tracking; the image Jacobian; visual servo control&lt;/li&gt;
&lt;li&gt;Advanced Topics (one or more of the following depending on time and class interest):
Lagrangian dynamics; path planning; mobile robots; force sensing and force control; &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;policy&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;3. Grading Policy&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Labs: 25%&lt;/li&gt;
&lt;li&gt;Homeworks: 25%&lt;/li&gt;
&lt;li&gt;Exam 1: 25%&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exam 2:  25%&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Targeted grade ranges:  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                - A: 90-100%

                - B: 80-89%

                - C: 70-79%

                - D: 60-69%

                - F: &amp;lt;60%
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a mixed class. Graduate and undergraduate students will be graded and
curved separately.&lt;/p&gt;

&lt;p&gt;There will be two exams given during the semester. In the event of an excused
absence (illness, job-related travel, holy day absence, etc.), students
&lt;u&gt;&lt;strong&gt;must&lt;/strong&gt;&lt;/u&gt; inform the instructor ahead of time and provide proper documentation of
the conflict. An accommodation will be attempted for verifiable problems.
Homework assignments will be collected graded and discussed in class (as time
permits). &lt;/p&gt;

&lt;p&gt;Homework will be collected at the beginning of the class period when
it is due. Homework that is not reasonably neat and readable, or not bound,
will be marked down. &lt;u &gt;&lt;strong&gt;Late Homework will not be accepted&lt;/strong&gt;&lt;/u&gt;. It may not be
returned to you until a week after it is due, which means you may not have it
back for a problem-solving session, or to use in studying for an exam. &lt;u &gt;&lt;strong&gt;If you
want to have it available at these times, you will have to make a photocopy
of it before you turn it in.&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name='maths'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;4.  Mathematical Modeling of Robots&lt;/h2&gt;

&lt;h3&gt;4.1.  DEFINITION OF A ROBOT&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;A robot is a reprogrammable, multifunctional manipulator designed to move material, parts, tools, or specialized devices through variable programmed motions for the performance of a variety of tasks.&lt;/em&gt;    &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        -- Spong, Hutchinson and Vidyasagar, Robot Modeling and Control (2006)    
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;

&lt;p&gt;Basically, a robot should be able to &lt;code&gt;sense&lt;/code&gt;, &lt;code&gt;move&lt;/code&gt; and &lt;code&gt;act intelligently&lt;/code&gt;. Put difeerently, a robot should have sensors on it to enable it to be environmentally-aware. It should be equipped with mechanical parts such as electric motors to make it able to navigate its environment. Lastly, it needs some &amp;#39;smarts&amp;#39; (otherwise referred to as &lt;code&gt;reprogrammability&lt;/code&gt; in engineering literature) to make it intelligent. The reporgrammability aspect of the definition is very important because it gives a robot &lt;code&gt;adaptibility&lt;/code&gt; and &lt;code&gt;usefulness&lt;/code&gt; making the robot &lt;code&gt;reconfigurable&lt;/code&gt;.   &lt;/p&gt;

&lt;h3&gt;4.2.  ROBOT MANIPULATORS&lt;/h3&gt;

&lt;p&gt;Robot manipulators are made up of &lt;u&gt; &lt;strong&gt;links&lt;/strong&gt;&lt;/u&gt;, \(i\), which are connected by &lt;u&gt;&lt;strong&gt;joints&lt;/strong&gt;&lt;/u&gt;, \(j_i\), to form a &lt;u&gt;&lt;strong&gt;kinematic chain&lt;/strong&gt;&lt;/u&gt;. Joints are typically rotary (henceforth referred to as &lt;em&gt;revolute&lt;/em&gt;) or linear (henceforth referred to as &lt;em&gt;prismatic&lt;/em&gt;) with a simple &lt;code&gt;end effector&lt;/code&gt; for manipulating work pieces. Applications vary from simple tasks such as pick and place operations to navigating complex environments (&lt;a href=&quot;https://www.youtube.com/watch?v=_luhn7TLfWU&quot;&gt;MIT Cheetah Robot&lt;/a&gt;) and medical robots such as the &lt;a href=&quot;http://www.davincisurgery.com/da-vinci-surgery/da-vinci-surgical-system/&quot;&gt;Davinci Surgical System&lt;/a&gt; among others.&lt;/p&gt;

&lt;p&gt;A &lt;u&gt;&lt;strong&gt;revolute joint&lt;/strong&gt;&lt;/u&gt; is akin to a hinge and allows relative rotation between two links. A &lt;u&gt;&lt;strong&gt;prismatic&lt;/strong&gt;&lt;/u&gt; joint allows a linear relative motion between the adjoint links. We will denote the revolute joints by \(R\) and the prismatic joints by \(P\).&lt;/p&gt;

&lt;p&gt;An example of a revolute/ joint is depicted in the figure below:&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/Lec1/Revolute.png&quot; width=&quot;49%&quot; height=&quot;250&quot; align=&quot;middle&quot;&gt;  
  &lt;img src=&quot;/assets/Lec1/Prismatic.png&quot; width=&quot;49%&quot; height=&quot;250&quot; style=&quot;border-left: 1px solid black;&quot;&gt;
  &lt;div class=&quot;figcaption&quot; align=&quot;right&quot;&gt;Fig. 1. An example of a revolute joint (left) and a prismatic joint (right). &lt;i&gt;Courtesy, &lt;a href=&quot;https://code.google.com/p/impsim/wiki/jmanual_jointRevolute&quot;&gt;  impsim. &lt;/a&gt;&lt;/i&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The angle \(\Phi\) in the left figure is defined as the joint angle and it connects links &lt;strong&gt;LK1&lt;/strong&gt; and &lt;strong&gt;LK2&lt;/strong&gt;. Similarly, the displacement \(\Phi\) in the right figure connects  links &lt;strong&gt;LK1&lt;/strong&gt; and &lt;strong&gt;LK2&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;&lt;u&gt;Rotations follow the right hand rule&lt;/u&gt;. Essentially, this means if we have three orthonormal vectors \(x\), \(y\), and \(z\) \(\in\) \(\mathcal{R}^3\) which define a coordinate frame, they must satisfy the mathematical relation, 
&lt;center&gt;\(z\) = \(x\) \(\times\) \(y\).&lt;/center&gt;
We will denote the axis of rotation of a revolute joint or the axis of displacement of a prismatic joint as \(z^i\) if the joint connects links \(i\) and \(i+1\). \(\Phi\) is referred to as the &lt;strong&gt;joint variable&lt;/strong&gt; for a revolute or prismatic joint as the case may be.&lt;/p&gt;

&lt;h3&gt;Things to remember&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;There are two types of joints: prismatic and revolute joints.&lt;/li&gt;
&lt;li&gt;\(\Phi\) is referred to the joint variable.&lt;/li&gt;
&lt;li&gt;A revolute joint will allow rotation between two links.&lt;/li&gt;
&lt;li&gt;A prismatic joint will allow displacement between two links.&lt;/li&gt;
&lt;li&gt;The axis of rotation for a typical joint, \(j_i\) that connects links \(i\) and \(i+1\) is \(z^i\).&lt;/li&gt;
&lt;li&gt;Angles are measured in a clockwise manner so that if an angle along a directed axis is positive if it represents a clockwise rotation about the direction from which we are viewing. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A three link arm with 2 revolute joints for example is referred to as an &lt;strong&gt;RRP&lt;/strong&gt; arm, where the R&amp;#39;s stand for &lt;em&gt;revolute&lt;/em&gt; and the P stands for &lt;em&gt;prismatic&lt;/em&gt;. An example of an RRP robotic arm is the SCARA robot which we shall deal with shortly.&lt;/p&gt;

&lt;h3&gt;4.3.  Taxonomy of Robot Manipulators&lt;/h3&gt;

&lt;h4&gt;4.3.1. Configuration&lt;/h4&gt;

&lt;p&gt;The configuration of a manipulator is a complete description of the location of every point on the robot manipulator.
When we know the set of all possible configurations, we say we know the &lt;strong&gt;configuration space&lt;/strong&gt; of the robot. The configuration space will correspond mathematically to knowing the set of all possible \(\theta_i\) for a revolute joint or \(d_i\) for a prismatic robot where \(\theta_i\) denotes the respective joint angles and \(d_i\) denotes the respective displacements. The set of angles \(\theta_i\) for a revolute joint is naturally associated with a unit circle in the plane denoted by \(\mathcal{S}^1\). It is typical for you to see revolute joints written as \(\theta_i\) \(\in\)  \(\mathcal{S}^1\) in literature. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt; For a single revolute joint arm, the configuration space is \(\mathcal{S}^1\), which geometrically is a one-dimensional sphere (or 1-D sphere).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 2:&lt;/strong&gt; A two-revolute joint arm will have \(\mathcal{S}^1\) \(\times\) \(\mathcal{S}^1\) configuration space. This is visually equivalent to moving an outer circle arrount an inner concentric circle. This is called a torus (donut-shaped).&lt;/p&gt;

&lt;p&gt;&lt;center&gt; &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/c/c6/Simple_Torus.svg&quot; alt=&quot;Torus&quot;&gt;&lt;/center&gt;
&lt;center&gt; Fig 2. An example of a Torus. &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 3:&lt;/strong&gt; For a one revolute and one prismatic joint, the configuration space is \(\mathcal{S}^1\) \(\times\) \(\mathcal{R}\) which is geometrically the equivalent of a cylinder.&lt;/p&gt;

&lt;h4&gt;4.3.2.  Degrees of freedom&lt;/h4&gt;

&lt;p&gt;The number of degrees of freedom of a robot is the minimum number of parameters required to specify the configuration of a robot. Generally, this is equivalent to the size of the configuration space. Typically, the number of joints for a robotic manipulator will tell us about how many degrees of freedom it has. The &lt;a href=&quot;http://www.robai.com/robots/robot/cyton-epsilon-300/&quot;&gt;cyton arm robot&lt;/a&gt; which we will use in most of the labs that accompany this class has seven joints and hence seven degrees of freedom. A rigid object in three-dimensional space will have six degrees of freedom(DOF) because it will have 3 dedicated to orientation and 3 dedicated to translation. When the DOF is lesser than 6, the robot is said to be &lt;u&gt;underactuated&lt;/u&gt;. An example of underactuated robots include quadrotors with four rotors. When a manipulator has more than 6 DOF, we say such a robot is &lt;u&gt;kinematically redundant&lt;/u&gt;.&lt;/p&gt;

&lt;h4&gt;4.3.3.  The State&lt;/h4&gt;

&lt;p&gt;The state includes the geometry of all inputs, all disturbances, velocities, forces and et cetera that determine the current and future response of the manipulator.&lt;/p&gt;

&lt;h4&gt;4.3.4.  The State Space&lt;/h4&gt;

&lt;p&gt;The state space is the set of all possible states of a manipulator. &lt;/p&gt;

&lt;h4&gt;4.3.5.  The Workspace&lt;/h4&gt;

&lt;p&gt;The volume of the motion traversed by the end-effector as the manipulator executes possible motions is called the workspace. Often separated into the &lt;b&gt;reachable workspace&lt;/b&gt; and &lt;b&gt;dexterous workspace&lt;/b&gt; depending on how many of the possible set of points an end-effector can reach based. The &lt;strong&gt;reachable workspace&lt;/strong&gt; is the set of all possible points the manipulator can reach while the &lt;strong&gt;dexterous workspace&lt;/strong&gt; is the set of points the manipulator can reach based on an arbitrary orientation of the end effector.&lt;/p&gt;

&lt;h2&gt;4.4.  Accuracy and Repeatability&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;accuracy&lt;/strong&gt; of a manipulator is a measure of how close the manipulator can reach a specific point \(x, y, z\) in the state space. There is no absolute correct way to measure the accuracy of a robot. One method is to use position encoders in joints at known locations. This would use the assumed geometry of the manipulator and its rigidity to infer the end-effector position from the measured joint positions. It becomes apparent therefore that accuracy is affected by computational errors, machining accuracy during the manipulator construction, gear backlash among other things.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Repeatability&lt;/strong&gt; is a measure of how close a manipulator can return to a previously taught point. The &lt;u&gt;resolution of the controller&lt;/u&gt; will affect a manipulator&amp;#39;s repeatability. The resolution is the smallest degree of increase in manipulator motion that a robot can sense and is the ratio of the total distance travelled and \(2^n\) where \(n\) is the number of bits of the encoder accuracy. Therefore a prismatic axis will have a greater resolution compared to a revolute joint (a straight line is shorter than an arc length).&lt;/p&gt;

&lt;p&gt;&lt;a name='common-robot-arm-configurations'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;5. Common Robot Arm Configurations&lt;/h2&gt;

&lt;p&gt;Following the prismatic and revolute joint taxonomy, there are many possiblities in the way a manipulator arm can be designed. This section discusses the common design attributes of the typical arrangements.&lt;/p&gt;

&lt;h3&gt;5.1.  Articulated Manipulator (RRR)&lt;/h3&gt;

&lt;p&gt;This is otherwise called the &lt;strong&gt;anthromorphic&lt;/strong&gt; manipulator named out of its anthropomorphic characteristics. It has three revolute joints with each axis designated as the &lt;strong&gt;waist&lt;/strong&gt; (\(z_0\)), &lt;strong&gt;shoulder&lt;/strong&gt; (\(z_1\)), and &lt;strong&gt;elbow&lt;/strong&gt; (\(z_2\)). More often than not, the joint axis \(z_2\) will be parallel to \(z_1\) while both \(z_2\) and \(z_1\) will be perpendicular to \(z_0\). The revolute manipulator has a considerably large degree of freedom of movement in a compact space.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;/assets/Lec1/RRR.png&quot; alt=&quot;Articulated Robot rm&quot;&gt;&lt;/center&gt;
&lt;center&gt;Fig 5.1. Symbolic representation of a six-DOF elbow manipulator with links and joints similar to those of a a human joint/limbs which explains why it is called an anthroppmorphic robot. Photo courtesy of [\(^2\)].&lt;/center&gt;&lt;/p&gt;

&lt;h3&gt;5.2.  The Spherical Manipulator (RRP)&lt;/h3&gt;

&lt;p&gt;If we replace the elbow or last joint in the &lt;strong&gt;RRR&lt;/strong&gt; manipulator with a prismatic joint, we would be left with what is called the &lt;strong&gt;spherical manipulator&lt;/strong&gt;. Spherical joints are capable of arbitrary rotations and the name derives from the fact that the &amp;quot;joint coordinates coincide with the spherical coordinates of the end effector relative to a coordinate frame located at the shoulder joint&amp;quot;\(1\). Passive spherical joints is consists of a &lt;em&gt;ball and socket&lt;/em&gt; joint. This does not work adequately if the joint is to exert forces and torques and hence actuated spherical joints are often constructed such that three revolute joints are combined with motors to the end of making their axes intersect at a point\(^3\).&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;/assets/Lec1/Spherical.png&quot; alt=&quot;Spherical Robot rm&quot;&gt;&lt;/center&gt;
&lt;center&gt;Fig 5.2. Symbolic representation of a spherical arm (left); The Stanford Arm (right) is an example of a spherical manipulator. Photo courtesy of [\(^2\)].&lt;/center&gt;&lt;/p&gt;

&lt;h3&gt;5.3.  The SCARA Manipulator (RRP)&lt;/h3&gt;

&lt;p&gt;Short for &lt;strong&gt;S&lt;/strong&gt;elective &lt;strong&gt;Co&lt;/strong&gt;mpliant &lt;strong&gt;A&lt;/strong&gt;rticulated &lt;strong&gt;R&lt;/strong&gt;obot for &lt;strong&gt;A&lt;/strong&gt;ssembly. Introduced in 1979 in Japan and the United States. Typically used for pick and place operations.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;/assets/Lec1/SCARA.png&quot; alt=&quot;The Adept One robot&quot;&gt;&lt;/center&gt;
&lt;center&gt;Fig.5.3.  Symbolic representation of the Adept One Robot. Photo Courtesy of [\(^3\)].&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;It is a bit different from the Spherical Manipulators in that its \(z_0\), \(z_1\) and \(z_2\) are all parallel to one another.&lt;/p&gt;

&lt;h3&gt;5.4.  The Cylindrical Manipulator (RPP)&lt;/h3&gt;

&lt;p&gt;The cylindrical manipulator has two independent degrees of freedom and is typically a combination of a revolute and two prismatic joints such that their axes &lt;strong&gt;intersect&lt;/strong&gt;. An example of the cylindrical joint is shown in fig 5.4.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;http://icosym-nt.cvut.cz/odl/partners/fuh/EXAMPLES/eqs5/stan_FM.gif&quot; width=&quot;60%&quot; height=&quot;500&quot; align=&quot;middle&quot;&gt; 
  &lt;div class=&quot;figcaption&quot; align=&quot;middle&quot;&gt;Fig.5.4.  Symbolic representation of the Cylindrical Robot. Photo Courtesy of [4]&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3&gt;5.5.  The Cartesian Manipulator (PPP)&lt;/h3&gt;

&lt;p&gt;We say a manipulator is cartesian of its first three joints are prismatic. The variables of the joints are the cartesian Cartesian coordinates with respect to the base. They find uses in sealant application to materials as in table-top assembly, as gantry robots e.g. for cargo transfer etc. Its axes are coincident to a Cartesuab coordinator&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://prime.jsc.nasa.gov/ROV/images/cartesian.GIF&quot; alt=&quot;An example cartesian robot manipulator&quot;&gt;&lt;/center&gt;
&lt;center&gt;Fig. 5.5. An example cartesian robot manipulator.&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name='summary'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this class, we have covered the mathematical basis of robotics and built a foundation upon which we shall build the next several modules. Please go through the material posted on elearning and chapter 1 of Dr. Spong&amp;#39;s book [\(^2\)] to get familiarized all the more with the topics we have discussed so far. &lt;/p&gt;

&lt;p&gt;We&amp;#39;ll see you in the next class.&lt;/p&gt;

&lt;p&gt;&lt;a name='add'&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;1.  &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics&quot;&gt;Asimov, Isaac (circa 1950) I, Robot.&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;2.  Robot Modeling and Control, Mark W. Spong, Seth Hutchinson and M. Vidyasagar. John Wiley &amp;amp; Sons Inc. 2006.&lt;/li&gt;
&lt;li&gt;3.  A Mathematical Introduction to Robotic Manipulation, Richard Murray, Zexiang Li and S. Shankar Sastry. CRC Press. 1994.&lt;/li&gt;
&lt;li&gt;4.  &lt;a href=&quot;http://icosym-nt.cvut.cz/odl/partners/fuh/EXAMPLES/eqs5/stan_FM.gif&quot;&gt;Dynast&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
&lt;a href=&quot;https://twitter.com/share&quot; class=&quot;twitter-share-button&quot; data-via=&quot;patmeansnoble&quot;&gt;Tweet&lt;/a&gt;
&lt;script&gt;!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');&lt;/script&gt;
--&gt;
</description>
        <pubDate>2015-08-24 00:00:00 -0500</pubDate>
        <link>lakehanne.github.iorobots-modeling</link>
        <guid isPermaLink="true">lakehanne.github.iorobots-modeling</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Safely upgrading your linux kernel.&lt;/center&gt;</title>
        <description>&lt;p&gt;It&amp;#39;s a bit difficult safely upgrading your linux kernel from an older kernel without glitches losing your unity desktop or lightdm environement or being able to correctly log into your newly installed kernel after all has been said and done. So I decided to write this blog post after previous efforts at unsuccessfully upgrading my kernel from a Linux 3.16 version. For simplicity, we&amp;#39;ll assume you are running a debian distro such as ubuntu or cent-os. If you have a different distro, you could download the kernel straight from &lt;a href=&quot;https://www.kernel.org/&quot;&gt;Linux Kernel&lt;/a&gt;, untar the downloaded file and follow the instructions in the README in order to compile with cmake. After compiling your kernel, go to &lt;a href=&quot;#setting-up-your-new-kernel&quot;&gt;setting up your new kernel&lt;/a&gt; to be able to log on to your new kernel when next you reboot.&lt;/p&gt;

&lt;p&gt;The maintainers over at Ubuntu have compiled the kernel on their &lt;a href=&quot;http://kernel.ubuntu.com/%7Ekernel-ppa/mainline/&quot;&gt;ppa&amp;#39;s&lt;/a&gt; and you could pick which ever kernel you wish to upgrade to. For me, I was upgrading from 3.16 to 4.04; so for the purpose of this tutorial, our instructions will be based on upgrading to kernel 4.0.4.&lt;/p&gt;

&lt;p&gt;First of all, make sure you have all your files properly backed up in a safe place before attempting an upgrade of your kernel.&lt;/p&gt;

&lt;h2&gt;Downloads&lt;/h2&gt;

&lt;p&gt;For 32-bit systems, let&amp;#39;s pull the following files from the ubuntu kernel repositories.&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-headers-4.0.4-040004_4.0.4-040004.201505171336_all.deb

$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-headers-4.0.4-040004-generic_4.0.4-040004.201505171336_i386.deb

$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-image-4.0.4-040004-generic_4.0.4-040004.201505171336_i386.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If, however, your system is a 64-Bit System, you would want to pull the following files instead:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-headers-4.0.4-040004_4.0.4-040004.201505171336_all.deb

$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-headers-4.0.4-040004-generic_4.0.4-040004.201505171336_amd64.deb

$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-image-4.0.4-040004-generic_4.0.4-040004.201505171336_amd64.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, install each of the files from your terminal window&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo dpkg -i linux-headers-4.0.4*.deb linux-image-4.0.4*.deb&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hang in there. Let&amp;#39;s set up &lt;strong&gt;GRUB&lt;/strong&gt; to point to the new kernel installation. This is necessary so that when you restart your system, your bootloader gives you the option to choose which kernel to load (Grub 2.02 does this, I have not tried it on other grub versions)&lt;/p&gt;

&lt;h2&gt;Setting up your new kernel&lt;/h2&gt;

&lt;p&gt;We&amp;#39;ll assume your boot-loader is &lt;strong&gt;GRUB&lt;/strong&gt;. The steps we carried out above will upgrade the linux installation which could also have been performed with the following (rather &lt;em&gt;unsafe&lt;/em&gt;) commands:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo aptitude update 
$ sudo aptitude safe-upgrade &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I called those unsafe because you do not have control of what happens during the upgrade process and it is well documented that such methods have been known to tamper with graphic drivers and what-a-view after upgrades. I generally prefer going the old-fashioned way as I describe in this post.&lt;/p&gt;

&lt;p&gt;Depending on your system configuration settings, after the steps above, you may find that your kernel and GRUB bootloader configuration have already been updated. Matter-of-factly, GRUB may automatically select the right boot menu option when you next restart your system. But before you go ahead and restart, you want to ensure everything is correctly set or you might find (as I did) that Mr. Lightdm/Gdm/Unity Desktop doesn’t show up after your reboot. &lt;/p&gt;

&lt;p&gt;Therefore, we will configure GRUB to  automatically check out our new kernel, and should it fail, we would like to be able to revert to the previous kernel. Since GRUB is configured by the file &lt;code&gt;/boot/grub/grub.cfg&lt;/code&gt;, it makes sense to edit this brobdinagian file. But rather than work with the bugging lines of code in &lt;code&gt;/boot/grub/grub.cfg&lt;/code&gt;, we will edit relevant lines in &lt;code&gt;/etc/default/grub&lt;/code&gt;, then update grub to to have &lt;code&gt;/boot/grub/grub.cfg&lt;/code&gt; see our latest grub edit.&lt;/p&gt;

&lt;p&gt;Fire up &lt;code&gt;/etc/default/grub&lt;/code&gt; in your favorite editor and set the following configurations:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
GRUB_DEFAULT=saved
GRUB_TIMEOUT=0
GRUB_CMDLINE_LINUX_DEFAULT=”quiet splash panic=10″
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Basically, we are telling GRUB to use the last-saved selection, do an auto boot-up after 5 seconds if user does nothing, and inform all kernels to reboot after 10 seconds if they die completely. Note, the TIMEOUT option is disabled in GRUB 2.0.2; so if your grub version is greater than version 2.0, you can discount line 2 above. &lt;/p&gt;

&lt;p&gt;We need to set the kernel that is initially &lt;em&gt;saved&lt;/em&gt;. We will set this to the known working configuration within the terminal as follows:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo grub-set-default &quot;Ubuntu&gt;Ubuntu-with-Linux-3.16.0-30-generic&quot; &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could check the kernel you’re currently running with &lt;code&gt;uname -r&lt;/code&gt; and copy the label used in place of what I have above.  Next, we will make GRUB try the new kernel on the next reboot, such as:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo grub-reboot ”Ubuntu&gt;Ubuntu-with-Linux-4.0.4-04-new” &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will save our configurations to the boot folder using:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo update-grub &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should have an output that says something like this:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
    Generating grub configuration file ...
    Found linux image: /boot/vmlinuz-4.0.4-040004-generic
    Found initrd image: /boot/initrd.img-4.0.4-040004-generic
    Found linux image: /boot/vmlinuz-3.16.0-30-generic
    Found initrd image: /boot/initrd.img-3.16.0-30-generic
    Found memtest86+ image: /memtest86+.elf
    Found memtest86+ image: /memtest86+.bin
    done
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Try rebooting. &lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo reboot &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should be fine afterwards.&lt;/p&gt;

&lt;p&gt;If everything goes honky-dory, set the new kernel as the saved default for future boot-up&amp;#39;s:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo grub-set-default &quot;Ubuntu&gt;Ubuntu-with-Linux-4.0.4-040004-generic&quot; &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If at anytime, you choose to uninstall, just go ahead and do the following in your terminal:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo apt-get remove 'linux-headers-4.0.4*' 'linux-image-4.0.4*'&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations!&lt;/p&gt;
</description>
        <pubDate>2015-07-22 18:39:00 -0500</pubDate>
        <link>lakehanne.github.ioUpgrading-Linux-Kernel</link>
        <guid isPermaLink="true">lakehanne.github.ioUpgrading-Linux-Kernel</guid>
        
        
        <category>lakehanne.github.io</category>
        
      </item>
    
  </channel>
</rss>
