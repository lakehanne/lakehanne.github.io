<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Understanding Generative Adversarial Networks</title>
    <meta name="viewport" content="width=device-width,  initial-scale=1, maximum-scale=1">
    <meta name="description" content="To discover and understand.
">
    <link rel="canonical" href="gans">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Lekan Ogunmolu posts" />
    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <link href="/vendor/css/bootstrap.min.css" rel="stylesheet">
    <link href="/vendor/css/font-awesome.min.css" rel="stylesheet">
    <link href="/vendor/css/academicons.min.css" rel="stylesheet">
    <link href="/vendor/pygments/default.css" rel="stylesheet">

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-3698471-23', 'auto');
      ga('send', 'pageview');
    </script>

    <div class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="row">
          <div class="col-md-10 col-md-offset-1">
            <div class="navbar-header">
                <a href="/" class="navbar-brand">
                    <div>
                        <img src="/downloads/me-style.jpg" class="img-circle" alt="Me" style="width:20px;height:20px;" ></img>
                        Lekan Ogunmolu &nbsp; &nbsp; &nbsp;
                    </div>
                </a>
              <button class="navbar-toggle" type="button" data-toggle="collapse"
                      data-target="#navbar-main">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="navbar-collapse collapse" id="navbar-main">
              <ul class="nav navbar-nav">
                <li>
                  <a href="/blog">Blog &nbsp; &nbsp; &nbsp;</a>
                </li>
                <li>
                  <a href="/quotes">Fave. Quotes &nbsp; &nbsp; &nbsp;</a>
                </li>
                <li> 
                  <a href="/resume">CV &nbsp; &nbsp; &nbsp;</a>
                </li> 
                <li>
                  <a href="/about">About&nbsp; &nbsp; &nbsp;</a>
                </li>

                <li>
                  <div style="float:right; margin-top:20px; margin-right:40px;">
                  <a href="/feed.xml">
                    <img src="/downloads/rssicon.svg" width="20">
                  </a>
                  </div>
                  </a>
                </li>
              </ul>
              <nav class="site-nav">
                <a href="#" class="menu-icon">
                  <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
                    <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
                      h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
                    <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
                      h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
                    <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
                      c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
                  </svg>
                </a>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>
</head>

    <br>
    <body>

    <!-- Included file 'header.html' not found in _includes directory -->

    <div class="page-content">
      <div class="wrap">

      <div class="post">

    

  <header class="post-header">
    <h1>Understanding Generative Adversarial Networks</h1>
    <p class="meta">Jun 11, 2017</p>
  </header>

  <article class="post-content">
  
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!--Mathjax Parser -->
<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<h4 id="introduction">INTRODUCTION</h4>

<p>Deep learning strives to discover a probability distribution on the underlying nonlinear system. This probability distribution is usually discovered using composite mappings of neurons stacked into layers (with many such layers composing a deep network model), providing an abstraction of underlying low-level features for ease of inference at a much higher-level.</p>

<p>The most successful deep learning models have been discriminative, supervised ones that map a high-dimensional, sensory-rich input to an output using the backpropagation and/or dropout algorithms.</p>

<p>In order to learn in an unsupervised approach, one has to find a model that is capable of approximating the underlying probabilistic distribution. This is, by no means, difficult due to the intractability of probabilistic computations that arise in
maximum likelihood estimates, importance sampling and Gibb’s distributions. In such deep generative contexts, it is difficult to leverage on the scalability of piecewise linear models used in discriminative algorithms in an unsupervised context.</p>

<h4 id="deciphering-generative-adversarial-networks">Deciphering Generative Adversarial Networks</h4>

<p>Since such deep generative models are difficult to train, the question to ask then is, “is it possible to train a generative model from a classical discriminative classification setting?” If we have labeled data available, one can train a differentiable function that maps from an input space to the output space of a Borel measurable set in a supervised setting; this differentiable function can be thought of as fitting a probability distribution on a generative model.</p>

<p>What happens if we pit the  generative model against a counterfeiter (an adversary) whose goal is to score the <em>likelihood</em> (not exactly, but we’ll see the definition shortly) that a sample drawn from the probability distribution of the generative model is real or fake?
You see, things get interesting here. If the adversary samples from the latent space of the generator, then it improves its likelihood of passing off as a real model. This is the central idea behind <strong>generative adversarial networks</strong>.
Interesting, huh?</p>

<h5 id="prior-work">Prior Work</h5>

<p>Deep Deterministic Networks are directed graphical models. To generate stochastic distributions that represent an underlying model, people have used undirected graphical models with hidden variables such as restricted Boltzmann machines (<a href="http://dl.acm.org/citation.cfm?id=104290">RBMs</a>, <a href="https://www.ncbi.nlm.nih.gov/pubmed/16764513">RBMs</a>) and  deep Boltzmann machines (<a href="#dbms">DBMs</a>). Unless the underlying <em>partition function</em> within such models are trivial, the product of normalized potential functions and the gradients used are intractable, making such methods limiting.</p>

<p><!-- With Monte Carlo Markov Chain methods, however, one can approximate such functions. --></p>

<p>With deep belief networks, the gradient of a log likelihood expectation is computed with two different techniques:  the expectations of <em>data-dependent</em> modes are <em>estimated</em> with variational approximations while the expectations of <em>data-independent</em> modes are approximated by <em>persistent Markov chains</em>.</p>

<p>Noise contrastive estimates (<a href="http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">NCEs</a>) and generative stochastic networks (<a href="https://arxiv.org/pdf/1306.1091.pdf">GSNs</a>) are the closer of the generative adversarial network ancestors. NCEs do not approximate or propose a bound on the log likelihood estimate but require the learned probability distribution to be analytically specified up to a normalization constant. But with deep models containing many latent variables, providing a normalized constant for the probability distribution is almost an impossible task.</p>

<p>GSNs belong to those class of algorithms that instead of bounding the log likelihood estimate, they train a generative model with backpropagation to draw samples from a desired distribution in a parameterized Markov chain manner. Adversarial nets, on the contrast, do not require parameterized Markov chains for sampling from a data distribution but instead use piecewise linear units (ReLUs, MaxOuts etc) to improve the performance of backpropagation.</p>

<h4 id="inside-gans">Inside GANs</h4>

<p>The original <a href="https://arxiv.org/abs/1406.2661">adversarial net</a> was designed using two multilayer perceptrons as the modeling framework. It’s defined as follows:</p>

<p><strong>Generator:</strong></p>

<ul>
  <li>suppose we have a data set, \(\textbf{x} \sim p_g(\centerdot)\)
    <ul>
      <li>where \(p_g\) represents the parameterization of the probability distribution of data \(\textbf{x}\);</li>
    </ul>
  </li>
  <li>
    <p>suppose also that we have a noisy dataset \(\textbf{z}\) with a prior defined as \(p_z(\textbf{z})\)</p>
  </li>
  <li>let \(G(\textbf{z};\theta_g)\) be a differentiable mapping from \(\textbf{z}\)’s state space to \(\textbf{x}\)’s state space parameterized by \(\theta_g\)
    <ul>
      <li>i.e. \(G_{\theta_g}: \mathbb{R}^{n_z} \rightarrow \mathbb{R}^{n_x}\)</li>
    </ul>
  </li>
  <li><strong>define \(G(\textbf{z};\theta_g)\) as the generator</strong></li>
</ul>

<p><strong>Discriminator:</strong>
- let us define \(D(\textbf{x})\) as the probability that the data \(\textbf{x}\) is drawn from the data state space and not its parameterized probability distribution \(p_g(\centerdot)\)</p>

<ul>
  <li>
    <p>let \(D(\textbf{x}; \theta_d): \mathbb{R}^{n_x} \rightarrow  \mathbb{R}^1\) represent the differentiable function that parameterizes the probability \(D(\textbf{x})\)</p>
  </li>
  <li>
    <p><strong>define \(D(\textbf{x}; \theta_d)\) as the discriminator</strong></p>
  </li>
</ul>

<p><strong>GANs Algorithm</strong></p>

<p>GANs <strong><em>simulataneously</em></strong> train two <u>different</u> multilayer perceptrons in a minimax scenario:</p>

<ul>
  <li>one maximizes the probability of assigning the correct label to both training examples and samples drawn from \(G(z; \theta_g)\),
    <ul>
      <li>i.e. train to  \(\max_D \text{ log } D(x)\);</li>
      <li>note that this is done with supervised learning</li>
    </ul>
  </li>
  <li>the second perceptron trains \(G(\textbf{z};\theta_g)\)  to minimize the \(\text{log }[1 - D(G(\textbf{z}))]\)</li>
</ul>

<p>Essentially, we are playing a two-player minimax game with value function \(V(G(\centerdot), D(\centerdot))\):</p>

<p>\begin{equation}
  \min_G \max_D V(D,G) = \mathbb{E}_{x \sim p(x)} \left[\text{ log } D(\textbf{x}) \right] + \mathbb{E}_{\textbf{z} \sim p(\textbf{z})} \left[\text{ log } \left(1 - D(G(\textbf{z})\right)\right]
\end{equation}</p>

<p>One may wonder why we are taking the logarithms of the differentiable functions: my gut is the authors used the logarithms to hasten the optimization process since differentiating the logarithm of a high cost takes less time than the bare-bones cost function itself \(i.e. (\textbf{O}(\text{ log } (n)) &lt; &lt;  \textbf{ O } (n))\). If the functions are matrix variables, we could take their log determinants.</p>

<div class="fig figcenter fighighlight">
  <img src="assets/Gans/fig1.jpg" width="20%" height="60%" border="0" />
  <img src="assets/Gans/fig2.jpg" width="20%" height="60%" border="0" />  
  <img src="assets/Gans/fig3.jpg" width="20%" height="60%" border="0" />  
  <img src="assets/Gans/fig4.jpg" width="20%" height="60%" border="0" />  
  <div class="figcaption" align="left">
    Fig.1: Intuition behind Generative Adversarial Networks (reproduced from the original <a href="https://arxiv.org/abs/1406.2661">paper).</a>.
  </div>
</div>

<p>Essentially, GANs involve simultaneously:</p>

<ul>
  <li>
    <p>generating samples from a data with probability distribution, \(p_{\text{data}}(x)\) (black, dotted system in Fig 1)</p>
  </li>
  <li>generating samples from a generative distribution, \(p_g(G)\) (green, solid system in Fig 1),
    <ul>
      <li>\(z\) is typically sampled uniformly from a uniform Gaussian noise distribution (lower horizontal line in Fig 1).</li>
    </ul>
  </li>
  <li>updating a discriminant, \(D\) (blue, dashed system in Fig 1),
    <ul>
      <li>this distinguishes between samples from \(p_{\text{data}}(x)\) from samples from \(p_g(G)\). \(\textbf{z}\)</li>
    </ul>
  </li>
  <li>the horizontal line is part of the domain of \(\textbf{x}\) such that
    <ul>
      <li>\(\textbf{x} =G(\textbf{z})\) in the upward arrows imposes the non-uniform distribution \(p_g\) on transformed samples.</li>
    </ul>
  </li>
  <li>
    <p>when the underlying probability distribution is very dense, \(G(\centerdot)\) contracts and it expands in regions of low density in $p_g$.</p>
  </li>
  <li>
    <p>near optimum, \(p_g\) is similar to $p_\text{data}$ and $D$ is a partially accurate classifier (fig1a).</p>
  </li>
  <li>
    <p>in the inner loop of the algorithm, at convergence, \(D\) discriminates samples from data, yielding \(D^\star(\textbf{x}) = \frac{p<em>\text{data}(\textbf{x})}{ p</em>\text{data}(\textbf{x}) + p_g(\textbf{x})}\) (fig1b) .</p>
  </li>
  <li>
    <p>updating \(G\), the gradients of \(D\) guides \(G(\textbf{z})\) to flow to regions that are more likely to be classified as data (fig1c) .</p>
  </li>
  <li>after multiple steps of backprop, provided that $G$ and $D$ have enough capacity, they will reach a <em>saddle point</em> where neither can improve, since $p_g = p_\text{data}$ (fig1d) .</li>
</ul>

<p>At equilibrium, discriminator is unable to differentiate between the two
distributions, i.e. \(D(\textbf{x}) = \frac{1}{2}\).</p>

<p>GANs  allow us to train a discriminator as an unsupervised “density estimator” which outputs a low value for an original data and a high value for fictitious data. By pitting the adversary against the discriminator, the generator parameterizes a nonlinear manifold of a dynamical system, maps it to a point on the data manifold, and the discriminator develops internal dynamics that is capable of solving a difficult unsupervised learning problem.</p>

<h4 id="training-gans">Training GANS</h4>

<p>The algorithm is as detailed below:</p>

<p><code>python
  for i in N do
      for steps in k do
        obtain m noise samples {z(1), ..., z(m)} from generator noise prior p(z)
        obtain m minibatch examples {x(1), ..., x(m)} from data generating distribution p(x)
        update discriminator by ascending its stochastic gradient:
</code></p>

<p>\begin{equation}
\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m} [\text{log } D(x(i)) + \text{ log } (1 - D(G(z{i})))].  \nonumber
\end{equation}</p>

<p><code>python
      end for
      obtain m noise samples {z(1), ..., z(m)} from generator noise prior p(z)
      update generator by descending its stochastic gradient:
</code></p>

<p>\begin{equation}
\nabla_{\theta_g}\frac{1}{m}\sum_{i=1}^{m} \text{ log } (1 - D(G(z{i}))).  \nonumber
\end{equation}</p>

<p><code>python
  end for
</code></p>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  <!-- disqus comments -->
 
 <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'patmeansnoble'; // required: replace example with your forum shortname
      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  

</div>


     <!--<nav class="nav-primary" role="navigation" >
    <ul>
        
        <li>
        	 <a class= "post-link" href=""></a>
        </li>
        
        <li>
        	 <a class= "post-link" href=""></a>
        </li>
        
        <li>
        	 <a class= "post-link" href=""></a>
        </li>
        
        <li>
        	 <a class= "post-link" href=""></a>
        </li>
        
        <li>
        	 <a class= "post-link" href=""></a>
        </li>
        
        <li>
        	 <a class= "post-link" href=""></a>
        </li>
        
        <li>
        	 <a class= "post-link" href=""></a>
        </li>
        
        <li>
        	 <a class= "post-link" href=""></a>
        </li>
        
        <li>
        	 <a class= "post-link" href=""></a>
        </li>
        
    </ul>
</nav>
-->
      </div>
    </div>

    <head>

 <!--Font Awesome -->
 <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

  <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">Lekan Ogunmolu</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>
          <a class="github-button" href="https://github.com/lakehanne" data-style="mega" data-count-href="/lakehanne/followers" aria-label="Follow @lakehanne on GitHub"></a>
        </li>
      </ul>

        <ul><li>
          <a href="https://twitter.com/patmeansnoble" target="_blank" ><img src = "/downloads/TwitterLogo_blue.png" width="16%", height="17%"></a>
        </li></ul>
    </div>

    <div class="footer-col-2 column">

      <ul>
        
        <li>
          <a href="https://www.quora.com/Olalekan-Ogunmolu" target="_blank"><img src="/downloads/quora.svg" width="12%", height="5%"></a>

        </li>
        
      </ul>


      <ul>
        
        <li>
          <!-- <a href="mailto:patlekno@icloud.com"><i class="fa fa-fw fa-envelope-square"> Email</i></a> -->
          <a href="mailto:patlekno@icloud.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-o"></i></a>

        </li>
      </ul>
      

    </div>

    <div class="footer-col-3 column">
      <p class="text"><b>To discover and understand.
</b></p>
    </div>

  </div>
<script async defer id="github-bjs" src="https://buttons.github.io/buttons.js"></script>
</footer>

</head>


    </body>
</html>
