<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Olalekan Ogunmolu</title>
    <description>To Discover and Understand.</description>
    <link>lakehanne.github.io/</link>
    <atom:link href="lakehanne.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>2017-04-05 11:57:58 -0500</pubDate>
    <lastBuildDate>2017-04-05 11:57:58 -0500</lastBuildDate>
    <generator>Jekyll v</generator>
    
      <item>
        <title>&lt;center&gt;Backpropagation and convex programming&lt;/center&gt;</title>
        <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;!--Mathjax Parser --&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;h3 id=&quot;table-of-contentstable-of-contents&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;Table of Contents&lt;/a&gt;:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#feedback-linearization&quot;&gt;Feedback Linearization&lt;/a&gt;    &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;introduction&quot;&gt;&lt;/a&gt;
## Introduction&lt;/p&gt;

&lt;p&gt;The backpropagation algorithm is very useful for general optimization tasks. Particularly, in deep learning applications, great progress has been made due to the effectiveness of the backprop algorithm. Whereas in traditional control applications, we typically use feedback regulation to stabilize the states of the system, in model reference adaptive control systems, we typically want to specify an index of performance to determine the “goodness” of our adaptation. An auxiliary dynamic system called the &lt;strong&gt;reference model&lt;/strong&gt; is used in generating this index of performance (IP). The reference model specifies in terms of the input and states of the model a given index of performance. A comparison check is used to determine appropriate  control laws  by comparing the given IP and measured IP based on the outputs of the adjustable system to that of the reference model system. &lt;/p&gt;

&lt;p&gt;Note that the comparator block in an MRAS system does use the difference from the comparator block to either modify the parameters of the adjustable system or to generate auxiliary input signals which modify the difference between the two IP’s expressed as a functional of the difference between the IPs of the reference model and the adjustable system.&lt;/p&gt;

&lt;h2 id=&quot;nonlinear-multivariable-model-reference-adaptive-systems&quot;&gt;Nonlinear (Multivariable) Model Reference Adaptive Systems&lt;/h2&gt;

&lt;p&gt;With nonlinear (possibly multivariable systems), it is typical to approximate the unknown function \(f(.)\) with a function approximator such as simple neural networks. To date, the state-of-the-art method in optimizingh the weights of a neural networkis the &lt;a href=&quot;https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/&quot;&gt;backpropagation algorithm&lt;/a&gt;.  But the optimizationin classical backprop is unrolled end-to-end so that the complexity of the network increases supposingthat we want to add as &lt;i&gt;argmin layer&lt;/i&gt;. When we want the backpropagation algorithm to generate control laws that fit into our actuator constraints such as model predictive control schemes allow, we cannot easily fit a convex optimization layer into the backprop algorithm using classical gradient descent. This is because in the backpropagation algorithm, the explicit Jacobians of the gradients of the system’s energy function with respect to system parameters is not exactly formulated. But in control applications, we would want to define a quadratic programming layer as the last layer of our neural network optimization algorithm so that effective control laws that exactly fit into actuator saturation limits are generated. Doing this requires a bit of tweaking of the backprop algorithm on our part. &lt;/p&gt;

&lt;h3 id=&quot;problem-formulation-solving-the-standard-form-qp-in-a-backprop-setting&quot;&gt;Problem formulation: Solving the standard-form QP in a Backprop setting&lt;/h3&gt;

&lt;p&gt;We define the standard QP canonical form problem thus (without equality constraints):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\text{minimize}  \frac{1}{2}x^TQx + q^Tx \\
\text{ subject to }  \quad G x \le h
&lt;/script&gt;

&lt;p&gt;where \(Q\) is symmetric, positive definite matrix \(\in \mathbb{R}^n, q \un \mathbb{R}^n, G \in \mathbb{R}^{p \times n}, h \in \mathbb{R}^p, A \in \)
Suppose we have our convex quadratic optimization problem in canonical form, we can use primal-dual interior point methods (PDIPM) to find an optimal solution to such a problem (PDIPMs are the state-of-the-art in solving such problems currently, for example, see &lt;a href=&quot;https://stanford.edu/~boyd/papers/pdf/code_gen_impl.pdf&quot;&gt;Boyd and Mattingley&lt;/a&gt;). primal-dual methods with Mehrota predictor-corrector are effective and consistent for reliably solving QP embedded optimization problems within 5-25iterations, without warm-start.&lt;/p&gt;

</description>
        <pubDate>2017-04-05 06:15:00 -0500</pubDate>
        <link>lakehanne.github.ioAdaptive-Control-Notes</link>
        <guid isPermaLink="true">lakehanne.github.ioAdaptive-Control-Notes</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Neural Networks and Adaptive Control&lt;/center&gt;</title>
        <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;!--Mathjax Parser --&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;h3 id=&quot;table-of-contentstable-of-contents&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;Table of Contents&lt;/a&gt;:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#preamble&quot;&gt;Preamble&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#feedback-linearization&quot;&gt;Feedback Linearization&lt;/a&gt;    &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;preamble&quot;&gt;&lt;/a&gt;
## Preamble&lt;/p&gt;

&lt;p&gt;Over the past five-some months, my research has come full circle at the intersection of neural networks and nonlinear adaptive systems. The notes below are jottings I made during my learning process. I hope it helps folks just immersing themselves into nonlinear adaptive control using neural networks and function approximators. These notes were sporadically written and may contain some minor typos and shenanigans. I implore the humble inquirer after knowledge to bite the meat, and not my silly typographical errors. Please leave comments if you find errors.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;introduction&quot;&gt;&lt;/a&gt;
## Introduction&lt;/p&gt;

&lt;p&gt;The motivation for adaptive control was from systems whose parameters varied with time. Adaptive control can deal with any size of parametric uncertainty as well as dynamic uncertainties arising from neglected dynamics if correct robust algorithms are used. The bounds for the allowable dynamic uncertainites cannot be calculated as easily as in the nonadaptive case because of the nonlinear nature of the adaptive system coupled with the fact that the plant parameters are deemed unknown. &lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Adaptive Control designed for LTI plants give rise to a closed-loop system that is nonlinear. &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As a result poles, zeros, gain and phase margins make little sense for system analysis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nonlinear techniques based on Lyapunov analysis and passivity arguments plus linear systems theory can be used in establishing the stability/robustness and margins that are not so easy to compute as in the LTI case.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LTI methods can be used in understanding the dynamics of robust modification laws to adaptive systems e.g. dynamic normalizing signal that limits the rate of adaptation to be finite and small relative to level of dynamic uncertainty.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The limitation of estimated controller parameters to assume large values eliminates the possibility of high gain control&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;High gain or high speed control can increase instability due to high bandwidth that the controller is subjected to.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So you see, adaptive control are really complex to tune and get right. A bit of a background below.&lt;/p&gt;

&lt;p&gt;Adaptive control research was motivated in the 50s by the problem of designing autopilots whose parameters changed over a wide operating range in speeds and altitudes. Fixed-gain controllers could not solve the frequent parameter variations in such systems. Therefore, people developed gain scheduling techniques using auxiliary measurements of airspeed. With gain scheduling came rudimentary methods of adjusting the adaptation mechanism in model reference systems – the idea was to develop a self-tuning controller that adapted for parameter variations in a closed-loop reference model scheme. Adjustment mechnisms developed included sensitivity rules such as M.I.T. rule, which performed reasonably well under some conditions. Rudolf Kalman in 1958 rigorously analyzed the self-tuning controller and established the explicit identification of the controller parameters of a linear SISO plant so that these could be used to tune an optimal linear quadratic controller. &lt;/p&gt;

&lt;p&gt;In the 60’s, Parks [1966], demonstrated use of Lyapunov analysis in establishing the stability and convergence of adaptive systems. Advances in system identification enhanced the way update laws were determined for model reference schemes. Stochastic control and dynamic programming (from the work of Bellman) coupled with Lyapunov stability placed a firm footing on proving convergence for adaptive control systems. The 70s era witnessed a resurgence in the complete proofs of stability for model reference adaptive schemes e.g. Liapunov state space proofs from Narendra, Lin &amp;amp; Valavani and Morse. In the discrete time deterministic and stochastic domains, stability proofs also appeared about this time. Then came Rohr’s example whereby the assumptions of stability were found to be very sensitive to the presence of unmodeled dynamics (e.g. ignored high-frequency parasitic modes in order not to complicate controller design). Researchers started working on the robustness of adaptive schemes and their sensitivity to transient behaviors. &lt;/p&gt;

&lt;p&gt;The extension of adaptive control to linear time-varying parameters was a major obstacle until the 80s when basic robustness questions were answered. Tactics such as dead-zone modification, dynamic normalizing signal together with leakage or parameter projection were used to deal with a great deal of parameter variations. This class included slowly-varying parameters as well as infrequent jumps in parameter values. In several cases, the error from time-varying signals can be reduced through proper parameterizations of the time-varying plant model used in the control design. In the linear time-varying case, stability margins, bandwidth margins, bandwidth, frequency domain characteristics, poles, zeros do not make much sense even for time-varying parameters unless approximations are made using the assumption of slowly varying parameters, etc.&lt;/p&gt;

&lt;p&gt;Adaptive control applied to nonlinear systems is in its infancy. Why is this so?
In nonlinear systems, it is not only the parameters that are nonlinear, but also the functions. Adaptive control was designed to stabilize system parameters by adapting for nonlinear parameters  and &lt;strong&gt;NOT&lt;/strong&gt; nonlinear functions. The extension of adaptive controllers to nonlinear systems from LTI and LTV systems is therefore a complicated one. There are two general cases of adopting adaptive control to nonlinear systems: (i) nonlinear systems whose nonlinear functions are known but unknown parameters appear linearly. (ii) the nonlinear functions are assumed known by multiplying nonlinear basis functions with unknown parameters to be determined. The second option falls under categories where the basis functions are typically deduced from neural networks parameters (or weights as they are called) and they are assumed to appear linearly e.g. in a single layer &lt;strong&gt;(so far, this is what has been analytically proven)&lt;/strong&gt;. This property is fundamental to developing analytical stability results with large regions of attraction.&lt;/p&gt;

&lt;p&gt;Some control background below.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;feedback-linearization&quot;&gt;&lt;/a&gt;
### Feedback Linearization&lt;/p&gt;

&lt;p&gt;Feedback linearization consists in changing the coordinates of a system so as to cancel all or most of the unknown nonlinear terms so that the system behaves as a linear or partly linear system. Consider the following system&lt;/p&gt;

&lt;p&gt;\begin{align}
  \dot{x} = f(x) + g(x)u,  &lt;br /&gt;
  y = h(x)
  \label{eq:nlnr1}
\end{align}&lt;/p&gt;

&lt;p&gt;where \(x \in \mathbb{R}^n, u,y \in \mathbb{R} \text{ and } f,g,h, \) are smooth nonlinear functions.&lt;/p&gt;

&lt;p&gt;Differentiating \(y\) in \eqref{eq:nlnr1} with respect to time, we find that&lt;/p&gt;

&lt;p&gt;\begin{align}
\dot{y} = \dfrac{\partial{h}}{\partial{x}}(x) f(x) +     \dfrac{\partial{h}}{\partial{x}}(x) g(x) u,
\label{eq:ydot}
\end{align}&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;\begin{align}
\dfrac{\partial{h}}{\partial{x}}(x) f = \dfrac{\partial{h}}{\partial{x}_1}(x) f_1 + \cdots + \dfrac{\partial{h}}{\partial{x}_n}(x) f_n \triangleq L_f \, f
\label{eq:Lie}
\end{align}&lt;/p&gt;

&lt;p&gt;where \(L_f f\) is the &lt;i&gt;Lie derivative&lt;/i&gt;. If \(\frac{\partial{h}}{\partial{x}}(x_0) \, g(x_0) \neq 0\) at some point \(x_0\), then the system of \eqref{eq:nlnr1} is said to be of relative degree 1 at \(x_0\). For an LTI system, this means the output is different from the input by one integrator only. This would be a strictly proper tranfer function. A good way of thinking about this is that the output has to be differentiated by the number of the relative degree(s) until the input appears in the output expression. &lt;/p&gt;

&lt;p&gt;If \(\frac{\partial{h}}{\partial{x}}(x) \, g(x) = 0 \forall x \in {B_x}_0 \text{ of } x_0 \), then one can take the second derivative of \(y\) to obtain (abusing notation and dropping the terms in parentheses), &lt;/p&gt;

&lt;p&gt;\begin{align}
\ddot{y} = \dfrac{\partial}{\partial{x}} \left(\dfrac{\partial{h}}{\partial{x}}f\right)f + \dfrac{\partial}{\partial{x}}\left(\dfrac{\partial{h}}{\partial{x}}f\right) g \, u.
\end{align}&lt;/p&gt;

&lt;p&gt;If \(\frac{\partial}{\partial{x}} \left(\frac{\partial{h}}{\partial{x}}(x)f(x)\right) g(x) \, |\ _{x = x_0} \neq 0 \) then \eqref{eq:nlnr1} is said to have a relative degree 2 at \(x_0\). 
We can continue the differentiation in a neighborhood of \(x_0\) if \(\frac{\partial}{\partial{x}} \left(\frac{\partial{h}}{\partial{x}}(x)f(x)\right)g(x) = 0 \) in the neighborhood of \(x_0\). &lt;/p&gt;

&lt;p&gt;More generally, &lt;/p&gt;

&lt;p&gt;\begin{align}
L_f^i h = \dfrac{\partial}{\partial{x}}\left(\dfrac{\partial}{\partial{x}} \left(\cdot \dfrac{\partial}{\partial{x}} \left(\dfrac{\partial{h}}{\partial{x}}f\right) \cdot f \cdots \right) \cdot f \right) \cdot f.
\end{align}&lt;/p&gt;

&lt;p&gt;Defining&lt;/p&gt;

&lt;p&gt;\begin{align}
L_g L_fh \triangleq \frac{\partial(L_fh)}{\partial x}\cdot g,
\end{align}&lt;/p&gt;

&lt;p&gt;then the SISO nonlinear system \eqref{eq:nlnr1} has a relative degree \(\rho = n\) at a point \(x_0\) if&lt;/p&gt;

&lt;p&gt;(i)  \(L_g L_f^i h(x) = 0 \forall x \in {B_x}_0, \) where \({B_x}_0\) is some neighborhood of \(x_0\) \(\forall i  = 1, 2, 3, \ldots, \rho - 2. \)&lt;/p&gt;

&lt;p&gt;(ii) \(L_g L_f^{\rho -1} h(x_0) \neq 0. \)&lt;/p&gt;

&lt;p&gt;if \eqref{eq:nlnr1} has relative degree \(\rho = n\) at \(x\), where \(n\) is the order of \eqref{eq:nlnr1}, then given the transfoirmation &lt;/p&gt;

&lt;p&gt;\begin{align}
z_1 = y = h(x), \qquad z_2 = \dot{y} = L_f h(x), \qquad z_3 = \ddot{y} = L_f^2 h(x), 
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
z_i = y^{(i-1)} = L_f^{(i-1)} h(x) , \ldots, z_n = y^{(n-1)} = L_f^{n-1} h(x).
\end{align}&lt;/p&gt;

&lt;p&gt;we find that &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\dot{z}_1 = z_2, \nonumber \\
\dot{z}_1 = z_2, \nonumber \\
\quad \vdots  \nonumber \\
\dot{z}_{n-1} = z_n,   \nonumber \\
\dot{z}_n = L_f^nh(x) + (L_gL_f^{n-1}h(x))u,  \\
y = z_1
\label{eq:canon}
&lt;/script&gt;

&lt;p&gt;which is the &lt;strong&gt;&lt;i&gt;canonical form&lt;/i&gt;&lt;/strong&gt; of the system with &lt;em&gt;&lt;strong&gt;no zero dynamics&lt;/strong&gt;&lt;/em&gt;.  From feedback linearization, we find that &lt;/p&gt;

&lt;p&gt;\begin{align}
u = \dfrac{1}{L_g L_f^{n-1}h(x)}[\nu - L_f^n h(x)],
\label{eq:control}
\end{align}&lt;/p&gt;

&lt;p&gt;where \(\nu \in \mathbb{R} \) is the new input, leading to the LTI system&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\dot{z}_1 = z_2, \\
\dot{z}_2 = z_3, \\
\quad \vdots  \\
\dot{z}_{n-1} = z_n, \\
\dot{z}_n = \nu, \\
y = z_1
&lt;/script&gt;

&lt;p&gt;Vectorizing the two equations, we have&lt;/p&gt;

&lt;p&gt;\begin{align}
\dot{z} = A z + B \nu, \qquad y = C^T z, 
\label{eq:linear}
\end{align}&lt;/p&gt;

&lt;p&gt;where &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

A = 
\begin{bmatrix}
0      &amp;    1   &amp;    0   &amp; \ldots &amp;    0   \\
\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \\
\vdots &amp;        &amp; \ddots &amp; \ddots &amp;    0   \\
\vdots &amp;        &amp;        &amp; \ddots &amp;    1   \\
0      &amp; \ldots &amp; \ldots  &amp; \ldots &amp;    0   \\
\end{bmatrix},

\qquad 
B = 
\begin{bmatrix}
0 \\
\vdots \\
\vdots \\
1
\end{bmatrix},

\quad 
C = \begin{bmatrix}
1 \\ 0 \\ \vdots \\ \vdots \\ 0
\end{bmatrix}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;\eqref{eq:linear} is an observable and controllable LTI system, and thus the input \(\nu\) can be carefully chosen to meet regulation or tracking objectives for the plant output \(y\). The \eqref{eq:control} cancels all nonlinearities and turns the closed-loop system to an LTI one. If \eqref{eq:nlnr1} has relative degree \(\rho &amp;lt; n \), the change of coordinates become&lt;/p&gt;

&lt;p&gt;\begin{align}
z_1 = y, \quad z_2 = \dot{y}, \ldots \quad z_\rho = y^{(\rho-1)} = L_f^{(\rho -1)h(x) }
\end{align}&lt;/p&gt;

&lt;p&gt;thus resulting in &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\dot{z}_1 = z_2 \\
\vdots \\
\dot{z}_{\rho-1} = z_\rho \\
\dot{z}_\rho = L_f^\rho h(x) + \left(L_gL_f^{\rho-1}h(x)\right) u
&lt;/script&gt;

&lt;p&gt;Since the order of the system is \(n\), we are gonna need \(n - \rho \) states; we can define functions \(h_{\rho+1}(x), \ldots, h_n(x)\) with \(\frac{\partial{h}_i(x)}{\partial{x}g(x)} = 0, \, i = \rho +1, \ldots, n\) and define the \(n - \rho\)  states as &lt;/p&gt;

&lt;p&gt;\(z_{\rho+1} = h_{\rho + 1}, \ldots, z_n = h_n(x)\)&lt;/p&gt;

&lt;p&gt;arising in the additional states&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\dot{z}_{\rho+1} = \dfrac{\partial{h}_{\rho+1}(x)}{\partial{x}}\cdot f(x) \triangleq \phi\_{\rho +1}(z), \\
\quad \vdots \\
\dot{z}_{n} = \dfrac{\partial{h}_{n}(x)}{\partial{x}}\cdot f(x) \triangleq \phi_{n}(z), \\
y = z_1
&lt;/script&gt;

&lt;p&gt;where \(z = [z_1, z_2, \ldots, z_n]^T\) is the new state. With feedback linearization, we have &lt;/p&gt;

&lt;p&gt;\begin{align}
u = \dfrac{1}{L_g L_f^{\rho-1}h(x)}[\nu - L_f^\rho h(x)],
\label{eq:control2}
\end{align}&lt;/p&gt;

&lt;p&gt;so that we have the system&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\dot{z}_1 = z_2 \\
\vdots \\
\dot{z}_{\rho-1} = z_\rho \\
\dot{z}_\rho = \nu, \\
\dot{\rho + 1} = \phi_{\rho +1}(z), \\
\quad \vdots \\
\dot{z}_n = \phi_n(z), \\
y = z_1.
&lt;/script&gt;

&lt;p&gt;We see that the input \(\nu\) may be utilized to drive the output \(y\) and states \(z_1, \ldots, z&lt;em&gt;\rho\) to zero or meet some regulation goal for \(y\). When the choice of controller \(\nu\) does not guarantee that the states \(z&lt;/em&gt;{\rho+1}, \ldots, z_n \) are bounded despite \(z_1, \ldots, z_\rho\) being  driven to zero, we say the states \(\dot{z}_{\rho+1}, \ldots, \dot{z}_n\) are the zero dynamics of \eqref{eq:nlnr1}. These are the dynamics of \eqref{eq:nlnr1} when \(y\) and its first \(\rho\) derivatives are set to zero. When the equilibrium states of the \(z_{\rho+1}\) are asymptotically stable, the system is said to be &lt;i&gt;minimum-phase&lt;/i&gt;.&lt;/p&gt;

&lt;p&gt;The process illustrated above is called I/O feedback linearization. When there are no zero dynamics involved, the process is called &lt;i&gt; full-state feedback linearization&lt;/i&gt;.&lt;/p&gt;

&lt;h2 id=&quot;control-lyapunov-functions&quot;&gt;Control Lyapunov Functions&lt;/h2&gt;
</description>
        <pubDate>2017-04-03 14:25:00 -0500</pubDate>
        <link>lakehanne.github.ioAdaptive-Control-Notes</link>
        <guid isPermaLink="true">lakehanne.github.ioAdaptive-Control-Notes</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Algorithms and Data Structures&lt;/center&gt;</title>
        <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;!--Mathjax Parser --&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;h3 id=&quot;table-of-contentstable-of-contents&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;Table of Contents&lt;/a&gt;:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#warm-up-questions&quot;&gt;Soft-Balls&lt;/a&gt;  &lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#array-decays-to-a-pointer&quot;&gt;Array decays to a pointer&lt;/a&gt;    &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#atoi-implementation&quot;&gt;Atoi Implementation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reverse-a-string&quot;&gt;Reversing a string&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#const-iterator&quot;&gt;Const Iterator on a vector dynamic set&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#unordered-map&quot;&gt;Unordered Map&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#matrix-multiply&quot;&gt;Matrix Multiply&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sorting&quot;&gt;Sorting&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#insert-sort&quot;&gt;Insert-Sort (2.1-1)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#insert-sort-in-descending-order&quot;&gt;Insert-Sort in Descending Order (2.1-2)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lineSearch&quot;&gt;LineSearch (2.1-3)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#merge-sort&quot;&gt;Merge-Sort (2.3-1)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#heapsort&quot;&gt;HeapSort&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#quicksort&quot;&gt;QuickSort&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#randomized-quicksort&quot;&gt;Randomized-QuickSort&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-structures&quot;&gt;Data Structures&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#queue&quot;&gt;Queues&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#stack&quot;&gt;Stack&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#min&quot;&gt;Minimum/Maximum value in an array&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#Simulatneous-min-and-max&quot;&gt;Simulatneous minimum and maximum in an array&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#Deque&quot;&gt;Deque&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#list-array&quot;&gt;Array-based List&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#singly-linked-list&quot;&gt;Singly-Linked List&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#doubly-linked-list&quot;&gt;Doubly-Linked List&lt;/a&gt; &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#binary-tree&quot;&gt;Binary Search Tree&lt;/a&gt;   &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;introduction&quot;&gt;&lt;/a&gt;
## &lt;code&gt;Introduction&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;These are C++/Python solutions to some of the algorithm problems in the Introduction to Algorithms book by Cormen et. al. Zhenchao Gan has some pseudocode solutions to similar problems on his &lt;a href=&quot;https://github.com/gzc/CLRS&quot;&gt;github page&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Please note that it is not typical that you would be directly asked to implement most of these algorithms. But knowing these algorithms and how to implement them in code would help you in faster answering your questions.&lt;/p&gt;

&lt;p&gt;These solutions have been tested using &lt;code&gt;g++ 4.8.4&lt;/code&gt; with c++ 11/14 support on Ubuntu 14.04. The solutions are being developed over time. You may find more solutions on this page if you check in at a a later time.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;warm-up-questions&quot;&gt;&lt;/a&gt;
## &lt;code&gt;Soft-balls&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Usually, there will be a first &lt;strong&gt;&lt;i&gt;soft-ball&lt;/i&gt;&lt;/strong&gt; question which you are expected to finish in a reasonable amount of time before a &lt;strong&gt;&lt;i&gt;harder&lt;/i&gt;&lt;/strong&gt; question that will test your understanding of data structures. I gleaned these soft-ball questions from the internet (since I am bound by an NDA, I am not allowed to share the questions I got) and answer a few of them in this section. The first question is usually for the interviewer to figure out if you can actually write code. You are being hired to write code. Better be ready to prove it. If you do not pass this stage, you might not make it to the next stage(s) of interview(s). I’ve heard a lot of people flunk it at this stage. So it is worth the while boning up on your basic coding skills. If you are a PhD student, you might be thinking coding is beneath you. But trust me, you &lt;i&gt;actually&lt;/i&gt; have to practice writing the code without an IDE in order not to make mistakes during the whiteboard/doc coding interview(s).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;array-decays-to-a-pointer&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Array decays to a pointer&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In C++, know that the raw array &lt;code&gt;A[]&lt;/code&gt; decays to a pointer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/src/arraydecaystopointer.cxx&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;atoi-implementation&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Atoi Implementation&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Atoi converts the pointee in the pointer &lt;code&gt;char* argv&lt;/code&gt; (or pointees in the pointer to an array 
&lt;code&gt;char *argv[]/char **argv&lt;/code&gt;) to an integer. This code should be easily adaptable for &lt;code&gt;atoll&lt;/code&gt; and its other variants&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/atoi.cxx&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;reverse-a-string&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Reverse a String&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is a fairly common interview question. Implemented in evil C.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/reverse.c&quot;&gt;C Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;const-iterator&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Const Iterator on a vector dynamic set&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is to test your understanding of dynamic set operations and how a pointer that reads from memory 
should not modify the data it is reading from. A similar question might be to implement a print function from a data structure (in this case, remember to add a const keyword in front of the print member function). &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/vec.cxx&quot;&gt;C++ 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;unordered-map&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Unordered Map&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Interviewers sometimes ask this to get a feel for your understanding of data structures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/unordered_map.cxx&quot;&gt;C++ 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;matrix-multiply&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Multiplying-two-square-matrices&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is also a fairly common question. The standard time-based multiplication requires \(\theta(n^3)\) time. If you are asked to optimize the code, &lt;a href=&quot;https://en.wikipedia.org/wiki/Strassen_algorithm&quot;&gt;Strassen’s algorithm&lt;/a&gt; allows us to do the multiplication in \(\theta(n^{\, lg 7})\ \approx \theta^{2.8074}\) time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Matmuls/sqmatmaul.py&quot;&gt;Python Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;sorting&quot;&gt;&lt;/a&gt;
## &lt;code&gt;Sorting&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here, I illustrate with examples two \(\theta(n \, lg \, n)\) worst-case algorithms (i.e. mergesort and heapsort), and two  \(\theta(n^2)\) worst-case algorithms (i.e. insertion sort and quicksort). I also implement the randomized version of the quicksort algorithm in &lt;code&gt;Python&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;insert-sort&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Insert-Sort (CLRS 2.1-1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Using Figure 2.2 as a model, illustrate the operation of &lt;code&gt;INSERTION-SORT&lt;/code&gt; on the
array \(A= (31, 41, 59, 26, 41, 58)\).&lt;/p&gt;

&lt;p&gt;This sorts in increasing order from left to right.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Sorting/insertsort.cxx&quot;&gt;C++ 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;insert-sort-in-descending-order&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Insert-Sort in Descending Order (2.1-2)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Rewrite the INSERTION-SORT procedure to sort into non-increasing instead of non-decreasing order.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Sorting/insertsort_descending.cxx&quot;&gt;C++ 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;lineSearch&quot;&gt;&lt;/a&gt;
### &lt;code&gt;LineSearch (2.1-3)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Consider the searching problem:&lt;/p&gt;

&lt;p&gt;Input: A sequence of n numbers \(A = (a_1, a_2, \ldots , a_n) \) and a value \(\nu.\)&lt;/p&gt;

&lt;p&gt;Output: An index i such that \(\nu = A[i] \) or the special value NIL if \(\nu\) does not appear in \(A\).
Write pseudocode for &lt;em&gt;&lt;strong&gt;linear search&lt;/strong&gt;&lt;/em&gt;, which scans through the sequence, looking for \(\nu\). Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties.&lt;/p&gt;

&lt;p&gt;Complexity: \(O(n)\) as you have to go though every element in the list in the worst case.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Sorting/linesearch.cxx&quot;&gt;C++ 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;merge-sort&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Merge-Sort (2.3-1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Using Figure 2.4 as a model, illustrate the operation of merge sort on the array \( A = (3, 41, 52, 26, 38, 57, 9, 49) \).&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Complexity: Worst case: \(\theta(n \, lg \, n)\)&lt;/td&gt;
      &lt;td&gt;Average Case: \(\theta(n \, lg \, n)\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Sorting/mergesort.cxx&quot;&gt;C++ 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Sorting/mergesort.py&quot;&gt;Python Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;heapsort&quot;&gt;&lt;/a&gt;
### &lt;code&gt;HeapSort&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Complexity: Worst case: \(O(n \, lg \, n)\). The Max-Heapify algorithm is \(O(lg \, n)\) while the build-max heap algorithm takes \(O(n)\) time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Sorting/heapsort.cxx&quot;&gt;C++ 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Sorting/heapsort.py&quot;&gt;Python Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;quicksort&quot;&gt;&lt;/a&gt;
### &lt;code&gt;QuickSort&lt;/code&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Complexity: Worst case: \(\theta(n^2\)&lt;/td&gt;
      &lt;td&gt;Average Case: \(\theta(n^2\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Sorting/quicksort.py&quot;&gt;Python Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;randomized-quickSort&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Randomized-QuickSort&lt;/code&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Complexity: Worst case: \(\theta(n^2)\)&lt;/td&gt;
      &lt;td&gt;Average Case: \(\theta(n^2)\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here, the only difference from quicksort is that we randomize the choice of the end of our subarray.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/Sorting/randomized_quicksort.py&quot;&gt;Python Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;data-structures&quot;&gt;&lt;/a&gt;
## Data Structures&lt;/p&gt;

&lt;p&gt;This is the meat of most big-tech company interviews and you should try your best to know what is going on &lt;strong&gt;under the hood&lt;/strong&gt; in these algorithms. CLRS is a good reference. Covering the Part II section of the book is very important to understanding these algorithms. In this section, I provide implementations in C++  11 and python for &lt;code&gt;randomized binary trees&lt;/code&gt;, &lt;code&gt;hash maps&lt;/code&gt; and &lt;code&gt;lists&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;queue&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Queues&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Complexity: Each enqueue and queue dictionary operation takes \(O(1)\) time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/DataStructures/queue.cxx&quot;&gt;C++ 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;stack&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Stack&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Complexity: Each of pop, and push operation takes \(O(1)\) time.&lt;/p&gt;

&lt;p&gt;Question 10.1&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/DataStructures/stack.cxx&quot;&gt;C++ 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;min&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Minimum/Maximum value in an array&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Complexity: \(O(n)\) worst-case time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/DataStructures/min.py&quot;&gt;Python 2.7 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Simulatneous-min-and-max&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Simulatneous minimum and maximum in an array&lt;/code&gt;
Section 9.1 in CLRS.&lt;/p&gt;

&lt;p&gt;This algorithm has a worst case running time of \(O(\frac{3n}{2})\). &lt;/p&gt;

&lt;p&gt;We are maintaining both the minimum and maximum elements seen thus far.
Rather than processing each element of the input by comparing it against 
the current minimum and maximum, at a cost of 2 comparisons per element,
we process elements in pairs. We compare pairs of elements from the input 
first with each other, and then we compare the smaller with the current 
minimum and the larger to the current maximum, at a cost of 3 comparisons 
for every 2 elements&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/DataStructures/simultaneous_minmax.py&quot;&gt;Python 2.7 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Deque&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Deque&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;10.1-5 in CLRS
Whereas a stack allows insertion and deletion of elements at only one end, and a
queue allows insertion at one end and deletion at the other end, a deque (double-ended queue) allows insertion and deletion at both ends. Write four \(O(1)\)-time
procedures to insert elements into and delete elements from both ends of a deque
implemented by an array.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/DataStructures/dequeue.py&quot;&gt;Python Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;list-array&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Array-based List&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Complexity is \(O(n)\) because of &lt;code&gt;INSERT&lt;/code&gt; and &lt;code&gt;FIND&lt;/code&gt;. My implementation supports a dynamic array-based list: it doubles the size of the list every time a new element is inserted that increases the index of the array beyond the default maximum.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/DataStructures/lists/listArray.cxx&quot;&gt;Cxx 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;singly-linked-list&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Singly-Linked List&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Complexity is \(O(n)\) because the &lt;code&gt;SEARCH&lt;/code&gt; dictionary operation may have to traverse the entire length of the list if the value being searched is not in the list.
The nodes are created with a freelist memory allocator derived from an overloaded &lt;code&gt;new&lt;/code&gt; operator.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/DataStructures/lists/listSinglyLinked.cxx&quot;&gt;Cxx 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;doubly-linked-list&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Doubly-Linked List&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Complexity is \(O(n)\) because the &lt;code&gt;SEARCH&lt;/code&gt; dictionary operation may have to traverse the entire list if the value being searched is not in the list.
Again, I overloaded the &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; operators with a freelist to better manage memory when we are inserting, searching or deleting large records.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/DataStructures/lists/listDoublyLinked.cxx&quot;&gt;Cxx 11 Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;binary-tree&quot;&gt;&lt;/a&gt;
### &lt;code&gt;Binary Search Tree&lt;/code&gt;
The worst case running time for any of the dynamic set operations, &lt;code&gt;INSERT&lt;/code&gt;, &lt;code&gt;SEARCH&lt;/code&gt;, &lt;code&gt;DELETE&lt;/code&gt;, &lt;code&gt;SUCCESSOR&lt;/code&gt;, &lt;code&gt;PREDECESSOR&lt;/code&gt;, &lt;code&gt;MINIMUM&lt;/code&gt;, &lt;code&gt;MAXIMUM&lt;/code&gt; on a binary tree of height \(h\) is \(O(h)\). If the tree has \(n\)-elements, it has a worst-case running time of \(O(lg \, n)\) with&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lakehanne/CLRS/blob/master/src/DataStructures/binaryTrees/binaryTree.cxx&quot;&gt;Cxx 11 Code&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>2016-12-23 06:25:00 -0600</pubDate>
        <link>lakehanne.github.ioCLRS-C++</link>
        <guid isPermaLink="true">lakehanne.github.ioCLRS-C++</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Cloning specific folders from git&lt;/center&gt;</title>
        <description>&lt;!--##Table of Contents
###[Sparse Checkout](#sparse-checkout)
###[SVN Checkout](#svn-checkout)
## Directory &amp;&amp; Sub-directory checkout from git repos--&gt;

&lt;p&gt;This is a relatively newie but a goodie.  Have you ever been stuck trying to clone specific folders from a &lt;code&gt;git&lt;/code&gt; repo recently?&lt;/p&gt;

&lt;p&gt;Well, starting from &lt;code&gt;git 1.9&lt;/code&gt;, this feature is now part of git features. For the sake of illustration, let’s say we want to retrieve only the &lt;a href=&quot;https://github.com/PointCloudLibrary/pcl/tree/master/examples&quot;&gt;examples&lt;/a&gt; directory of the point cloud repo commited at the &lt;a href=&quot;https://github.com/PointCloudLibrary&quot;&gt;point cloud library git page&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;sparse-checkout&quot;&gt;Sparse-Checkout&lt;/h3&gt;
&lt;p&gt;A new feature called &lt;a href=&quot;https://git-scm.com/docs/git-read-tree/&quot;&gt;sparse checkout&lt;/a&gt; allows us to sparsely populate the working directory by using skip-worktree bit to inform &lt;code&gt;GIT&lt;/code&gt; if the file in the working directory deserves a look. &lt;i&gt;git read-tree&lt;/i&gt; and other merge commands native to &lt;code&gt;git&lt;/code&gt; such as &lt;code&gt;checkout&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, or &lt;code&gt;pull&lt;/code&gt; are useful in maintaining the skip-working tree bitmap and working directory update. &lt;/p&gt;

&lt;p&gt;A paraphrased quote from the manual here:&lt;/p&gt;

&lt;p&gt;”&lt;code&gt;$GIT_DIR/info/sparse-checkout&lt;/code&gt; defines the skip-worktree reference bitmap. When git read-tree needs to update the `working directory, it resets the skip-worktree bit in the index based on this file, which uses the same syntax as .gitignore files. If an entry matches a pattern in this file, skip-worktree will not be set on that entry. Otherwise, skip-worktree will be set.&lt;/p&gt;

&lt;p&gt;Then it compares the new skip-worktree value with the previous one. If skip-worktree turns from set to unset, it will add the corresponding file back. If it turns from unset to set, that file will be removed.&lt;/p&gt;

&lt;p&gt;“While &lt;code&gt;$GIT_DIR/info/sparse-checkout&lt;/code&gt; is usually used to specify what files are in, you can also specify what files are not in, using negate patterns. For example, to remove the file unwanted:&lt;/p&gt;

&lt;p&gt;Another tricky thing is fully repopulating the working directory when you no longer want sparse checkout. You cannot just disable “sparse checkout” because skip-worktree bits are still in the index and your working directory is still sparsely populated. You should re-populate the working directory with the $GIT_DIR/info/sparse-checkout file content as follows:&lt;/p&gt;

&lt;p&gt;So to check out the pcl examples directory for example, we could combine the &lt;code&gt;sparse checkout&lt;/code&gt; and &lt;code&gt;shallow clone&lt;/code&gt; features. By using the &lt;code&gt;shallow clone&lt;/code&gt; feature, we cut off the history and the &lt;code&gt;sparse check out&lt;/code&gt; only pulls files matching the pattern(s) we specify. &lt;/p&gt;

&lt;p&gt;Take a look at the following example:
&amp;lt;pre class=&quot;terminal&quot;&amp;gt;&lt;code&gt;
$ mkdir pcl-examples
$ cd pcl-examples								#make a directory we want to copy folders to
$ git init                            			#initialize the empty local repo
$ git remote add origin -f https://github.com/PointCloudLibrary/pcl.git     #add the remote origin
$ git config core.sparsecheckout true			#very crucial. this is where we tell git we are checking out specifics
$ echo &quot;examples/*&quot; &amp;gt;&amp;gt; .git/info/sparse-checkout #recursively checkout examples folder
$ git pull --depth=2 origin master			#go only 2 depths down the examples directory
&lt;/code&gt;&amp;lt;/pre&amp;gt;&lt;/p&gt;

&lt;h4 id=&quot;explanation&quot;&gt;Explanation&lt;/h4&gt;

&lt;p&gt;The line 
                  &amp;lt;pre class=&quot;terminal&quot;&amp;gt;&lt;code&gt;$git remote add origin -f https://github.com/PointCloudLibrary/pcl.git &lt;/code&gt;&amp;lt;/pre&amp;gt; &lt;/p&gt;

&lt;p&gt;adds a remote named &lt;name&gt; e.g. a repository given by &lt;url&gt;.&lt;/url&gt;&lt;/name&gt;&lt;/p&gt;

&lt;p&gt;It does not create nor update remote working branches by any chance. We do that by adding the “-f” or “–fetch” argument to update all remote tracking branches in our index. Note that this merely updates the git index. The files nor folders are as yet not populated.&lt;/p&gt;

&lt;p&gt;The files are updated in our &lt;code&gt;pcl-examples&lt;/code&gt; directory with the next line’s command.&lt;/p&gt;

&lt;p&gt;Since we are cloning everything in the examples directory –which, by the way, have a depth of 2 – we pull every subdirectory and file under the examples folder by doing:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ git pull --depth=2 origin master&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;svn-checkout&quot;&gt;SVN Checkout&lt;/h3&gt;

&lt;p&gt;If you are using &lt;code&gt;svn&lt;/code&gt; instead of git, there is a straightforward way to do this. Simply cd into your working directory and replace the “&lt;code&gt;/tree/master&lt;/code&gt;” path within the &lt;code&gt;url&lt;/code&gt; with &lt;code&gt;trunk&lt;/code&gt;. To clone the subdirectory &lt;code&gt;examples&lt;/code&gt; in the point cloud git repo for example, using &lt;code&gt;svn&lt;/code&gt;, we would do the following in terminal&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ svn checkout https://github.com/PointCloudLibrary/pcl/trunk/examples&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>2016-03-28 07:23:00 -0500</pubDate>
        <link>lakehanne.github.iogit-sparse-checkout</link>
        <guid isPermaLink="true">lakehanne.github.iogit-sparse-checkout</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Optimal Control: Model Predictive Controllers.&lt;/center&gt;</title>
        <description>&lt;!-- Google Tag Manager --&gt;
&lt;noscript&gt;&lt;iframe src=&quot;//www.googletagmanager.com/ns.html?id=GTM-WXGQX2&quot; height=&quot;0&quot; width=&quot;0&quot; style=&quot;display:none;visibility:hidden&quot;&gt;&lt;/iframe&gt;&lt;/noscript&gt;
&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WXGQX2');&lt;/script&gt;

&lt;!-- End Google Tag Manager --&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;!--Mathjax Parser --&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
&lt;/script&gt;

&lt;table border=&quot;5&quot; width=&quot;100%&quot; cellpadding=&quot;1&quot; cellspacing=&quot;2&quot;&gt;

    &lt;tr&gt;
      &lt;th colspan=&quot;4&quot;&gt;&lt;br /&gt;&lt;h3&gt;Table of Notations and Abbreviations&lt;/h3&gt;
      &lt;/th&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
      &lt;th&gt;Notation&lt;/th&gt;
      &lt;th&gt;Meaning&lt;/th&gt;
      &lt;th&gt;Abbreviation&lt;/th&gt;
      &lt;th&gt;Meaning&lt;/th&gt;
    &lt;/tr&gt;

    &lt;tr align=&quot;CENTER&quot;&gt;
      &lt;td&gt;$z^{-1}$&lt;/td&gt;
      &lt;td&gt;Unit delay operator&lt;/td&gt;  
      &lt;td&gt;MFD&lt;/td&gt;
      &lt;td&gt;Matrix Fraction Description&lt;/td&gt;   
    &lt;/tr&gt;

    &lt;tr align=&quot;CENTER&quot;&gt;
      &lt;td&gt;$x_{\rm \rightarrow}$&lt;/td&gt;
      &lt;td&gt;Future values of x&lt;/td&gt;  
      &lt;td&gt;LTI&lt;/td&gt;
      &lt;td&gt;Linear Time Invariant&lt;/td&gt;    
    &lt;/tr&gt;   

    &lt;tr align=&quot;CENTER&quot;&gt;
      &lt;td&gt;$x_{\rm \leftarrow}$&lt;/td&gt;
      &lt;td&gt;Past values of x&lt;/td&gt; 
      &lt;td&gt;FSR&lt;/td&gt;
      &lt;td&gt;Finite Step Response&lt;/td&gt;     
    &lt;/tr&gt;  

    &lt;tr align=&quot;CENTER&quot;&gt;
      &lt;td&gt;$\Delta = 1 - z^{-1}$&lt;/td&gt;
      &lt;td&gt;Differencing Operator&lt;/td&gt;  
      &lt;td&gt;FIR&lt;/td&gt;
      &lt;td&gt;Finite Impulse Response&lt;/td&gt;  
    &lt;/tr&gt;  

    &lt;tr align=&quot;CENTER&quot;&gt;
      &lt;td&gt;$z^{-1}$&lt;/td&gt;
      &lt;td&gt;Backward shift operator (z-transforms)&lt;/td&gt;   
      &lt;td&gt;TFM&lt;/td&gt;
      &lt;td&gt;Transfer Function Model&lt;/td&gt;
    &lt;/tr&gt;  

    &lt;tr align=&quot;CENTER&quot;&gt;
      &lt;td&gt;$n_y$&lt;/td&gt;
      &lt;td&gt;Output Prediction Horizon&lt;/td&gt;    
      &lt;td&gt;SISO&lt;/td&gt;
      &lt;td&gt;Single-Input Single Output&lt;/td&gt;
    &lt;/tr&gt; 

    &lt;tr align=&quot;CENTER&quot;&gt;
      &lt;td&gt;$n_u$&lt;/td&gt;
      &lt;td&gt;Input Horizon&lt;/td&gt;  
      &lt;td&gt;MIMO&lt;/td&gt;
      &lt;td&gt;Multiple-Input Multiple Output&lt;/td&gt;
    &lt;/tr&gt;   

    &lt;tr align=&quot;CENTER&quot;&gt;
      &lt;td&gt;$u(k-i)$&lt;/td&gt;
      &lt;td&gt;Past input&lt;/td&gt; 
      &lt;td&gt;IMC&lt;/td&gt;
      &lt;td&gt;Internal Model Control&lt;/td&gt;
    &lt;/tr&gt; 

    &lt;tr align=&quot;CENTER&quot;&gt;
      &lt;td&gt;$y(k-i)$&lt;/td&gt;
      &lt;td&gt;Past output&lt;/td&gt;
      &lt;td&gt;IM&lt;/td&gt;
      &lt;td&gt;Independednt Model&lt;/td&gt;
    &lt;/tr&gt;      
&lt;/table&gt;

&lt;h3 id=&quot;centeroptimal-controllers-an-overviewcenter&quot;&gt;&lt;center&gt;**Optimal Controllers: An Overview**&lt;/center&gt;&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;If you are already familiar with LQ methods, you can skip this section and go straight to &lt;a href=&quot;#Model-Predictive-Controllers&quot;&gt;&lt;strong&gt;Model Predictive Controllers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Optimal controllers belong to the class of controllers that minimize a cost function with respect to a &lt;u&gt;&lt;b&gt;predicted&amp;lt;/u&amp;gt;&lt;/b&gt; control law over a given &lt;u&gt;&lt;b&gt;prediction horizon&amp;lt;/u&amp;gt;&lt;/b&gt;. They generate a desired &lt;u&gt;&lt;b&gt;output control&amp;lt;/u&amp;gt;&lt;/b&gt; sequence from which the optimal &lt;u&gt;&lt;b&gt;_control law_&amp;lt;/u&amp;gt;&lt;/b&gt; may be determined. &lt;/u&gt;&lt;/u&gt;&lt;/u&gt;&lt;/u&gt;&lt;/p&gt;

&lt;!-- &lt;br&gt;&lt;/br&gt; --&gt;
&lt;p&gt;Take your car cruise speed control for an example. The vehicle dynamics of the car is &lt;em&gt;_modeled_&lt;/em&gt; into the cruise control system. The road curvature ahead of you gets &lt;em&gt;_predicted_&lt;/em&gt; online as you drive and &lt;u&gt;_control sequences_&lt;/u&gt; are generated based on an &lt;em&gt;*internal model*&lt;/em&gt; &lt;em&gt;(*prediction*)&lt;/em&gt; of the slope of the road/road curvature using past observations; other disturbances (such as rain, friction between tire and road) are modeled into the prediction ideally. At time \(k + 1\), only the first element (&lt;b&gt;&lt;em&gt;control action&lt;/em&gt;&lt;/b&gt;) within the generated control sequence is used as a &lt;em&gt;feedback&lt;/em&gt; control mechanism for your car. At the next &lt;b&gt;&lt;em&gt;sampling time instant&lt;/em&gt;&lt;/b&gt;, new predictions are made based on the prediction horizon and a new control sequence is generated online. Again, the chosen control action is the first element in the control sequence such  that we are constantly choosing the best among a host of control actions using our anticipated &lt;b&gt;&lt;em&gt;prediction&lt;/em&gt;&lt;/b&gt; of the road curvature and other environmental variables to give the desired &lt;b&gt;&lt;em&gt;performance&lt;/em&gt;&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;
Optimal controllers are mostly used in discrete processes (&lt;a href=&quot;#Gawthrop&quot;&gt;others&lt;/a&gt; have postulated continuous time models for such controllers but the vast majority of commercial users of optimal controllers are in discrete time) and are very useful for processes where the setpoint is known ahead of time. Typically, a criterion function is used to determine how well the controller is tracking a desired trajectory. An example model predictive controller (MPC) criterion function, in its basic form, is given as  &lt;/p&gt;

&lt;p&gt;&amp;lt;a name = eqn:cost-function&amp;gt;&amp;lt;/a&amp;gt;
\begin{equation}            \label{eqn:cost function}
  J  = \sum\nolimits_{i=n_w}^{n_y} [\hat{y}(k+i) - r(k + i)]^2 
\end{equation}  &lt;/p&gt;

&lt;p&gt;where \(i\) is typically taken as 1, \(n_y\) is the prediction horizon, \(\hat{y}(k+i)\) is the prediction and \(r(k+i)\) is the desired trajectory (or setpoint).&lt;/p&gt;

&lt;p&gt;The controller output sequence, \(\Delta u \), is obtained by minimizing \(J\) over the prediction horizon, \(n_y\), with respect to \(\Delta u \), i.e.,&lt;/p&gt;

&lt;p&gt;\begin{equation}
  \Delta u  = \text{arg} \min_{\rm \Delta u} J \label{eqn:min J} 
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\Delta u \) is the future control sequence.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;
If equation \eqref{eqn:cost function} is &lt;strong&gt;&lt;em&gt;well-posed&lt;/em&gt;&lt;/strong&gt;&lt;a href=&quot;#footnote-1&quot;&gt;\(^{1}\)&lt;/a&gt;, then \(\Delta u \) would be optimal with respect to the criterion function and hence &lt;a href=&quot;#eqn:carima&quot;&gt;eliminate offset in trajectory tracking&lt;/a&gt;. &lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
  &amp;lt;img src=&quot;/downloads/MPC/MPCConcept.jpg&quot; width=&quot;50%&quot; height=&quot;350&quot;, border=&quot;0&quot; style=&quot;float:left;&quot;&amp;gt;
  &lt;img src=&quot;/downloads/MPC/LQG.jpg&quot; width=&quot;50%&quot; height=&quot;350&quot; border=&quot;0&quot; style=&quot;float:right;&quot; /&gt;  
  &lt;div class=&quot;figcaption&quot; align=&quot;left&quot;&gt;Fig.1.0.0. Tracking by a Model Predictive Controller. &lt;div class=&quot;figcaption&quot; align=&quot;right&quot;&gt;Fig. 1.0.1. Reference Tracking by an LQ Controller.
  &lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;!--![MPCControl.jpg](/downloads/MPC/MPCConcept.jpg){: .center-image }--&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;
It becomes obvious from equation \eqref{eqn:cost function} that minimizing the criterion function is an optimization problem. One useful approach in typical problems is to make \(J\) &lt;b&gt;&lt;em&gt;quadratic&lt;/em&gt;&lt;/b&gt; such that the model becomes linear; &lt;u&gt;if there are no contraints, the solution to the quadratic minimization problem is analytic&lt;/u&gt;.&lt;/p&gt;

&lt;h4 id=&quot;centerlinear-quadratic-lq-controllerscenter&quot;&gt;&lt;center&gt;**Linear Quadratic (LQ) Controllers**&lt;/center&gt;&lt;/h4&gt;
&lt;p&gt;Linear Quadratic Controllers are indeed predictive controllers except for the infinite horizon which they employ in minimizing the criterion function. The criterion function is minimized only once resulting in an optimal controller output sequence from which the controller output is selected.
Similar to model predictive controllers, they are in general discrete time controllers. For a linear problem such as&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=eqn:state-model&amp;gt;&amp;lt;/a&amp;gt;
\begin{gather} \label{eqn:state-model}
  x(k+1) = A \, x(k) + B \, u(k)  \nonumber \newline 
  y(k) = C^T \, x(k)              \nonumber
\end{gather}&lt;/p&gt;

&lt;p&gt;The cost function is typically constructed as &lt;/p&gt;

&lt;p&gt;&amp;lt;a name=eqn:LQ-cost&amp;gt;&amp;lt;/a&amp;gt;
\begin{equation}  \label{eqn:LQ-cost}
J = \sum_{k=0}^n x^T(k)\,Q\,x(k) + R \, u(k)^T \, u(k) + 2 x(k)^T \, N \, u(k)
\end{equation}  &lt;/p&gt;

&lt;p&gt;where \(n\) is the terminal sampling instant, \(Q\) is a symmetric, positive semi-definite matrix that weights the \(n\)-states of the matrix. \(N\) specifies a matrix of appropriate dimensions that penalizes the cross-product between the input and state vectors, while \(R\) is a symmetric, positive defiite weighting matrix on the control vector , \(u\). &lt;/p&gt;

&lt;p&gt;Other variants of equation \eqref{eqn:LQ-cost} exist where instead of weighting the states of the system, we instead weight the output of the system by constructing the cost function as&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=eqn:LQ-costy&amp;gt;&amp;lt;/a&amp;gt;
\begin{equation}   \label{eqn:LQ-costy}
  J = \sum_{k=0}^n y^T(k) \,Q \,y(k) + R\, u(k)^T \, u(k) +  2 x(k)^T \, N \, u(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;In practice, it is typical to set the eigen values of the \(Q\)-matrix to one while adjusting the eigenvalue(s) of the \(R\) matrix until one obtains the desired result. \(N\) is typically set to an appropriate matrix of zeros. This is no doubt not the only way of tuning the LQ controller as fine control would most likely require weighting the eigen-values of the Q-matrix differently (this is from my experience when tuning LQ controllers).&lt;/p&gt;

&lt;p&gt;If we model disturbances into the system’s states, the optimization problem becomes a stochastic optimization problem that must be solved. But the separation theorem applies such that we can construct a state estimator which asymptotically tracks the internal states from observed outputs using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Algebraic_Riccati_equation&quot;&gt;algebraic Riccati equation&lt;/a&gt; given as &lt;/p&gt;

&lt;p&gt;\begin{equation}    \label{eqn:Riccati}
  A^T P A -(A^T P B + N)(R + B^T P B)^{-1}(B^T P A + N) + Q.
\end{equation}&lt;/p&gt;

&lt;p&gt;\(P\) is an unknown \(n \times n\) symmetric matrix and \(A\), \(B\), \(Q\), and \(R\) are known coefficient matrices as in equations \eqref{eqn:LQ-cost} and \eqref{eqn:LQ-costy}. We find an optimal control law by solving the minimization of the deterministic LQ problem, equation \eqref{eqn:LQ-cost} which we then feed into the states. &lt;/p&gt;

&lt;p&gt;The optimal controller gains, \(K\), are determined from the equation&lt;/p&gt;

&lt;p&gt;\begin{equation}
  K_{lqr} = R^{-1}(B^T \, P + N^T)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(P\) is the solution to the algebraic Riccati equation \eqref{eqn:Riccati}.&lt;/p&gt;

&lt;p&gt;In classical control, we are accustomed to solving a control design problem by determining design parameters based on a given set of design specifications (e.g. desired overshoot, gain margin, settiling time, e.t.c). Discrete LQ controllers pose a significant difficulty with respect to solving such problems as it is challenging to solve the criterion function in terms of these design parameters since discrete state space matrices hardly translate to any physical meaning. If you use the &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00207728808964057&quot;&gt;orthogonal least squares algorithm&lt;/a&gt; or &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00207178908559767&quot;&gt;forward regression orthogonal least squares&lt;/a&gt; in identifying your physical system, you might be able to make some sense of your model though. &lt;/p&gt;

&lt;p&gt;It turns out that if the cost function is quadratic in the parameters and the state space matrices are in  &lt;strong&gt;&lt;em&gt;continuous time&lt;/em&gt;&lt;/strong&gt;, the weighting matrices would no longer correspond to the artificial, \(A\), \(B\), and \(C\) matrices (which is indeed what they are in the discrete state space).&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=Model-Predictive-Controllers&amp;gt;&amp;lt;/a&amp;gt;
####**&lt;center&gt;Model Predictive Controllers (MPC)**&lt;/center&gt;
A key thing about LQ closed loop problems is that we can pretty much guarantee closed loop stability if the prediction horizon, \(N \rightarrow \infty\). Model Predictive Controllers, on the other hand, employ a &lt;a href=&quot;http://www.cds.caltech.edu/~murray/books/AM08/pdf/obc08-rhc_30Jan08.pdf&quot;&gt;receding (which is a finite) horizon concept&lt;/a&gt; in establishing the control sequence and closed-loop stability is generally not guaranteed. &lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;http://www.mdpi.com/energies/energies-08-01505/article_deploy/html/images/energies-08-01505-g005-1024.png&quot; width=&quot;60%&quot; height=&quot;450&quot; align=&quot;middle&quot; /&gt;  
  &lt;div class=&quot;figcaption&quot; align=&quot;left&quot;&gt;Fig. 1. Receding Horizon Concept. &lt;i&gt;Courtesy, &lt;a href=&quot;http://www.mdpi.com/1996-1073/8/2/1505&quot;&gt; MDPI &lt;/a&gt;&lt;/i&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- ![Receding Horizon Concept](http://www.mdpi.com/energies/energies-08-01505/article_deploy/html/images/energies-08-01505-g005-1024.png) --&gt;

&lt;p&gt;Basically, for inifinite horizon controllers such as LQ methods, this means there is no model mismatch between the predictor model and the plant. In practice, this is tough to achieve as disturbances play a large role in virtually all real-world systems. Typically in state-space or transfer function control model structures, we’ll add an integrator in the feedback loop to correct for offset errors at steady state. This would not be optimal in zeroing steady-state errors in an MPC approach. One way of avoiding mismatch is to include an integrator in the prediction model as an &lt;strong&gt;&lt;em&gt;internal model&lt;/em&gt;&lt;/strong&gt; of the disturbance. This could be a &lt;strong&gt;CARIMA&lt;/strong&gt;  (&lt;strong&gt;C&lt;/strong&gt;ontrolled &lt;strong&gt;A&lt;/strong&gt;uto &lt;strong&gt;R&lt;/strong&gt;egressive &lt;strong&gt;I&lt;/strong&gt;ntegral &lt;strong&gt;M&lt;/strong&gt;oving &lt;strong&gt;A&lt;/strong&gt;verage) model, for example. A typical &lt;strong&gt;CARIMA&lt;/strong&gt; model takes the polynomial form&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=eqn:carima&amp;gt;&amp;lt;/a&amp;gt;
\begin{equation}  \label{eqn:carima}
  A(z^{-1})\,y(k) = B(z^{-1})\,u(k) + \dfrac{C(z^{-1})}{1-z^{-1}}\,e(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(A\), \(B\), and \(C\) are polynomials in the backward shift operator \(z^{-1}\) given by&lt;/p&gt;

&lt;p&gt;\begin{equation} \label{eqn:poly}
  A(z^{-1}) = 1 + a_1\,z^{-1} + a_2 + \, z^{-2} + \cdots + a_{n_a}z^-{n_a}  \nonumber
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
  B(z^{-1}) = b_1 + b_2\, z^{-1} + b_3 \, z^{-2} + \cdots + b_{n_b}z^-{n_b}  \nonumber
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
  C(z^{-1}) = c_0 + c_1\, z^{-1} + c_2 \, z^{-2} + \cdots + c_{n_b}z^-{n_c},
\end{equation}&lt;/p&gt;

&lt;p&gt;and \(y(k)\), \(u(k)\) and \(e(k)\), for \(k = 1, 2\), \(\cdots \)  are respectively the plant output, input and integrated noise term in the model. &lt;/p&gt;

&lt;p&gt;MPC’s are generally good and better than traditional PID controllers if correctly implemented in that they handle disturbances typically well and can anticipate future disturbances thereby making the control action more effective as a result. This is because of an &lt;i&gt;&lt;strong&gt;internal model&lt;/strong&gt;&lt;/i&gt; of the plant that allows them to anticipate the future “behavior” of a plant’s output and mitigate such errors before the plant reaches the &lt;i&gt;“future time”&lt;/i&gt;. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.sheffield.ac.uk/acse/staff/jar&quot;&gt;Rossiter&lt;/a&gt; gives a classic analogy in the way human beings cross a road. It is not sufficient that a road is not busy with passing cars on it. As you cross, you constantly look at ahead (your prediction horizon) to anticipate oncoming vehicles and update your movement (=control action) based on your prediction (based on past observations).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Accurate prediction over a selected horizon is critical to a successful MPC implementation&lt;/strong&gt;. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Below are a brief overview of common models employed in MPC algorithms. Readers are referred to System Identification texts e.g., &lt;a href=&quot;#box-jenkins&quot;&gt;Box and Jenkins&lt;/a&gt;,  &lt;a href=&quot;#billings&quot;&gt;Billings&lt;/a&gt;, or &lt;a href=&quot;#Ljung&quot;&gt;Ljung&lt;/a&gt; where data modeling and system identification methods are elucidated in details.&lt;/p&gt;

&lt;!-- &lt;div class=&quot;fig figcenter fighighlight&quot;&gt; 
  &lt;img src=&quot;/downloads/MPC/lrpc.jpg&quot; width=&quot;75%&quot; height=&quot;450&quot;, border=&quot;0&quot; style=&quot;float:center;&quot;&gt; 
  &lt;div class=&quot;figcaption&quot; align=&quot;center&quot;&gt;Fig.2.1. Receding Horizon Concept (Copyright: Mahdi Mahfouf, University of Sheffield, 2011 - 2012). 
&lt;/div&gt;
&lt;/div&gt; --&gt;

&lt;h4 id=&quot;centerlinear-models-in-predictive-controlcenter&quot;&gt;&lt;center&gt;**Linear Models in Predictive Control**&lt;/center&gt;&lt;/h4&gt;
&lt;p&gt;An essential part of an MPC design is the internal model of the plant. If we want a high-fidelity control, we would want to develop a high fidelity model. For optimal control, our controller would only be as good as our model. If our understanding of the plant is faulty and this transfers to the model, we would typically have model mismatch between plant and predictor and our implementation of the MPC may be worse than using an infinite horizon control.&lt;/p&gt;

&lt;p&gt;We briefly discuss the common classical models that have been accepted as a standard by the control community in the past two-some decades. Linear models obey the superposition principle. Put differently, their internal structure can be approximated by a linear function in the proximity of the desired operating point. In general, linear system identification belong in two large categories: parametric and non-parametric methods. I will only touch upon parametric methods as these are the most commonly employed models in MPC approaches.&lt;/p&gt;

&lt;h5 id=&quot;center-autoregressive-moving-average-with-exogenous-input-model-armax-center&quot;&gt;&lt;center&gt; **Autoregressive Moving Average with Exogenous Input Model (ARMAX)** &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;The general structure of the linear finite-horizon system can be written as &lt;/p&gt;

&lt;p&gt;\begin{equation} \label{eqn:ARMAX}
  A(z^{-1})\,y(k) = \dfrac{B(z^{-1})}{F(z^{-1})}\,u(k) + \dfrac{C(z^{-1})}{D(z^{-1})}\,e(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;with \(A(z^{-1}), \, B(z^{-1}), \, C(z^{-1}) \) defined as in equation &lt;a href=&quot;#eqn:poly&quot;&gt;\eqref{eqn:poly}&lt;/a&gt; and the \(F(z^{-1}), \text{ and } D(z^{-1})\) polynomials defined as&lt;/p&gt;

&lt;p&gt;\begin{equation}
  D(z^{-1}) = 1 + d_1\,z^{-1} + d_2 + \, z^{-2} + \cdots + d_{n_d}z^{-n_d}  \nonumber
\end{equation} &lt;/p&gt;

&lt;p&gt;\begin{equation}
  F(z^{-1}) = 1 + f_1\,z^{-1} + f_2 + \, z^{-2} + \cdots + f_{n_f}z^{-n_f} 
\end{equation}&lt;/p&gt;

&lt;p&gt;The autoregressive part is given by the component \(A(z^{-1}) \, y(k)\), while the noise term is modeled as a moving average regression model in \(\dfrac{C(z^{-1})}{D(z^{-1})}\,e(k)\); the exogenous component is given by \(\dfrac{B(z^{-1})}{F(z^{-1})}\,u(k)\). &lt;/p&gt;

&lt;h5 id=&quot;center-autoregressive-model-ar-center&quot;&gt;&lt;center&gt; **Autoregressive Model (AR)** &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If in equation \eqref{eqn:ARMAX},  \(B(z^{-1}) = 0\) and \(C(z^{-1}) = D(z^{-1}) = 1\) then  we have an AR model,&lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k) = -a_1 \, y(k-1) -  a_2 \,  y(k-2) - \cdots - a_{n_a} \, y(k-{n_a})
\end{equation}&lt;/p&gt;

&lt;p&gt;Alternatively, if \(A(z^{-1}) = 1\) , \(B(z^{-1}) = 0, \,  \text{ and } \, C(z^{-1}) = 1 \) then we end up with the regression&lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k) = - d_1 \, y(k-1) - d_2 \, y(k-2) - \cdots - d_{n_d} y(k - n_d) + e(k) 
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;center-moving-average-model-ma-center&quot;&gt;&lt;center&gt; **Moving Average Model (MA)** &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If \(A(z^{-1}) = 1\), \(B(z^{-1}) = 0\) and \(D(z^{-1}) = 1\), then we have the following moving average noise model&lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k) = e(k)+ c_1e(k-1) + \cdots + c_{n_c}e(k-n_c)
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;center-autoregressive--with-exogenous-input-model-arx-center&quot;&gt;&lt;center&gt; **Autoregressive  with Exogenous Input Model (ARX)** &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If \(F(z^{-1}) = 1\), and \(C(z^{-1}) = D(z^{-1}) =1\), we obtain the ARX structure
\begin{equation}
\begin{split}
  y(k) &amp;amp; = -a_1 \, y(k-1) - \cdots - a_{n_a} \, y(k-{n_a}) + b_1u(k-1) &lt;br /&gt;
       &amp;amp;+\cdots + b_{n_b}u(k-n_b) + e(k)
       \end{split} 
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;center-autoregressive-moving-average-model-arma-center&quot;&gt;&lt;center&gt; **Autoregressive Moving Average Model (ARMA)** &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;Setting \(B(z^{-1}) = 0\), and \(D(z^{-1}) = 1\), we obtain the ARMA model,
\begin{equation}
\begin{split}
  y(k) &amp;amp; = -a_1 \, y(k-1) - \cdots - a_{n_a} \, y(k-{n_a}) + c_1eu(k-1) &lt;br /&gt;
       &amp;amp;+\cdots + c_{n_c}e(k-n_c)
       \end{split} 
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;center-finite-impulse-response-model-fir-center&quot;&gt;&lt;center&gt; **Finite Impulse Response Model (FIR)** &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;If in equation \eqref{eqn:ARMAX}, \(A(z^{-1}) = F(z^{-1}) = 1 \text{ and } C(z^{-1}) = 0\), we have an FIR model. This can be rewritten as &lt;/p&gt;

&lt;p&gt;\begin{equation}
  y(k+i) = \sum_{j=0}^{n_y - 1} \, h_j\, u(k-j+i-1)
\end{equation}&lt;/p&gt;

&lt;p&gt;for predictions of the process output at \(t = k+i\) with \(i \geq 1\). &lt;/p&gt;

&lt;p&gt;The FIR model has the advantage of being simple to construct in that no complex calculations are required of the model and model assumptions are required. They are arguably the most commonly used in commercial MPC packages. However, it does come up short for unstable systems and it also requires a lot of parameters to estimate an FIR model.&lt;/p&gt;

&lt;p&gt;My &lt;a href=&quot;https://github.com/SeRViCE-Lab/Matlab-Files/blob/master/ident_data/&quot;&gt;Github repo&lt;/a&gt; has various examples where typical &lt;a href=&quot;https://github.com/SeRViCE-Lab/Matlab-Files/blob/master/ident_data/Filtered%20GWN/carimaFWGN.m&quot;&gt;ARX, ARMAX, Impulse Response and AR models&lt;/a&gt;, are identified from finite data. &lt;/p&gt;

&lt;h5 id=&quot;center-box-jenkins-bj-model-center&quot;&gt;&lt;center&gt; **Box-Jenkins (BJ) Model** &lt;/center&gt;&lt;/h5&gt;
&lt;p&gt;The BJ Model is obtained by setting 
\begin{equation} \label{eqn:BJ}
  y(k) = \dfrac{B(z^{-1})}{F(z^{-1})}\,u(k) + \dfrac{C(z^{-1})}{D(z^{-1})}\,e(k)
\end{equation}&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=fsr&amp;gt;&amp;lt;/a&amp;gt;&lt;/p&gt;

&lt;h5 id=&quot;center-finite-step-response-model-fsr-center&quot;&gt;&lt;center&gt; **Finite Step Response Model (FSR)** &lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;\begin{equation}
  y(k) = \sum_{j=0}^{n_s - 1} \, s_j\, \Delta u(k-j-1)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\Delta\) is the differencing operator (\(\Delta = 1 - q^{-1}\)) and \(q^{-1}\) is the backward shift operator. \(n_s\) is the number of step response elements \(s_j\) used in predicting \(y\).&lt;/p&gt;

&lt;h3 id=&quot;centerpredictions-in-system-modelscenter&quot;&gt;&lt;center&gt;**Predictions in System Models**&lt;/center&gt;&lt;/h3&gt;

&lt;h5 id=&quot;centerubackgrounducenter&quot;&gt;&lt;center&gt;**&lt;u&gt;Background&lt;/u&gt;**&lt;/center&gt;&lt;/h5&gt;
&lt;p&gt;In principle, MPCs are long-range predictive controllers(LRPC). Contrary to classical control laws, they are potent if there is a process dead-time or if the setpoint is well-known ahead of time. They were introduced by &lt;a href=&quot;#richalet&quot;&gt;Richalet&lt;/a&gt; and &lt;a href=&quot;#cutler&quot;&gt;Cutler&lt;/a&gt; in the 1970’s and ‘80’s. As mentioned previously, predictions constitute the bulwark of model predictive controllers. Matter-of-factly, the prediction that this family of controllers have in their structure explains their robust performance when correctly implemented in a typical process. The basic algorithm involves:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;At a current time, \(k\), we predict an output, \(\hat{y}_k\), over a certain output horizon, \(n_y\), based on a mathematical model of the plant dynamics. The predicted output is a function of future possible control scenarios.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;From the proposed scenarios, the strategy that delivers the best control action to bring the current process output to the setpoint is chosen.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The chosen control law is applied to the real process input only at the present time \(k\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above procedure is repeated at the next sampling instant leading to an updated control action with correctness based on latest measurements. In literature, we refer to this as the &lt;strong&gt;&lt;em&gt;receding horizon concept&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Other model-based controllers include pole-placement methods and Linear Quadratic methods. When there are no contraints on the control law and the setpoint is not complex, LQ-, pole-placement and predictive controllers generally yield an equivalent result.&lt;/p&gt;

&lt;p&gt;We will assume the model of the plant has been correctly estimated as mentioned in the previous post. The model to be controlled (= &lt;em&gt;plant&lt;/em&gt;) is used in predicting the process output over a defined prediction horizon, $n_y$. Typically, we would want to use an \(i\)-step ahead prediction based on our understanding of the system dynamics. The prediction horizon needs to be carefully selected such the computed optimal control law is not equivalent to a linear quadratic controller. Typically this happens when \(n_y - n_u\) is greater than the open-loop settling time and \(n_u\) is \(\geq\) 5. For large \(n_y - n_u\) and large \(n_u\), &lt;a href=&quot;#GPCs&quot;&gt;Generalized Predictive Controllers&lt;/a&gt; give an almost equal control to an optimal controller with the same weights. With \(n_y - n_u\) and \(n_u\) small, the resulting control law may be severely &lt;a href=&quot;#Rossiter&quot;&gt;suboptimal&lt;/a&gt;. Model predictive controllers are implemented in discrete time since control decisions are made discretely ( = instanteneously). Continuous time systems are sampled to obtain a discrete-time equivalent. The rule of thumb is that the sampling time should be \(10 - 20\) times faster than the dominant transient response of the system (e.g. rise time or settling time). &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is important not to sample at a faster rate than the dominant transient response of the system; otherwise, the high frequency gains within the system will not be picked up by the model. In other words, we should sample fast enough to pick up disturbances, but no faster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Model Predictive Controllers are easy to implement when the model is linear and discrete but they do find applications in nonlinear processes as well. It behooves that a nonlinear model of the process would then be employed to design the controller in such an instance. Nonlinear models will be treated in a future post. Nonlinear systems that use model predictive control tend to have a more rigorous underpinning.&lt;/p&gt;

&lt;p&gt;Within the framework of model predictive controllers, there are several variants in literature. Among these variants are &amp;lt;a name=GPCs&amp;gt;&amp;lt;/a&amp;gt;Clarke’s Generalized Predictive Controllers (GPCs), &lt;a href=&quot;#cutler&quot;&gt;Dynamic Matrix Control&lt;/a&gt;, Extended Predictive Self-Adaptive Control (EPSAC), Predictive Functional Control (PFC), Ydstie’s Extended Horizon Adaptive Control (EHAC), and Unified Predictive Control (UPC) et cet’era. MPCs find applications in nonminimum phase systems, time-delay systems and unstable processes. I will briefly do a once-over on MAC, DMC and GPC with transfer function models and come back full circle to describe GPC with state-space models since these are generally straightforward to code and has less mathematical labyrinths.&lt;/p&gt;

&lt;h4 id=&quot;centeruprediction-in-richalets-model-algorithm-control-macucenter&quot;&gt;&lt;center&gt;&lt;u&gt;**Prediction in Richalet's Model Algorithm Control (MAC)**&lt;/u&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;Generally, these make use of impulse response function models. Suppose we denote the output of a discrete LTI system by a discrete impulse response \(h(j)\) as in&lt;/p&gt;

&lt;p&gt;\begin{equation}
  H(z^{-1}) = A^{-1}(z^{-1}) \, B^(z^{-1})
\end{equation} &lt;/p&gt;

&lt;p&gt;it follows that the output can be written as a function of \(h(t)\) as follows&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:impulse}
  y(t) = \sum_{j=1}^{n=\infty} \, h(j) \, u(t-j)
\end{equation} &lt;/p&gt;

&lt;p&gt;where \(h(j)\) are the respective coefficients of the impulse response. If we assume a stable and causal system, for an \(n\) terminal sampling instant, equation \eqref{eqn:impulse} becomes&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:impulse-lim}
  y(t) = \sum_{j=1}^{n} \, h(j) \, u(t-j)
\end{equation} &lt;/p&gt;

&lt;p&gt;such that we can compute the recursive form of equation \eqref{eqn:impulse-lim} as&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:impulse-rec}
  y_{mdl}(t + k) = \tilde{h}^T \, \tilde{u}(t + k) = \tilde{u}^T(t + k)\, \tilde{h}
\end{equation} &lt;/p&gt;

&lt;p&gt;where, \[\tilde{h}^T = h(1), \, h(2), \, h(3), \, \ldots, \, h(n) \]
and \[\tilde{u}^T(t + k) = u(t + k - 1), \, u(t + k - 2), \, u(t + k -3), \, \ldots, \, u(t + k - n) \].&lt;/p&gt;

&lt;p&gt;The cost function is given by&lt;/p&gt;

&lt;p&gt;\begin{gather}  \label{eqn:mac-cost}
  J_{MAC} &amp;amp;= e^T \, e + \beta^2 \Delta u^T \Delta u&lt;br /&gt;
\end{gather}&lt;/p&gt;

&lt;p&gt;where  \(e = y_r - y_{mdl}\), and \(\beta\) is a penalty function for the input variable \(u(t)\).&lt;/p&gt;

&lt;div class=&quot;boxed&quot;&gt;
&lt;u&gt;&lt;b&gt;MAC Algorithm Summary&lt;/b&gt;&lt;/u&gt;
&lt;ul&gt;
  &lt;li&gt;Define a reference trajectory, $y_{r}$, that $y(k)$ should track.&lt;/li&gt;
  &lt;li&gt;Tune output predictions based on an $FIR$ model order to deal with model disturbances and uncertainties. &lt;/li&gt;
  &lt;li&gt;Use $\beta$ to penalize the control law.&lt;/li&gt;
  &lt;li&gt;Formulate the output predictions using equation \eqref{eqn:impulse-rec} with the assumption that the plant is stable and causal.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;h4 id=&quot;centeruprediction-in-cutlers-dynamic-matrix-controlucenter&quot;&gt;&lt;center&gt;**&lt;u&gt;Prediction in Cutler's Dynamic Matrix Control&lt;/u&gt;**&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;Here, a &lt;a href=&quot;http://lakehanne.github.io/Optimal-Controllers-MPC/#fsr&quot;&gt;finite step response model&lt;/a&gt; is employed in the prediction. It is essentially composed of three tuning factors viz., the prediction horizon, \(n_y\), control weighting factor, \(\beta\), and the control horizon, \(n_u\).&lt;/p&gt;

&lt;div class=&quot;boxed&quot;&gt;
&lt;u&gt;&lt;b&gt;DMC Assumption&lt;/b&gt;&lt;/u&gt;
&lt;ul&gt;
  &lt;li&gt;The process is stable and causal process&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;h4 id=&quot;centerugpc-prediction-of-the-plant-output-described-by-a-transfer-functionucenter&quot;&gt;&lt;center&gt;&lt;u&gt;**GPC: Prediction of the plant output described by a transfer function**&lt;/u&gt;&lt;/center&gt;&lt;/h4&gt;

&lt;p&gt;It seems to me that most literature is filled with prediction of the output described by a transfer function model. I assume this is due to their relative easiness of computation compared against FIR and FSR models and their applicability to unstable processes. Most papers out of Europe tend to favor transfer function methods while US researchers typically prefer state-space methods. Typically, &lt;a href=&quot;http://mathworld.wolfram.com/DiophantineEquation.html&quot;&gt;&lt;strong&gt;Diophantine identity equations&lt;/strong&gt;&lt;/a&gt; are used to form the regressor model. In my opinion, this is royally complex and more often than not obscures the detail of what is being solved. My advice is one should generally stay out of Diophantine models when possible and only use them when necessary. I will briefly expand on this treatment and focus on MPC treatments using recursive state space models when describing GPC algorithms.&lt;/p&gt;

&lt;p&gt;The transfer function model is of the form:&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:tf_model}
  y(k) = \dfrac{z^{-d}B(z^{-1})}{A(z^{-1})}u(k-1)
\end{equation}&lt;/p&gt;

&lt;p&gt;with \(A(z^{-1})\) and \(B(z^{-1})\) defined by the polynomial expansions. A close inspection of equation \eqref{eqn:tf_model} reveals that the transfer function model subsumes both the FIR (i.e. \(A\) = 1 and the coefficients of the \(B\) polynomial are the impulse response elements)  and FSR models (i.e. A = 1 and the coeeficients of the \(B\) polynomial are the step response coefficients \(b_0 = s_0; \, b_j = s_j - s_{j-1} \forall j \geq 1 \).  &lt;/p&gt;

&lt;p&gt;Transfer function models have the good properties of using greedy polynomial orders and variables in representing linear processes but an assumption about the model order has to be made. &lt;/p&gt;

&lt;h5 id=&quot;centeruoutput-predictions-in-tfm-with-diophantine-identitiesucenter&quot;&gt;&lt;center&gt;&lt;u&gt;**Output predictions in TFM with Diophantine identities**&lt;/u&gt;&lt;/center&gt;&lt;/h5&gt;

&lt;p&gt;Another way of writing equation \eqref{eqn:tf_model} is by forming an \(i\)-step ahead predictor as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}  \label{eqn:tf_isa}
  y(k + i) = \dfrac{z^{-d}B(z^{-1})}{A(z^{-1})}u(k + i -1)
\end{equation}&lt;/p&gt;

&lt;p&gt;It follows from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_control#Certainty_equivalence&quot;&gt;certainty equivalence principle&lt;/a&gt; that if we substitute the delay, \(d, \text{ and } B, \, A \) polynomials with their estimates (i.e. \(\hat{d}, \hat{B}, \, \text{and} \hat{A} \)), the optimal control solution obtained would be the same as the system’s delay, \(d, \text{ and } B, \text{ and } A \) polynomials. Let’s form the predicted output estimate as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}   \label{eqn:tf_pred}
  \hat{y}(k + i) = \dfrac{z^{-\hat{d}}\hat{B}(z^{-1})}{\hat{A}(z^{-1})}u(k + i -1)
\end{equation}&lt;/p&gt;

&lt;p&gt;We could rearrange the above equation as&lt;/p&gt;

&lt;p&gt;\begin{split} &lt;br /&gt;
  \hat{y}(k + i) = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - \hat{y}(k + i)[\hat{A}(z^{-1}) -1] \nonumber 
\end{split}&lt;/p&gt;

&lt;p&gt;Since \(A(z^{-1}) = 1 + a_1\,z^{-1} + a_2 + \, z^{-2} + \cdots + a_{n_a}z^{-n_a} \), by definition, we can write &lt;/p&gt;

&lt;p&gt;\begin{align} &lt;br /&gt;
  \hat{y}(k + i) &amp;amp;= z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - \hat{y}(k + i)(a_1\,z^{-1} + a_2 z^{-2} + \cdots + a_{n_a}z^{-n_a}) \nonumber &lt;br /&gt;
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
    \qquad \qquad &amp;amp;= z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - a_1 \hat{y}(k + i -1) - a_2 \hat{y}(k + i -2) - \cdots - a_{n_a} \hat{y}(k + i - n_a)    \nonumber
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
    &amp;amp; = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - (a_1 + a_2 z^{-1} + \cdots + a_{n_a}z^{-n_a + 1}) \hat{y}(k + i - 1) \nonumber
\end{align}&lt;/p&gt;

&lt;p&gt;such that we can write &lt;/p&gt;

&lt;p&gt;\begin{align}  \label{eqn:tf_isa2}
    \hat{y}(k + i) &amp;amp; = z^{-\hat{d}}\hat{B}(z^{-1})u(k + i -1) - z(\hat{A} - 1) \hat{y}(k + i - 1) 
\end{align}&lt;/p&gt;

&lt;p&gt;from equation \eqref{eqn:tf_pred}, where \(z(\hat{A} - 1) = a_1 + a_2 z^{-1} + \, z^{-1} + \cdots + a_{n_a}z^{-n_a + 1}\).&lt;/p&gt;

&lt;p&gt;The equation above is the \(i\)-step ahead predictor using the estimates and runs independently of the process. However, it is a fact of life that disturbances and uncertainties get a vote in any prediction model of a physical/chemical process. Therefore equation \eqref{eqn:tf_isa2} is not well-posed. To avoid prediction errors, we could replace \(\hat{y}(k)\) with \(y(k) \) in the equation and rearrange the model a bit further. Let’s introduce the Diophantine identity equation &lt;/p&gt;

&lt;p&gt;&amp;lt;a name=Dioph&amp;gt;&amp;lt;/a&amp;gt;
&amp;lt;div class=&quot;boxed&quot;&amp;gt;
&lt;b&gt;&lt;center&gt;Diophantine Identity Equation&lt;/center&gt;&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
\begin{equation}  
    \dfrac{1}{\hat{A}} = E\_i + z^{-i}\dfrac{F\_i}{\hat{A}}
\end{equation}
&lt;/ul&gt;
&lt;p&gt;&amp;lt;/div&amp;gt;&lt;/p&gt;

&lt;p&gt;where degree of \(E_i \leq i -1\) and \(F_i\) being of degree \(n_A -1\).&lt;/p&gt;

&lt;p&gt;Multiplying out equation \eqref{eqn:tf_pred} with \(E_i\) gives&lt;/p&gt;

&lt;p&gt;\begin{align}
    E_i \hat{A}(z^{-1}) \hat{y}(k+i) &amp;amp; = z^{-\hat{d}} E_iB(z^{-1}) u(k+i-1)
\end{align}&lt;/p&gt;

&lt;p&gt;Rearranging the Diophantine equation and substituting \(E_i \hat{A}(z^{-1}) = 1 - z^{-i}F_i \) into the above equation, we find that&lt;/p&gt;

&lt;p&gt;\begin{align}   \label{eqn:rigged}
    \hat{y}(k+i) &amp;amp; = z^{-\hat{d}} E_iB(z^{-1}) u(k+i-1) + F_i \underbrace{\hat{y}(k)}_{\textbf{replace with \(y(k)\)}}.
\end{align}&lt;/p&gt;

&lt;p&gt;But&lt;/p&gt;

&lt;p&gt;\begin{equation}
    E_i\hat{B}(z^{-1}) = \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} - \dfrac{q^{-i} \hat{B}(z^{-1}) F_i}{\hat{A(z^{-1})}},
\end{equation}&lt;/p&gt;

&lt;p&gt;if we multiply the &lt;a href=&quot;#Dioph&quot;&gt;Diophantine equation&lt;/a&gt; by \(\hat{B}(z^{-1})\) such that &lt;/p&gt;

&lt;p&gt;\begin{align}  \label{eqn:tf_isasep}
    \hat{y}(k+i) &amp;amp; = z^{-\hat{d}} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} u(k+i-1) + F_i y(k) - z^{-d} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} F_i u(k -1)   \nonumber
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
\qquad \qquad \quad &amp;amp; = \underbrace{z^{-\hat{d}} \dfrac{\hat{B}(z^{-1})}{\hat{A}(z^{-1})} u(k+i-1)} + \underbrace{F_i (y(k) - \hat{y}(k) ) }_{\textbf{correction } }
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
\quad  &amp;amp;  \textbf{prediction} \nonumber 
\end{align}&lt;/p&gt;

&lt;p&gt;We see that equation \eqref{eqn:tf_isasep} nicely separates the output predictor into a prediction part (&lt;i&gt;from the past input&lt;/i&gt;) and a correction part (based on error between model and prediction at time, \(k\) ). Essentially, we have manipulated the equation \(\eqref(eqn:rigged)\) such that we have a correcting term in the prediction of the output by subsituting \(y(k)\) in place of \(\hat{y}(k)\) in equation \eqref(eqn:rigged) to negate issue of model-plant mismatch.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;
&amp;lt;a name=box-jenkins&amp;gt;&amp;lt;/a&amp;gt;[Box and Jenkins]: Box, G.E.P. and Jenkins, G.M., ‘&lt;em&gt;Time Series Analysis:Forecasting and Control. San Francisco, CA&lt;/em&gt;’, Holden Day, 1970.&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=billings&amp;gt;&amp;lt;/a&amp;gt;[Billings] :  Stephen A. Billings. ‘&lt;em&gt;Nonlinear System Identification. NARMAX Methods in the Time, Frequency and Spatio-Temporal Domains&lt;/em&gt;’. John Wiley &amp;amp; Sons Ltd, Chichester, West Sussex, United Kingdom,  2013.&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=cutler&amp;gt;&amp;lt;/a&amp;gt;&lt;a href=&quot;https://www.infona.pl/resource/bwmeta1.element.ieee-art-000004232009&quot;&gt;[C. R. Cutler, B. L. Ramaker]: Dynamic matrix control – A computer control algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=richalet&amp;gt;&amp;lt;/a&amp;gt; [J. Richalet, A. Rault, J.L. Testud and J. papon]: ‘Model Predictive Heuristic Control: Applications to Industrial Processes’, &lt;em&gt;Automatica&lt;/em&gt;, Vol. 14. No. 5, pp. 413 - 428, 1978.&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=Rossiter&amp;gt;&amp;lt;/a&amp;gt;   &lt;a href=&quot;https://www.crcpress.com/Model-Based-Predictive-Control-A-Practical-Approach/Rossiter/9780849312915&quot;&gt;[J.A. Rossiter]:   ‘Model Predictive Control: A Practical Approach’. CRC Press LLC, Florida, USA. 2003&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&amp;lt;a name=Gawthrop&amp;gt;&amp;lt;/a&amp;gt;[Gawthrop] : ‘&lt;em&gt;Continuous-Time Self-Tuning Control&lt;/em&gt;’ Vols I and II, Tannton, Research Studies Press, 1990.&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=Ljung&amp;gt;&amp;lt;/a&amp;gt;[Ljung] : L. Ljung. ‘&lt;em&gt;System Identification Theory for the User&lt;/em&gt;’. 2nd Edition. Upper Saddle River, NJ, USA. Prentice Hall, 1999.&lt;/p&gt;

&lt;p&gt;&amp;lt;a name=Soeterboek&amp;gt;&amp;lt;/a&amp;gt; &lt;a href=&quot;http://repository.tudelft.nl/view/ir/uuid%3A18a07849-f43c-4d98-afb2-0b8a3a2e8b20/&quot;&gt;[Ronald Soeterboek]. ‘Predictive Control: A Unified Approach’. Prentice Hall International (UK) Limited, 1992&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;
&amp;lt;p id=&quot;footnote-1&quot;&amp;gt;[1] A well-posed \(J\) should have an unbiased predictions in steady state. That is the minimizing argument of \(J\) must be consistent with an offset-free tracking. This implies the predicted control sequence to maintain zerto tracking error must be zero. The minimum of \(J = 0\) in steady state is equivalent to \(r_{\rm \rightarrow}\) \(- \, y_{\rm \rightarrow} = 0\) and \(u_{k+i} - u_{k+i-1} = 0\).&lt;/p&gt;

&lt;!-- (Note that \\(x\_{\rm \rightarrow}\\) denotes future values of \\(x\\) ).  --&gt;
</description>
        <pubDate>2015-11-03 04:21:00 -0600</pubDate>
        <link>lakehanne.github.ioOptimal-Controllers-MPC</link>
        <guid isPermaLink="true">lakehanne.github.ioOptimal-Controllers-MPC</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Mathematical Modeling of Robots (Introduction).&lt;/center&gt;</title>
        <description>&lt;!-- Analytics --&gt;
&lt;script&gt;
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-64680332-1', 'auto');
  ga('send', 'pageview');
    ga('send', 'pageview' '/robots-modeling');

&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;!-- Google Tag Manager --&gt;
&lt;noscript&gt;&lt;iframe src=&quot;//www.googletagmanager.com/ns.html?id=GTM-N9TQBW&quot; height=&quot;0&quot; width=&quot;0&quot; style=&quot;display:none;visibility:hidden&quot;&gt;&lt;/iframe&gt;&lt;/noscript&gt;
&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-N9TQBW');&lt;/script&gt;

&lt;!-- End Google Tag Manager --&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;This is a lecture I delivered to UT Dallas EE Senior students on the first day of class in Fall 2015 reposted from &lt;a href=&quot;http://service-lab.github.io/Lecture-1/&quot;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;center&gt;**The Three Laws of Robotics\\(^1\\).**&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;A robot may not injure a human being or, through inaction, allow a human being to come to harm.&lt;/li&gt;
  &lt;li&gt;A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.&lt;/li&gt;
  &lt;li&gt;A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ## Instructor: [Olalekan Ogunmolu](http://github.com/lakehanne).
    Date of Lecture: August 25, 2015.
    Venue:           ECSN 2.126, UT Dallas, Richardson, Texas. --&gt;

&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#objectives&quot;&gt;Course Objectives&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#topics&quot;&gt;Topics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#policy&quot;&gt;Grading Policy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#maths&quot;&gt;Mathematical Modeling of Robots&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#common-robot-arm-configurations&quot;&gt;Common Robot Arm Configurations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#add&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;objectives&quot;&gt;&lt;/a&gt;
## 1. Course Objectives&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Students will learn and utilize the mathematical representation of rigid body motions,
including homogeneous transformations, to solve for position and orientation and
velocities of objects. They will apply this by programming physical robotics systems.&lt;/li&gt;
  &lt;li&gt;Students will formulate and solve forward and inverse kinematic equations and write
programs to solve these equations and carry them out on physical robots.&lt;/li&gt;
  &lt;li&gt;Students will formulate and solve velocity kinematics equations and equations and write
programs to solve these equations and carry them out on physical robots.&lt;/li&gt;
  &lt;li&gt;Student will learn the mathematical procedures for robot motion planning and trajectory
generation and execute such algorithms in simulation and programming robots.&lt;/li&gt;
  &lt;li&gt;Students will expound the mathematical theory and physical implementation of common
robot sensors including rotary encoders and cameras, as well as write programs to read
and process data from such sensors to send control commands to robots.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;topics&quot;&gt;&lt;/a&gt;
## 2. Topics&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Introduction: Historical development of robots; basic terminology and structure; robots in
automated manufacturing&lt;/li&gt;
  &lt;li&gt;Rigid Motions and Homogeneous Transformation: Rotations and their composition;
Euler angles; roll-pitch-yaw; homogeneous transformations&lt;/li&gt;
  &lt;li&gt;Forward Kinematics: Common robot configurations; Denavit-Hartenberg convention; Amatrices;
T-matrices&lt;/li&gt;
  &lt;li&gt;Inverse kinematics: Planar mechanisms; geometric approaches; spherical wrist&lt;/li&gt;
  &lt;li&gt;Velocity kinematics: Angular velocity and acceleration; The Jacobian; singular
configurations; singular values; pseudoinverse; manipulability&lt;/li&gt;
  &lt;li&gt;Motion planning: Configuration space; artificial potential fields; randomized methods;
collision detection&lt;/li&gt;
  &lt;li&gt;Trajectory generation: Joint space interpolation; polynomial splines; trapezoidal velocity
profiles; minimum time trajectories&lt;/li&gt;
  &lt;li&gt;Vision-based control: The geometry of image formation; feature extraction; feature
tracking; the image Jacobian; visual servo control&lt;/li&gt;
  &lt;li&gt;Advanced Topics (one or more of the following depending on time and class interest):
Lagrangian dynamics; path planning; mobile robots; force sensing and force control; &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;policy&quot;&gt;&lt;/a&gt;
## 3. Grading Policy&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Labs: 25%&lt;/li&gt;
  &lt;li&gt;Homeworks: 25%&lt;/li&gt;
  &lt;li&gt;Exam 1: 25%&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exam 2:  25%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Targeted grade ranges:  &lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;                  - A: 90-100%

                  - B: 80-89%

                  - C: 70-79%

                  - D: 60-69%

                  - F: &amp;lt;60%
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a mixed class. Graduate and undergraduate students will be graded and
curved separately.&lt;/p&gt;

&lt;p&gt;There will be two exams given during the semester. In the event of an excused
absence (illness, job-related travel, holy day absence, etc.), students&lt;/p&gt;
&lt;u&gt;**must**&lt;/u&gt;
&lt;p&gt;inform the instructor ahead of time and provide proper documentation of
the conflict. An accommodation will be attempted for verifiable problems.
Homework assignments will be collected graded and discussed in class (as time
permits). &lt;/p&gt;

&lt;p&gt;Homework will be collected at the beginning of the class period when
it is due. Homework that is not reasonably neat and readable, or not bound,
will be marked down. &lt;u&gt;**Late Homework will not be accepted**&lt;/u&gt;. It may not be
returned to you until a week after it is due, which means you may not have it
back for a problem-solving session, or to use in studying for an exam. &lt;u&gt;**If you
want to have it available at these times, you will have to make a photocopy
of it before you turn it in.**&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;maths&quot;&gt;&lt;/a&gt;
##  4.  Mathematical Modeling of Robots&lt;/p&gt;

&lt;h3 id=&quot;definition-of-a-robot&quot;&gt;4.1.  DEFINITION OF A ROBOT&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;A robot is a reprogrammable, multifunctional manipulator designed to move material, parts, tools, or specialized devices through variable programmed motions for the performance of a variety of tasks.&lt;/em&gt;    &lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;pre&gt;&lt;code&gt;        -- Spong, Hutchinson and Vidyasagar, Robot Modeling and Control (2006)    
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;

&lt;p&gt;Basically, a robot should be able to &lt;code&gt;sense&lt;/code&gt;, &lt;code&gt;move&lt;/code&gt; and &lt;code&gt;act intelligently&lt;/code&gt;. Put difeerently, a robot should have sensors on it to enable it to be environmentally-aware. It should be equipped with mechanical parts such as electric motors to make it able to navigate its environment. Lastly, it needs some ‘smarts’ (otherwise referred to as &lt;code&gt;reprogrammability&lt;/code&gt; in engineering literature) to make it intelligent. The reporgrammability aspect of the definition is very important because it gives a robot &lt;code&gt;adaptibility&lt;/code&gt; and &lt;code&gt;usefulness&lt;/code&gt; making the robot &lt;code&gt;reconfigurable&lt;/code&gt;.   &lt;/p&gt;

&lt;h3 id=&quot;robot-manipulators&quot;&gt;4.2.  ROBOT MANIPULATORS&lt;/h3&gt;

&lt;p&gt;Robot manipulators are made up of &lt;u&gt; **links**&lt;/u&gt;, \(i\), which are connected by &lt;u&gt;**joints**&lt;/u&gt;, \(j_i\), to form a &lt;u&gt;**kinematic chain**&lt;/u&gt;. Joints are typically rotary (henceforth referred to as &lt;em&gt;revolute&lt;/em&gt;) or linear (henceforth referred to as &lt;em&gt;prismatic&lt;/em&gt;) with a simple &lt;code&gt;end effector&lt;/code&gt; for manipulating work pieces. Applications vary from simple tasks such as pick and place operations to navigating complex environments (&lt;a href=&quot;https://www.youtube.com/watch?v=_luhn7TLfWU&quot;&gt;MIT Cheetah Robot&lt;/a&gt;) and medical robots such as the &lt;a href=&quot;http://www.davincisurgery.com/da-vinci-surgery/da-vinci-surgical-system/&quot;&gt;Davinci Surgical System&lt;/a&gt; among others.&lt;/p&gt;

&lt;p&gt;A &lt;u&gt;**revolute joint**&lt;/u&gt; is akin to a hinge and allows relative rotation between two links. A &lt;u&gt;**prismatic**&lt;/u&gt; joint allows a linear relative motion between the adjoint links. We will denote the revolute joints by \(R\) and the prismatic joints by \(P\).&lt;/p&gt;

&lt;p&gt;An example of a revolute/ joint is depicted in the figure below:&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/Lec1/Revolute.png&quot; width=&quot;49%&quot; height=&quot;250&quot; align=&quot;middle&quot; /&gt;  
  &lt;img src=&quot;/assets/Lec1/Prismatic.png&quot; width=&quot;49%&quot; height=&quot;250&quot; style=&quot;border-left: 1px solid black;&quot; /&gt;
  &lt;div class=&quot;figcaption&quot; align=&quot;right&quot;&gt;Fig. 1. An example of a revolute joint (left) and a prismatic joint (right). &lt;i&gt;Courtesy, &lt;a href=&quot;https://code.google.com/p/impsim/wiki/jmanual_jointRevolute&quot;&gt;  impsim. &lt;/a&gt;&lt;/i&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The angle \(\Phi\) in the left figure is defined as the joint angle and it connects links &lt;strong&gt;LK1&lt;/strong&gt; and &lt;strong&gt;LK2&lt;/strong&gt;. Similarly, the displacement \(\Phi\) in the right figure connects  links &lt;strong&gt;LK1&lt;/strong&gt; and &lt;strong&gt;LK2&lt;/strong&gt;. &lt;/p&gt;

&lt;u&gt;Rotations follow the right hand rule&lt;/u&gt;
&lt;p&gt;. Essentially, this means if we have three orthonormal vectors \(x\), \(y\), and \(z\) \(\in\) \(\mathcal{R}^3\) which define a coordinate frame, they must satisfy the mathematical relation, &lt;/p&gt;
&lt;center&gt;\\(z\\) = \\(x\\) \\(\times\\) \\(y\\).&lt;/center&gt;
&lt;p&gt;We will denote the axis of rotation of a revolute joint or the axis of displacement of a prismatic joint as \(z\^i\) if the joint connects links \(i\) and \(i+1\). \(\Phi\) is referred to as the &lt;strong&gt;joint variable&lt;/strong&gt; for a revolute or prismatic joint as the case may be.&lt;/p&gt;

&lt;h3 id=&quot;things-to-remember&quot;&gt;Things to remember&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;There are two types of joints: prismatic and revolute joints.&lt;/li&gt;
  &lt;li&gt;\(\Phi\) is referred to the joint variable.&lt;/li&gt;
  &lt;li&gt;A revolute joint will allow rotation between two links.&lt;/li&gt;
  &lt;li&gt;A prismatic joint will allow displacement between two links.&lt;/li&gt;
  &lt;li&gt;The axis of rotation for a typical joint, \(j_i\) that connects links \(i\) and \(i+1\) is \(z^i\).&lt;/li&gt;
  &lt;li&gt;Angles are measured in a clockwise manner so that if an angle along a directed axis is positive if it represents a clockwise rotation about the direction from which we are viewing. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A three link arm with 2 revolute joints for example is referred to as an &lt;strong&gt;RRP&lt;/strong&gt; arm, where the R’s stand for &lt;em&gt;revolute&lt;/em&gt; and the P stands for &lt;em&gt;prismatic&lt;/em&gt;. An example of an RRP robotic arm is the SCARA robot which we shall deal with shortly.&lt;/p&gt;

&lt;h3 id=&quot;taxonomy-of-robot-manipulators&quot;&gt;4.3.  Taxonomy of Robot Manipulators&lt;/h3&gt;

&lt;h4 id=&quot;configuration&quot;&gt;4.3.1. Configuration&lt;/h4&gt;

&lt;p&gt;The configuration of a manipulator is a complete description of the location of every point on the robot manipulator.
When we know the set of all possible configurations, we say we know the &lt;strong&gt;configuration space&lt;/strong&gt; of the robot. The configuration space will correspond mathematically to knowing the set of all possible \(\theta_i\) for a revolute joint or \(d_i\) for a prismatic robot where \(\theta_i\) denotes the respective joint angles and \(d_i\) denotes the respective displacements. The set of angles \(\theta_i\) for a revolute joint is naturally associated with a unit circle in the plane denoted by \(\mathcal{S}^1\). It is typical for you to see revolute joints written as \(\theta_i\) \(\in\)  \(\mathcal{S}^1\) in literature. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt; For a single revolute joint arm, the configuration space is \(\mathcal{S}^1\), which geometrically is a one-dimensional sphere (or 1-D sphere).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 2:&lt;/strong&gt; A two-revolute joint arm will have \(\mathcal{S}^1\) \(\times\) \(\mathcal{S}^1\) configuration space. This is visually equivalent to moving an outer circle arrount an inner concentric circle. This is called a torus (donut-shaped).&lt;/p&gt;

&lt;center&gt; ![Torus](https://upload.wikimedia.org/wikipedia/commons/c/c6/Simple_Torus.svg)&lt;/center&gt;
&lt;center&gt; Fig 2. An example of a Torus. &lt;/center&gt;

&lt;p&gt;&lt;strong&gt;Example 3:&lt;/strong&gt; For a one revolute and one prismatic joint, the configuration space is \(\mathcal{S}^1\) \(\times\) \(\mathcal{R}\) which is geometrically the equivalent of a cylinder.&lt;/p&gt;

&lt;h4 id=&quot;degrees-of-freedom&quot;&gt;4.3.2.  Degrees of freedom&lt;/h4&gt;

&lt;p&gt;The number of degrees of freedom of a robot is the minimum number of parameters required to specify the configuration of a robot. Generally, this is equivalent to the size of the configuration space. Typically, the number of joints for a robotic manipulator will tell us about how many degrees of freedom it has. The &lt;a href=&quot;http://www.robai.com/robots/robot/cyton-epsilon-300/&quot;&gt;cyton arm robot&lt;/a&gt; which we will use in most of the labs that accompany this class has seven joints and hence seven degrees of freedom. A rigid object in three-dimensional space will have six degrees of freedom(DOF) because it will have 3 dedicated to orientation and 3 dedicated to translation. When the DOF is lesser than 6, the robot is said to be &lt;u&gt;underactuated&lt;/u&gt;. An example of underactuated robots include quadrotors with four rotors. When a manipulator has more than 6 DOF, we say such a robot is &lt;u&gt;kinematically redundant&lt;/u&gt;.&lt;/p&gt;

&lt;h4 id=&quot;the-state&quot;&gt;4.3.3.  The State&lt;/h4&gt;

&lt;p&gt;The state includes the geometry of all inputs, all disturbances, velocities, forces and et cetera that determine the current and future response of the manipulator.&lt;/p&gt;

&lt;h4 id=&quot;the-state-space&quot;&gt;4.3.4.  The State Space&lt;/h4&gt;

&lt;p&gt;The state space is the set of all possible states of a manipulator. &lt;/p&gt;

&lt;h4 id=&quot;the-workspace&quot;&gt;4.3.5.  The Workspace&lt;/h4&gt;

&lt;p&gt;The volume of the motion traversed by the end-effector as the manipulator executes possible motions is called the workspace. Often separated into the &lt;b&gt;reachable workspace&lt;/b&gt; and &lt;b&gt;dexterous workspace&lt;/b&gt; depending on how many of the possible set of points an end-effector can reach based. The &lt;strong&gt;reachable workspace&lt;/strong&gt; is the set of all possible points the manipulator can reach while the &lt;strong&gt;dexterous workspace&lt;/strong&gt; is the set of points the manipulator can reach based on an arbitrary orientation of the end effector.&lt;/p&gt;

&lt;h2 id=&quot;accuracy-and-repeatability&quot;&gt;4.4.  Accuracy and Repeatability&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;accuracy&lt;/strong&gt; of a manipulator is a measure of how close the manipulator can reach a specific point \(x, y, z\) in the state space. There is no absolute correct way to measure the accuracy of a robot. One method is to use position encoders in joints at known locations. This would use the assumed geometry of the manipulator and its rigidity to infer the end-effector position from the measured joint positions. It becomes apparent therefore that accuracy is affected by computational errors, machining accuracy during the manipulator construction, gear backlash among other things.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Repeatability&lt;/strong&gt; is a measure of how close a manipulator can return to a previously taught point. The &lt;u&gt;resolution of the controller&lt;/u&gt; will affect a manipulator’s repeatability. The resolution is the smallest degree of increase in manipulator motion that a robot can sense and is the ratio of the total distance travelled and \(2^n\) where \(n\) is the number of bits of the encoder accuracy. Therefore a prismatic axis will have a greater resolution compared to a revolute joint (a straight line is shorter than an arc length).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;common-robot-arm-configurations&quot;&gt;&lt;/a&gt;
##  5. Common Robot Arm Configurations&lt;/p&gt;

&lt;p&gt;Following the prismatic and revolute joint taxonomy, there are many possiblities in the way a manipulator arm can be designed. This section discusses the common design attributes of the typical arrangements.&lt;/p&gt;

&lt;h3 id=&quot;articulated-manipulator-rrr&quot;&gt;5.1.  Articulated Manipulator (RRR)&lt;/h3&gt;

&lt;p&gt;This is otherwise called the &lt;strong&gt;anthromorphic&lt;/strong&gt; manipulator named out of its anthropomorphic characteristics. It has three revolute joints with each axis designated as the &lt;strong&gt;waist&lt;/strong&gt; (\(z_0\)), &lt;strong&gt;shoulder&lt;/strong&gt; (\(z_1\)), and &lt;strong&gt;elbow&lt;/strong&gt; (\(z_2\)). More often than not, the joint axis \(z_2\) will be parallel to \(z_1\) while both \(z_2\) and \(z_1\) will be perpendicular to \(z_0\). The revolute manipulator has a considerably large degree of freedom of movement in a compact space.&lt;/p&gt;

&lt;center&gt;![Articulated Robot rm](/assets/Lec1/RRR.png)&lt;/center&gt;
&lt;center&gt;Fig 5.1. Symbolic representation of a six-DOF elbow manipulator with links and joints similar to those of a a human joint/limbs which explains why it is called an anthroppmorphic robot. Photo courtesy of [\\(^2\\)].&lt;/center&gt;

&lt;h3 id=&quot;the-spherical-manipulator-rrp&quot;&gt;5.2.  The Spherical Manipulator (RRP)&lt;/h3&gt;

&lt;p&gt;If we replace the elbow or last joint in the &lt;strong&gt;RRR&lt;/strong&gt; manipulator with a prismatic joint, we would be left with what is called the &lt;strong&gt;spherical manipulator&lt;/strong&gt;. Spherical joints are capable of arbitrary rotations and the name derives from the fact that the “joint coordinates coincide with the spherical coordinates of the end effector relative to a coordinate frame located at the shoulder joint”\(1\). Passive spherical joints is consists of a &lt;em&gt;ball and socket&lt;/em&gt; joint. This does not work adequately if the joint is to exert forces and torques and hence actuated spherical joints are often constructed such that three revolute joints are combined with motors to the end of making their axes intersect at a point\(^3\).&lt;/p&gt;

&lt;center&gt;![Spherical Robot rm](/assets/Lec1/Spherical.png)&lt;/center&gt;
&lt;center&gt;Fig 5.2. Symbolic representation of a spherical arm (left); The Stanford Arm (right) is an example of a spherical manipulator. Photo courtesy of [\\(^2\\)].&lt;/center&gt;

&lt;h3 id=&quot;the-scara-manipulator-rrp&quot;&gt;5.3.  The SCARA Manipulator (RRP)&lt;/h3&gt;

&lt;p&gt;Short for &lt;strong&gt;S&lt;/strong&gt;elective &lt;strong&gt;Co&lt;/strong&gt;mpliant &lt;strong&gt;A&lt;/strong&gt;rticulated &lt;strong&gt;R&lt;/strong&gt;obot for &lt;strong&gt;A&lt;/strong&gt;ssembly. Introduced in 1979 in Japan and the United States. Typically used for pick and place operations.&lt;/p&gt;

&lt;center&gt;![The Adept One robot](/assets/Lec1/SCARA.png)&lt;/center&gt;
&lt;center&gt;Fig.5.3.  Symbolic representation of the Adept One Robot. Photo Courtesy of [\\(^3\\)].&lt;/center&gt;

&lt;p&gt;It is a bit different from the Spherical Manipulators in that its \(z_0\), \(z_1\) and \(z_2\) are all parallel to one another.&lt;/p&gt;

&lt;h3 id=&quot;the-cylindrical-manipulator-rpp&quot;&gt;5.4.  The Cylindrical Manipulator (RPP)&lt;/h3&gt;

&lt;p&gt;The cylindrical manipulator has two independent degrees of freedom and is typically a combination of a revolute and two prismatic joints such that their axes &lt;strong&gt;intersect&lt;/strong&gt;. An example of the cylindrical joint is shown in fig 5.4.&lt;/p&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;http://icosym-nt.cvut.cz/odl/partners/fuh/EXAMPLES/eqs5/stan_FM.gif&quot; width=&quot;60%&quot; height=&quot;500&quot; align=&quot;middle&quot; /&gt; 
  &lt;div class=&quot;figcaption&quot; align=&quot;middle&quot;&gt;Fig.5.4.  Symbolic representation of the Cylindrical Robot. Photo Courtesy of [4].
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;the-cartesian-manipulator-ppp&quot;&gt;5.5.  The Cartesian Manipulator (PPP)&lt;/h3&gt;

&lt;p&gt;We say a manipulator is cartesian of its first three joints are prismatic. The variables of the joints are the cartesian Cartesian coordinates with respect to the base. They find uses in sealant application to materials as in table-top assembly, as gantry robots e.g. for cargo transfer etc. Its axes are coincident to a Cartesuab coordinator&lt;/p&gt;

&lt;center&gt;![An example cartesian robot manipulator](http://prime.jsc.nasa.gov/ROV/images/cartesian.GIF)&lt;/center&gt;
&lt;center&gt;Fig. 5.5. An example cartesian robot manipulator.&lt;/center&gt;

&lt;p&gt;&lt;a name=&quot;summary&quot;&gt;&lt;/a&gt;
##Summary
In this class, we have covered the mathematical basis of robotics and built a foundation upon which we shall build the next several modules. Please go through the material posted on elearning and chapter 1 of Dr. Spong’s book [\(^2\)] to get familiarized all the more with the topics we have discussed so far. &lt;/p&gt;

&lt;p&gt;We’ll see you in the next class.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;add&quot;&gt;&lt;/a&gt;
## References
- 1.  &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics&quot;&gt;Asimov, Isaac (circa 1950) I, Robot.&lt;/a&gt;.
- 2.  Robot Modeling and Control, Mark W. Spong, Seth Hutchinson and M. Vidyasagar. John Wiley &amp;amp; Sons Inc. 2006.
- 3.  A Mathematical Introduction to Robotic Manipulation, Richard Murray, Zexiang Li and S. Shankar Sastry. CRC Press. 1994.
- 4.  &lt;a href=&quot;http://icosym-nt.cvut.cz/odl/partners/fuh/EXAMPLES/eqs5/stan_FM.gif&quot;&gt;Dynast&lt;/a&gt;.&lt;/p&gt;

&lt;!--
&lt;a href=&quot;https://twitter.com/share&quot; class=&quot;twitter-share-button&quot; data-via=&quot;patmeansnoble&quot;&gt;Tweet&lt;/a&gt;
&lt;script&gt;!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');&lt;/script&gt;
--&gt;
</description>
        <pubDate>2015-08-24 00:00:00 -0500</pubDate>
        <link>lakehanne.github.iorobots-modeling</link>
        <guid isPermaLink="true">lakehanne.github.iorobots-modeling</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Safely upgrading your linux kernel.&lt;/center&gt;</title>
        <description>&lt;p&gt;It’s a bit difficult safely upgrading your linux kernel from an older kernel without glitches losing your unity desktop or lightdm environement or being able to correctly log into your newly installed kernel after all has been said and done. So I decided to write this blog post after previous efforts at unsuccessfully upgrading my kernel from a Linux 3.16 version. For simplicity, we’ll assume you are running a debian distro such as ubuntu or cent-os. If you have a different distro, you could download the kernel straight from &lt;a href=&quot;https://www.kernel.org/&quot;&gt;Linux Kernel&lt;/a&gt;, untar the downloaded file and follow the instructions in the README in order to compile with cmake. After compiling your kernel, go to &lt;a href=&quot;#setting-up-your-new-kernel&quot;&gt;setting up your new kernel&lt;/a&gt; to be able to log on to your new kernel when next you reboot.&lt;/p&gt;

&lt;p&gt;The maintainers over at Ubuntu have compiled the kernel on their &lt;a href=&quot;http://kernel.ubuntu.com/~kernel-ppa/mainline/&quot;&gt;ppa’s&lt;/a&gt; and you could pick which ever kernel you wish to upgrade to. For me, I was upgrading from 3.16 to 4.04; so for the purpose of this tutorial, our instructions will be based on upgrading to kernel 4.0.4.&lt;/p&gt;

&lt;p&gt;First of all, make sure you have all your files properly backed up in a safe place before attempting an upgrade of your kernel.&lt;/p&gt;

&lt;h2 id=&quot;downloads&quot;&gt;Downloads&lt;/h2&gt;

&lt;p&gt;For 32-bit systems, let’s pull the following files from the ubuntu kernel repositories.&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-headers-4.0.4-040004_4.0.4-040004.201505171336_all.deb

$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-headers-4.0.4-040004-generic_4.0.4-040004.201505171336_i386.deb

$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-image-4.0.4-040004-generic_4.0.4-040004.201505171336_i386.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If, however, your system is a 64-Bit System, you would want to pull the following files instead:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-headers-4.0.4-040004_4.0.4-040004.201505171336_all.deb

$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-headers-4.0.4-040004-generic_4.0.4-040004.201505171336_amd64.deb

$ wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.0.4-wily/linux-image-4.0.4-040004-generic_4.0.4-040004.201505171336_amd64.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, install each of the files from your terminal window&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo dpkg -i linux-headers-4.0.4*.deb linux-image-4.0.4*.deb&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hang in there. Let’s set up &lt;strong&gt;GRUB&lt;/strong&gt; to point to the new kernel installation. This is necessary so that when you restart your system, your bootloader gives you the option to choose which kernel to load (Grub 2.02 does this, I have not tried it on other grub versions)&lt;/p&gt;

&lt;h2 id=&quot;setting-up-your-new-kernel&quot;&gt;Setting up your new kernel&lt;/h2&gt;

&lt;p&gt;We’ll assume your boot-loader is &lt;strong&gt;GRUB&lt;/strong&gt;. The steps we carried out above will upgrade the linux installation which could also have been performed with the following (rather &lt;em&gt;unsafe&lt;/em&gt;) commands:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo aptitude update 
$ sudo aptitude safe-upgrade &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I called those unsafe because you do not have control of what happens during the upgrade process and it is well documented that such methods have been known to tamper with graphic drivers and what-a-view after upgrades. I generally prefer going the old-fashioned way as I describe in this post.&lt;/p&gt;

&lt;p&gt;Depending on your system configuration settings, after the steps above, you may find that your kernel and GRUB bootloader configuration have already been updated. Matter-of-factly, GRUB may automatically select the right boot menu option when you next restart your system. But before you go ahead and restart, you want to ensure everything is correctly set or you might find (as I did) that Mr. Lightdm/Gdm/Unity Desktop doesn’t show up after your reboot. &lt;/p&gt;

&lt;p&gt;Therefore, we will configure GRUB to  automatically check out our new kernel, and should it fail, we would like to be able to revert to the previous kernel. Since GRUB is configured by the file &lt;code&gt;/boot/grub/grub.cfg&lt;/code&gt;, it makes sense to edit this brobdinagian file. But rather than work with the bugging lines of code in &lt;code&gt;/boot/grub/grub.cfg&lt;/code&gt;, we will edit relevant lines in &lt;code&gt;/etc/default/grub&lt;/code&gt;, then update grub to to have &lt;code&gt;/boot/grub/grub.cfg&lt;/code&gt; see our latest grub edit.&lt;/p&gt;

&lt;p&gt;Fire up &lt;code&gt;/etc/default/grub&lt;/code&gt; in your favorite editor and set the following configurations:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
GRUB_DEFAULT=saved
GRUB_TIMEOUT=0
GRUB_CMDLINE_LINUX_DEFAULT=”quiet splash panic=10″
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Basically, we are telling GRUB to use the last-saved selection, do an auto boot-up after 5 seconds if user does nothing, and inform all kernels to reboot after 10 seconds if they die completely. Note, the TIMEOUT option is disabled in GRUB 2.0.2; so if your grub version is greater than version 2.0, you can discount line 2 above. &lt;/p&gt;

&lt;p&gt;We need to set the kernel that is initially &lt;em&gt;saved&lt;/em&gt;. We will set this to the known working configuration within the terminal as follows:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo grub-set-default &quot;Ubuntu&amp;gt;Ubuntu-with-Linux-3.16.0-30-generic&quot; &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could check the kernel you’re currently running with &lt;code&gt;uname -r&lt;/code&gt; and copy the label used in place of what I have above.  Next, we will make GRUB try the new kernel on the next reboot, such as:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo grub-reboot ”Ubuntu&amp;gt;Ubuntu-with-Linux-4.0.4-04-new” &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will save our configurations to the boot folder using:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo update-grub &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should have an output that says something like this:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
	Generating grub configuration file ...
	Found linux image: /boot/vmlinuz-4.0.4-040004-generic
	Found initrd image: /boot/initrd.img-4.0.4-040004-generic
	Found linux image: /boot/vmlinuz-3.16.0-30-generic
	Found initrd image: /boot/initrd.img-3.16.0-30-generic
	Found memtest86+ image: /memtest86+.elf
	Found memtest86+ image: /memtest86+.bin
	done
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Try rebooting. &lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo reboot &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should be fine afterwards.&lt;/p&gt;

&lt;p&gt;If everything goes honky-dory, set the new kernel as the saved default for future boot-up’s:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo grub-set-default &quot;Ubuntu&amp;gt;Ubuntu-with-Linux-4.0.4-040004-generic&quot; &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If at anytime, you choose to uninstall, just go ahead and do the following in your terminal:&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;$ sudo apt-get remove 'linux-headers-4.0.4*' 'linux-image-4.0.4*'&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations!&lt;/p&gt;
</description>
        <pubDate>2015-07-22 18:39:00 -0500</pubDate>
        <link>lakehanne.github.ioUpgrading-Linux-Kernel</link>
        <guid isPermaLink="true">lakehanne.github.ioUpgrading-Linux-Kernel</guid>
        
        
      </item>
    
      <item>
        <title>&lt;center&gt;Why bother with a PhD?&lt;/center&gt;</title>
        <description>&lt;!-- Analytics --&gt;
&lt;script&gt;
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-64680332-1', 'auto');
  ga('send', 'pageview');
  ga('send', 'pageview' '/gradlife');

&lt;/script&gt;

&lt;!-- Google Tag Manager --&gt;
&lt;noscript&gt;&lt;iframe src=&quot;//www.googletagmanager.com/ns.html?id=GTM-WSQ4LR&quot; height=&quot;0&quot; width=&quot;0&quot; style=&quot;display:none;visibility:hidden&quot;&gt;&lt;/iframe&gt;&lt;/noscript&gt;
&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WSQ4LR');&lt;/script&gt;

&lt;!-- End Google Tag Manager --&gt;

&lt;div style=&quot;text-align:center;&quot;&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This is a post from my old &lt;a href=&quot;http://fancylittlerobots.blogspot.com/2014/07/so-why-bother-with-phd.html&quot;&gt;blog&lt;/a&gt;. I am reposting this here to get catch a drift of jekyll et cetera. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Recently, I’ve been questioning my plans for the future and after a certain amount of reflection, I came up with the following (seeming worthwhile?) reasons why a grad school education would be worth the shot.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Passion for conceptualization.&lt;/strong&gt; I fancy conceptualizing, finding issues and directions, definitions, exposition and critical insight. In developing a new model or notion, I love making definitions and then developing algorithms and analyses which ultimately lead to the solution. The opportunity to feed (no, feast!) on ideas – to investigate the physics behind machines and to understand the interconnectedness of components, hearing people’s opinions, seeing the flaws in their ideas and improvising upon them to achieve some sort of epic grandeur has always been an intrinsic component of my thought-actions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quest for challenge.&lt;/strong&gt;  I like to try things that other people find difficult.  Yes, I might spend my free time differently than most, but I think synthesizing disparate sets of data and making robots fly, track objects and help humans live better is sexy. Also, I’m a pretty thoughtful person and I have a great respect for people who know a lot of stuff about an esoteric subject. It helps that I have long fancied tech gurus and nerds and have many times imagined myself to be some rock-star inventor someday.  Every so often, I would listen to a google/TED talk host or a similar facsimile and the sheer majesty of watching creativity in display always awed me. I fancy myself one day, at one of those Google tech talks varnishing the truths of some newly coined robotics technology.  It also helps that I like being a disseminator of knowledge and I figure that a PhD is a prerequisite for me if I am going to be someone that would matter in academe in the future.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Evolution from traditional mercenary work&lt;/strong&gt;  Since finishing my BS degree almost a decade ago, I have worked in the private sector for various international companies. I know I can hardly work on a problem that I want for myself without ensuring it is a problem that the business is interested in or one that could bring in immediate results in terms of profits. I also know the pressure that comes with delivering according to set goals by business executives - goals that sometimes make absolutely no sense to me except that they rake in profit for the company. Sometimes, the means to achieving such goals could be immoral and oftentimes unethical. But hey, so long as no one catches you and you are delivering results, who cares? My experience to come back to academe stems from my work experiences as well as love for learning, innovation and other life experiences.
I imagine that the life of an academic provides an eternal exploration for creativity and comfort in life. For example, I have been working as a lecturer at a small state college this past one year and I get so much satisfaction from my job. I can work around my schedule without disrupting someone else’s almighty timetable or Gantt chart as would be the case in a company. I also get to explore in my little world the possibilities of technology if one applies the right tools.
 I know the limitation placed on creativity with 9 – 5 jobs and I am not impressed with this sort of job at this stage in my career. Someday, I don’t plan on having a boss.  I also don’t plan on having a 9 - 5 job.  Given my love for/interest in higher education, a Ph.D. seems like a good tool to have to achieve this goal. I certainly would not mind becoming a rock-star professor but I am also in this PhD endeavor for the freedom and the opportunities. So rather than serving as a paid-agent in propagating someone else’s vision (as in a company rote-job), I would love to pay for the chance (with bloody sweat and hardwork) to solve open-ended problems.
Opportunity to develop my skills to the uttermost. In a PhD, I can choose what skills to develop, for example, coding, teaching, innovating (certainly) or even delivering a talk. I know that in exploring questions, I will have access to experts in their field. I want to develop a sense of confidence in the power of rational thought and the way of approaching methodically the job of solving complexity. This, I believe, would constantly take me to the edge in my quest to utilize my talents to the uttermost as I bring solutions to otherwise unsolvable problems. I expect I would be able to research anything and have the proclivity to question and understand all that is around me and seek out new ways of doing it or seeing a problem. Because I love asking difficult questions and finding the right solutions, I expect that the aftermath of the PhD pursuit would give me the confidence that I can move around research areas, pick shit up quickly and say something enlightening about it, even if others have looked at it a long time.
Pursuing a PhD is fun itself: The opportunity to have an increased appreciation for creativity in other people and every area of life, to think creatively and seek it out and to be more appreciative of what it took to do creativity and how it is different from previous works, to enjoy a new sense of taste and a critical sense, willingness to contradict conventional wisdom and being constructively critical, for me outweighs the costs (limited earning potential compared to my colleagues in the industry etc). In the unlikely scenario that I do not use my PhD in my future career, it will have been fun and a world-class training that is priceless not to mention the next five years would be a productive few years of my life.
I know that it won’t be a rosy picture all through graduate school: papers might/possibly will get rejected; what I work on may not interest much people outside of my little academic circle as the return-on-research value might not come early in my PhD pursuit or possibly even during my PhD program but I know the frustrations will be a part of the learning process: honing my research thrusts and learning how to achieve a high-level research objective along the way. I hope that it would help that I have learnt a great deal from knowing what it’s like to fail, come second, let people down, to let myself down in the past.
Because success is a mix of natural talent, hard work, judgment and, yes, a bit of luck along the way (which I guess I have copious amounts of), I am confident that I will be a good doctoral candidate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;WHY ROBOTICS?&lt;/strong&gt;
My decision to study robotics was a culmination of my experiences in an eclectic field. During my MS degree I happened to take a module titled “Robotics and Multisensor systems” which expanded on topics such as robotic manipulators, robotic architectures, robots geometry and kinematics, applications of robotics and this gave me a fair understanding of basic theoretical principals and research challenges in robotics (designing assistive rehabilitation robotics, medical robotics, trajectory path planning and SLAM among others).
But more importantly, the multisensors part of the course introduced me to target tracking and computer vision applications to robotics, data fusion models, control and estimaton and process architectures  and after a couple of labs and self-reading of many texts, my interests in the applications of robots boomed. I consulted journals and technical publications to gain a better overview of the state of the art research and current research challenges.
When it came time for my MS cohort to choose project topics, I decided for robotics having enjoyed the basic principles of robotics and being willing to accept the challenges of conceptualizing new models to tackle challenges in robotic development. With my advisor, Dr Tony Dodd, who directed the autonomous systems research lab at the time, I came up with the proposal that I develop a machine vision based navigation system for a UAV.
This project work taught me how to approach complex engineering tasks. Developing a new flying robot alone required an immense amount of consideration: from having to carefully choose the hardware and mechanical components that could aid achieving a light, flying object. The first time I coupled my arduino-based quadrotor and tested it for flight, I soon found that government regulations in the UK against outdoor flying objects and environmental perturbations would limit the scope of my experiment to an indoors one. I joined a PhD pet project tasked with developing a waypoints controlled quadrotor at the robotics challenge competition in Eindhoven to gain more experience at the subject, read technical papers, talked with other people doing similar research and consulted publications with related research topics. I ended up limiting the scope of the project work to an indoor environment where flight path was predetermined and goal definitions for the project was redrawn to include visual-guidance to a pre-determined landing target, image-based assessment of target and subsequent landing.
Doing this project taught me more than how to build a visual-servoing robot. I honed my control theory skills to fit the definition of my research goal, and more importantly, I learnt the value of having a good advisor who could guide me in the early stages of my research development. I also learnt about planning and coordination involved in building a large, 6-DOF system with a team of people of varied skills: departmental technicians, colleagues with robotics expertise, PhD robotics researchers and mechanical engineers. I learnt to have an increased appreciation for creativity in other people and every area of life; My experience helped me develop a new-found sense of appreciation for hardware and application-based research and gave me the confidence to commit myself to more ambitious dreams.
Though I enjoyed Robotics, I increasing found myself wanting to solve more relevant problems. Once while attending an engineers without borders symposium at my school, I learnt about a project challenge of developing a low-cost wind turbine for Haley farm in Sheffield and how these talented students were working on assistive robotics for disabled children and sick people in the South Yorkshire region. I soon enrolled with this team and began to combine my scientific elegance with practical impact to a greater degree. I soon found myself raising funds for the assistive robotics program along with the project director. Within two months, we raised a few thousand pounds which was enough for us to put together a perfect team of engineers that successfully developed a semi-automatic prosthetic limbs for a boy who had walking deformities.
Also, by speaking with university authorities, we gained scrap metals which we melted and moulded in the materials engineering department to build the engine for our wind turbine. Although generating a measly power of 700watts, we were proud of our little progress and our works brought us a little fame and fortune enabling the “Engineers without Borders” society to be named the University Society for the year. I found the experiences invaluable as I had the chance to work with engineers from diverse fields including mechanical, and civil and electrical engineering and all the more reinforced my cross-disciplinary research interests. I learnt that research areas are in a constant state of flux; as new technologies and problems appear yesterday’s problems are solved and go out of fashion. Old problems resurface in new clothes and lead to wonderful new discoveries.&lt;/p&gt;

&lt;h4 id=&quot;what-if-any-specific-topicsproblems-in-robotics-are-you-hoping-to-solveinvestigate&quot;&gt;What, if any, specific topics/problems in Robotics are you hoping to solve/investigate?&lt;/h4&gt;

&lt;p&gt;My experience with vision algorithms in autonomous systems so far has confirmed the importance of finding efficient distributed vision sensing algorithms for multi-target tracking by a team of cooperative robots, dynamic map-building by cooperative agents in a leader-follower relationship, distributed formation of a group of mobile robots or synchronization and rendezvous as in consensus networks. I therefore want my graduate research focussed on developing better build systems (efficient control and estimation algorithms for energy-saving in nodes through embedded systems processing), error checking mechanisms (by exploring control algorithms for optimum error correction and data compression models in control unit applications) and embedded video processors capable of opportunistic sensing, efficient navigation and scene analysis by autonomous robots. As my research carries along in graduate school, I look forward to developing effective communication protocols for the efficient delivery of sensed information in energy-constrained environments. 
Essentially, in a high-level sense, my research goal is to use visual-servoing and computer vision techniques, with minimal computational costs, to improve reliability and accuracy in robotic control and target tracking applications&lt;/p&gt;

&lt;!--
&lt;a href=&quot;https://twitter.com/share&quot; class=&quot;twitter-share-button&quot; data-via=&quot;patmeansnoble&quot;&gt;Tweet&lt;/a&gt;
&lt;script&gt;!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');&lt;/script&gt;

--&gt;
</description>
        <pubDate>2014-07-01 14:53:00 -0500</pubDate>
        <link>lakehanne.github.iogradlife</link>
        <guid isPermaLink="true">lakehanne.github.iogradlife</guid>
        
        <category>PhD;</category>
        
        <category>Thoughts;</category>
        
        <category>Plans;</category>
        
        <category>Purposes</category>
        
        
      </item>
    
  </channel>
</rss>
